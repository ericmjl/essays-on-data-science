
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
        <link rel="prev" href="../../computing/recursion/">
      
      
        <link rel="next" href="../markov-models/">
      
      
      <link rel="icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.5.2, mkdocs-material-9.2.8">
    
    
      
        <title>An Introduction to Probability and Computational Bayesian Statistics - Essays on Data Science</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.046329b4.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.85d0ee34.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="https://unpkg.com/mermaid@7.1.2/dist/mermaid.css">
    
      <link rel="stylesheet" href="../../css/ansi-colours.css">
    
      <link rel="stylesheet" href="../../css/jupyter-cells.css">
    
      <link rel="stylesheet" href="../../css/pandas-dataframe.css">
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="light-blue" data-md-color-accent="light-blue">
  
    
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#an-introduction-to-probability-and-computational-bayesian-statistics" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="Essays on Data Science" class="md-header__button md-logo" aria-label="Essays on Data Science" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Essays on Data Science
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              An Introduction to Probability and Computational Bayesian Statistics
            
          </span>
        </div>
      </div>
    </div>
    
      
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        <a href="https://github.com/ericmjl/essays-on-data-science" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.4.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    ericmjl/essays-on-data-science
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="Essays on Data Science" class="md-nav__button md-logo" aria-label="Essays on Data Science" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    Essays on Data Science
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/ericmjl/essays-on-data-science" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.4.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    ericmjl/essays-on-data-science
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Home
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" >
        
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Computing
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            Computing
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../computing/recursion/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Recursion
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
    
  
  
    
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" checked>
        
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Machine Learning
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            Machine Learning
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
    
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  <span class="md-ellipsis">
    An Introduction to Probability and Computational Bayesian Statistics
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    An Introduction to Probability and Computational Bayesian Statistics
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#probability-distributions" class="md-nav__link">
    Probability Distributions
  </a>
  
    <nav class="md-nav" aria-label="Probability Distributions">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#base-object-implementation" class="md-nav__link">
    Base Object Implementation
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#probability-density-function" class="md-nav__link">
    Probability Density Function
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#log-probability" class="md-nav__link">
    Log Probability
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#random-variables" class="md-nav__link">
    Random Variables
  </a>
  
    <nav class="md-nav" aria-label="Random Variables">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#definition" class="md-nav__link">
    Definition
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#realizations-of-a-random-variable" class="md-nav__link">
    Realizations of a Random Variable
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#data-generating-process" class="md-nav__link">
    Data Generating Process
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bayes-rule" class="md-nav__link">
    Bayes' Rule
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#translating-bayes-math-to-python" class="md-nav__link">
    Translating Bayes' Math to Python
  </a>
  
    <nav class="md-nav" aria-label="Translating Bayes' Math to Python">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#defining-posterior-log-likelihood" class="md-nav__link">
    Defining Posterior Log-Likelihood
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#computing-the-posterior-with-sampling" class="md-nav__link">
    Computing the Posterior with Sampling
  </a>
  
    <nav class="md-nav" aria-label="Computing the Posterior with Sampling">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#metropolis-hastings-sampling" class="md-nav__link">
    Metropolis-Hastings Sampling
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#transformations-as-a-hack" class="md-nav__link">
    Transformations as a Hack
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#samples-from-posterior" class="md-nav__link">
    Samples from Posterior
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#topics-we-skipped-over" class="md-nav__link">
    Topics We Skipped Over
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#an-anchoring-thought-framework-for-learning-computational-bayes" class="md-nav__link">
    An Anchoring Thought Framework for Learning Computational Bayes
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../markov-models/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Markov Models From The Bottom Up, with Python
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../generating-markov-chains-dirichlet/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Dirichlet Processes and Hidden Markov Model Transition Matrices
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../message-passing/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Computational Representations of Message Passing
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../reimplementing-models/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Reimplementing and Testing Deep Learning Models
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../differential-computing-jax/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Differential Computing Explained
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../nngp/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Infinitely Wide Neural Networks
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../graph-nets/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    An Attempt at Demystifying Graph Deep Learning
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../llm-dev-guide/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    A Developer-First Guide to LLM APIs
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" >
        
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Miscellaneous
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            Miscellaneous
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../miscellaneous/dashboarding-landscape/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    A Review of the Python Data Science Dashboarding Landscape in 2019
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../miscellaneous/learning-to-learn/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    How I Learned to Learn
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../miscellaneous/pydata-landscape/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    An Opinionated and Unofficial Guide to the PyData Ecosystem
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../miscellaneous/static-sites-on-dokku/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Static Sites and Apps On Your Own Dokku Server
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../miscellaneous/code-style-tools/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Code Style Tools
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5" >
        
          <label class="md-nav__link" for="__nav_5" id="__nav_5_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Software Skills
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5">
            <span class="md-nav__icon md-icon"></span>
            Software Skills
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../software-skills/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Importance
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../software-skills/code-formatting/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Formatting your code
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../software-skills/documentation/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Documenting your code
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../software-skills/environment-variables/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    A Data Scientist's Guide to Environment Variables
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../software-skills/refactoring/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Refactoring your code
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../software-skills/testing/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Testing your code
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6" >
        
          <label class="md-nav__link" for="__nav_6" id="__nav_6_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    People Skills
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_6">
            <span class="md-nav__icon md-icon"></span>
            People Skills
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../people-skills/hiring/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Hiring and Interviewing Data Scientists
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_7" >
        
          <label class="md-nav__link" for="__nav_7" id="__nav_7_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Terminal Hacks
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_7_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_7">
            <span class="md-nav__icon md-icon"></span>
            Terminal Hacks
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../terminal/cli-tools/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Tools and Upgrades for your CLI
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../terminal/pre-commits/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Using pre-commit git hooks to automate code checks
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_8" >
        
          <label class="md-nav__link" for="__nav_8" id="__nav_8_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Workflow
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_8_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_8">
            <span class="md-nav__icon md-icon"></span>
            Workflow
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../workflow/code-review/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Practicing Code Review
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../workflow/effective-commit-messages/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Effective Git Commits in Data Science
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../workflow/gitflow/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Principled Git-based Workflow in Collaborative Data Science Projects
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
    <li class="md-nav__item">
      <a href="../../supporters/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Supporters
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#probability-distributions" class="md-nav__link">
    Probability Distributions
  </a>
  
    <nav class="md-nav" aria-label="Probability Distributions">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#base-object-implementation" class="md-nav__link">
    Base Object Implementation
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#probability-density-function" class="md-nav__link">
    Probability Density Function
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#log-probability" class="md-nav__link">
    Log Probability
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#random-variables" class="md-nav__link">
    Random Variables
  </a>
  
    <nav class="md-nav" aria-label="Random Variables">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#definition" class="md-nav__link">
    Definition
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#realizations-of-a-random-variable" class="md-nav__link">
    Realizations of a Random Variable
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#data-generating-process" class="md-nav__link">
    Data Generating Process
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bayes-rule" class="md-nav__link">
    Bayes' Rule
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#translating-bayes-math-to-python" class="md-nav__link">
    Translating Bayes' Math to Python
  </a>
  
    <nav class="md-nav" aria-label="Translating Bayes' Math to Python">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#defining-posterior-log-likelihood" class="md-nav__link">
    Defining Posterior Log-Likelihood
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#computing-the-posterior-with-sampling" class="md-nav__link">
    Computing the Posterior with Sampling
  </a>
  
    <nav class="md-nav" aria-label="Computing the Posterior with Sampling">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#metropolis-hastings-sampling" class="md-nav__link">
    Metropolis-Hastings Sampling
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#transformations-as-a-hack" class="md-nav__link">
    Transformations as a Hack
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#samples-from-posterior" class="md-nav__link">
    Samples from Posterior
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#topics-we-skipped-over" class="md-nav__link">
    Topics We Skipped Over
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#an-anchoring-thought-framework-for-learning-computational-bayes" class="md-nav__link">
    An Anchoring Thought Framework for Learning Computational Bayes
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  

  
  


<h1 id="an-introduction-to-probability-and-computational-bayesian-statistics">An Introduction to Probability and Computational Bayesian Statistics</h1>
<p>In Bayesian statistics,
we often say that we are "sampling" from a posterior distribution
to estimate what parameters could be,
given a model structure and data.
What exactly is happening here?</p>
<p>Examples that I have seen on "how sampling happens"
tends to focus on an overly-simple example
of sampling from a single distribution with known parameters.
I was wondering if I could challenge myself
to come up with a "simplest complex example"
that would illuminate ideas that were obscure to me before.
In this essay, I would like to share that knowledge with you,
and hopefully build up your intuition behind
what is happening in computational Bayesian inference.</p>
<h2 id="probability-distributions">Probability Distributions</h2>
<p>We do need to have a working understanding
of what a probability distribution is before we can go on.
Without going down deep technical and philosophical rabbit holes
(I hear they are deep),
I'll start by proposing
that "a probability distribution is a Python object
that has a math function
that allocates credibility points onto the number line".</p>
<p>Because we'll be using the normal distribution extensively in this essay,
we'll start off by examining that definition
in the context of the standard normal distribution.</p>
<h3 id="base-object-implementation">Base Object Implementation</h3>
<p>Since the normal distribution is an object,
I'm implying here that it can hold state.
What might that state be?
Well, we know from math that probability distributions have parameters,
and that the normal distribution
has the "mean" and "variance" parameters defined.
In Python code, we might write it as:</p>
<div class="highlight"><pre><span></span><code><span class="k">class</span> <span class="nc">Normal</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mu</span> <span class="o">=</span> <span class="n">mu</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sigma</span> <span class="o">=</span> <span class="n">sigma</span>
</code></pre></div>
<h3 id="probability-density-function">Probability Density Function</h3>
<p>Now, I also stated that the normal distribution has a math function
that we can use to allocate credibility points to the number line.
This function also has a name,
called a "probability density function", or the "PDF".
Using this, we may then extend extend this object
with a method called <code>.pdf(x)</code>,
that returns a number
giving the number of credibility points
assigned to the value of <code>x</code> passed in.</p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="k">class</span> <span class="nc">Normal</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mu</span> <span class="o">=</span> <span class="n">mu</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sigma</span> <span class="o">=</span> <span class="n">sigma</span>

    <span class="k">def</span> <span class="nf">pdf</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">(</span>
            <span class="mi">1</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">sigma</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">)</span>
            <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span>
                <span class="o">-</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">mu</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span>
                <span class="o">/</span> <span class="mi">2</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">sigma</span> <span class="o">**</span> <span class="mi">2</span>
            <span class="p">))</span>
</code></pre></div>
<p>If we pass in a number <code>x</code> from the number line,
we will get back another number that tells us
the number of credibility points given to that value <code>x</code>,
under the state of the normal distribution instantiated.
We'll call this <span class="arithmatex"><span class="MathJax_Preview">P(x)</span><script type="math/tex">P(x)</script></span>.</p>
<p>To simplify the implementation used here,
we are going to borrow some machinery already available to us
in the Python scientific computing ecosystem,
particularly from the SciPy stats module,
which gives us reference implementations of probability distributions.</p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">norm</span>

<span class="k">class</span> <span class="nc">Normal</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mu</span> <span class="o">=</span> <span class="n">mu</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sigma</span> <span class="o">=</span> <span class="n">sigma</span>

        <span class="c1"># We instantiate the distribution object here.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dist</span> <span class="o">=</span> <span class="n">norm</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="n">mu</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">sigma</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">pdf</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># Now, our PDF class method is simplified to be just a wrapper.</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">dist</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</code></pre></div>
<h3 id="log-probability">Log Probability</h3>
<p>A common task in Bayesian inference is computing the likelihood of data.
Let's assume that the data <span class="arithmatex"><span class="MathJax_Preview">{X_1, X_2, ... X_i}</span><script type="math/tex">{X_1, X_2, ... X_i}</script></span> generated
are independent and identically distributed,
(the famous <em>i.i.d.</em> term comes from this).
This means, then, that the joint probability of the data that was generated
is equivalent to the product of the individual probabilities of each datum:</p>
<div class="arithmatex">
<div class="MathJax_Preview">P(X_1, X_2, ... X_i) = P(X_1) P(X_2) ... P(X_i)</div>
<script type="math/tex; mode=display">P(X_1, X_2, ... X_i) = P(X_1) P(X_2) ... P(X_i)</script>
</div>
<p>(We have to know the rules of probability to know this result;
it is a topic for a different essay.)</p>
<p>If you remember the notation above,
each <span class="arithmatex"><span class="MathJax_Preview">P(X_i)</span><script type="math/tex">P(X_i)</script></span> is an evaluation of <span class="arithmatex"><span class="MathJax_Preview">X_i</span><script type="math/tex">X_i</script></span>
on the distribution's probability density function.
It being a probability value means it is bound between 0 and 1.
However, multiplying many probabilities together
usually will result in issues with underflow computationally,
so in evaluating likelihoods,
we usually stick with log-likelihoods instead.
By the usual rules of math, then:</p>
<div class="arithmatex">
<div class="MathJax_Preview">\log P(X_1, X_2, ..., X_i) = \sum_{j=1}^{i}\log P(X_i)</div>
<script type="math/tex; mode=display">\log P(X_1, X_2, ..., X_i) = \sum_{j=1}^{i}\log P(X_i)</script>
</div>
<p>To our normal distribution class,
we can now add in another class method
that computes the sum of log likelihoods
evaluated at a bunch of i.i.d. data points.</p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">norm</span>

<span class="k">class</span> <span class="nc">Normal</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mu</span> <span class="o">=</span> <span class="n">mu</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sigma</span> <span class="o">=</span> <span class="n">sigma</span>

        <span class="c1"># We instantiate the distribution object here.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dist</span> <span class="o">=</span> <span class="n">norm</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="n">mu</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">sigma</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">pdf</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># Now, our PDF class method is simplified to be just a wrapper.</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">dist</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">logpdf</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">dist</span><span class="o">.</span><span class="n">logpdf</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</code></pre></div>
<h2 id="random-variables">Random Variables</h2>
<h3 id="definition">Definition</h3>
<p>Informally, a "random variable" is nothing more than
a variable whose quantity is non-deterministic (hence random)
but whose probability of taking on a certain value
can be described by a probability distribution.</p>
<p>According to the Wikipedia definition of a <a href="https://en.wikipedia.org/wiki/Random_variable">random variable</a>:</p>
<blockquote>
<p>A random variable has a probability distribution, which specifies the probability of its values.</p>
</blockquote>
<p>As such, it may be tempting to conceive of a random variable
as an object that has a probability distribution attribute attached to it.</p>
<h3 id="realizations-of-a-random-variable">Realizations of a Random Variable</h3>
<p>On the other hand, it can also be convenient to invert that relationship,
and claim that a probability distribution
can generate realizations of a random variable.
The latter is exactly how SciPy distributions are implemented:</p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">norm</span>

<span class="c1"># Normal distribution can generate realizations of an RV</span>
<span class="c1"># The following returns a NumPy array of 10 draws</span>
<span class="c1"># from a standard normal distribution.</span>
<span class="n">norm</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
</code></pre></div>
<details class="note">
<summary>Realizations of a Random Variable</summary>
<p>A "realization" of a random variable is nothing more than
generating a random number
whose probability of being generated
is defined by the random variable's probability density function.</p>
</details>
<p>Because the generation of realizations of a random variable
is equivalent to sampling from a probability distribution,
we can extend our probability distribution definition
to include a <code>.sample(n)</code> method:</p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">norm</span>

<span class="k">class</span> <span class="nc">Normal</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mu</span> <span class="o">=</span> <span class="n">mu</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sigma</span> <span class="o">=</span> <span class="n">sigma</span>

        <span class="c1"># We instantiate the distribution object here.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dist</span> <span class="o">=</span> <span class="n">norm</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="n">mu</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">sigma</span><span class="p">)</span>

    <span class="c1"># ...</span>

    <span class="k">def</span> <span class="nf">sample</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">dist</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
</code></pre></div>
<p>Now, if we draw 10 realizations of a normally distributed random variable,
and the drawing of each realization has no dependence of any kind
on the previous draw,
then we can claim that each draw is <strong>independent</strong>
and <strong>identically distributed</strong>.
This is where the fabled "<em>iid</em>" term in undergraduate statistics classes
comes from.</p>
<h2 id="data-generating-process">Data Generating Process</h2>
<p>Now that we have covered what probability distributions are,
we can now move on to other concepts
that are important in Bayesian statistical modelling.</p>
<p>Realizations of a random variable,
or draws from its probability distribution,
are how a Bayesian assumes data are generated.
Describing how data are generated using probability distributions,
or in other words, writing down the "data generating process",
is a core activity in Bayesian statistical modelling.</p>
<p>Viewed this way, data values generated by a random process
depend on the underlying random variable's probability distribution.
In other words, the random variable realizations are known,
given the probability distribution used to model it.
Keep this idea in mind:
it is going to be important shortly.</p>
<h2 id="bayes-rule">Bayes' Rule</h2>
<p>Now that we've covered probability distributions,
we can move on to Bayes' rule.
You probably have seen the following equation:</p>
<div class="arithmatex">
<div class="MathJax_Preview">P(B|A) = \frac{P(A|B)P(B)}{P(A)}</div>
<script type="math/tex; mode=display">P(B|A) = \frac{P(A|B)P(B)}{P(A)}</script>
</div>
<p>Bayes' rule states nothing more than the fact that
the conditional probability of B given A is equal to
the conditional probability of A given B
times the probability of B
divided by the probability of A.</p>
<p>When doing Bayesian statistical inference,
we commonly take a related but distinct interpretation:</p>
<div class="arithmatex">
<div class="MathJax_Preview">P(H|D) = \frac{P(D|H)P(H)}{P(D)}</div>
<script type="math/tex; mode=display">P(H|D) = \frac{P(D|H)P(H)}{P(D)}</script>
</div>
<p>It may look weird,
but didn't we say before that data are realizations from a random variable?
Why are we now treating data as a random variable?
Here, we are doing not-so-intuitive but technically correct step
of treating the data <span class="arithmatex"><span class="MathJax_Preview">D</span><script type="math/tex">D</script></span> as being part of this probabilistic model
(hence it "looks" like a random variable),
alongside our model parameters <span class="arithmatex"><span class="MathJax_Preview">H</span><script type="math/tex">H</script></span>.
There's a lot of measure theory that goes into this interpretation,
which at this point I have not yet mastered,
and so will wave my hands in great arcs
and propose that this interpretation be accepted for now and move on.</p>
<details class="note">
<summary>Data are random variables?</summary>
<p>Notes from a chat with Colin gave me a lot to chew on, as usual:</p>
<blockquote>
<p>The answer is in how you define "event" as
"an element of a sigma algebra".
intuitively, an "event" is just an abstraction,
so one event might be "the coin is heads",
or in another context the event might be
"the parameters are [0.2, 0.1, 0.2]".
And so analogously, "the data were configured as [0, 5, 2, 3]".
Notice also that the events are different
if the data being ordered vs unordered are different!</p>
</blockquote>
<p>This was a logical leap that I had been asked about before,
but did not previously have the knowledge to respond to.
Thanks to Colin, I now do.</p>
</details>
<p>With the data + hypothesis interpretation of Bayes' rule in hand,
the next question arises:
What math happens when we calculate posterior densities?</p>
<h2 id="translating-bayes-math-to-python">Translating Bayes' Math to Python</h2>
<h3 id="defining-posterior-log-likelihood">Defining Posterior Log-Likelihood</h3>
<p>To understand this, let's look at the simplest complex example
that I could think of:
Estimating the <span class="arithmatex"><span class="MathJax_Preview">\mu</span><script type="math/tex">\mu</script></span> and <span class="arithmatex"><span class="MathJax_Preview">\sigma</span><script type="math/tex">\sigma</script></span> parameters
of a normal distribution
conditioned on observing data points <span class="arithmatex"><span class="MathJax_Preview">y</span><script type="math/tex">y</script></span>.</p>
<p>If we assume a data generating process that looks like the following
(with no probability distributions specified yet):</p>
<div class="mermaid">graph TD;
    (()) --&gt; y(y);
    (()) --&gt; y(y);</div>
<p>We can write out the following probabilistic model
(now explicitly specifying probability distributions):</p>
<div class="arithmatex">
<div class="MathJax_Preview">\mu \sim Normal(0, 10)</div>
<script type="math/tex; mode=display">\mu \sim Normal(0, 10)</script>
</div>
<div class="arithmatex">
<div class="MathJax_Preview">\sigma \sim Exponential(1)</div>
<script type="math/tex; mode=display">\sigma \sim Exponential(1)</script>
</div>
<div class="arithmatex">
<div class="MathJax_Preview">y \sim Normal(\mu, \sigma)</div>
<script type="math/tex; mode=display">y \sim Normal(\mu, \sigma)</script>
</div>
<p>Let's now map the symbols onto Bayes' rule.</p>
<ul>
<li><span class="arithmatex"><span class="MathJax_Preview">H</span><script type="math/tex">H</script></span> are the parameters, which are <span class="arithmatex"><span class="MathJax_Preview">\mu</span><script type="math/tex">\mu</script></span> and <span class="arithmatex"><span class="MathJax_Preview">\sigma</span><script type="math/tex">\sigma</script></span> here.</li>
<li><span class="arithmatex"><span class="MathJax_Preview">D</span><script type="math/tex">D</script></span> is the data that I will observe</li>
<li><span class="arithmatex"><span class="MathJax_Preview">P(H|D)</span><script type="math/tex">P(H|D)</script></span> is the posterior, which we would like to compute.</li>
<li><span class="arithmatex"><span class="MathJax_Preview">P(D|H)</span><script type="math/tex">P(D|H)</script></span> is the likelihood,
and is given by <span class="arithmatex"><span class="MathJax_Preview">y</span><script type="math/tex">y</script></span>'s probability distribution <span class="arithmatex"><span class="MathJax_Preview">Normal(\mu, \sigma)</span><script type="math/tex">Normal(\mu, \sigma)</script></span>,
or in probability notation, <span class="arithmatex"><span class="MathJax_Preview">P(y|\mu, \sigma)</span><script type="math/tex">P(y|\mu, \sigma)</script></span>.</li>
<li><span class="arithmatex"><span class="MathJax_Preview">P(H)</span><script type="math/tex">P(H)</script></span> is the the prior, and is given by <span class="arithmatex"><span class="MathJax_Preview">P(\mu, \sigma)</span><script type="math/tex">P(\mu, \sigma)</script></span>.</li>
<li><span class="arithmatex"><span class="MathJax_Preview">P(D)</span><script type="math/tex">P(D)</script></span> is a hard quantity to calculate, so we sort of cheat and don't use it,
and merely claim that the posterior is proportional to likelihood times prior.</li>
</ul>
<p>If we look at the probability symbols again,
we should notice that <span class="arithmatex"><span class="MathJax_Preview">P(\mu, \sigma)</span><script type="math/tex">P(\mu, \sigma)</script></span>
is the joint distribution between <span class="arithmatex"><span class="MathJax_Preview">\mu</span><script type="math/tex">\mu</script></span> and <span class="arithmatex"><span class="MathJax_Preview">\sigma</span><script type="math/tex">\sigma</script></span>.
However, from observing the graphical diagram,
we'll notice that <span class="arithmatex"><span class="MathJax_Preview">\mu</span><script type="math/tex">\mu</script></span> and <span class="arithmatex"><span class="MathJax_Preview">\sigma</span><script type="math/tex">\sigma</script></span> have no bearing on one another:
we do not need to know <span class="arithmatex"><span class="MathJax_Preview">\mu</span><script type="math/tex">\mu</script></span> to know the value of <span class="arithmatex"><span class="MathJax_Preview">\sigma</span><script type="math/tex">\sigma</script></span>,
and vice versa.
Hence, they are independent of one another,
and so by the rules of probability,</p>
<div class="arithmatex">
<div class="MathJax_Preview">P(\mu, \sigma) = P(\mu | \sigma)P(\sigma) = P(\mu)P(\sigma) = P(H)</div>
<script type="math/tex; mode=display">P(\mu, \sigma) = P(\mu | \sigma)P(\sigma) = P(\mu)P(\sigma) = P(H)</script>
</div>
<p>Now, by simply moving symbols around:</p>
<div class="arithmatex">
<div class="MathJax_Preview">P(H|D) = P(D|H)P(H)</div>
<script type="math/tex; mode=display">P(H|D) = P(D|H)P(H)</script>
</div>
<div class="arithmatex">
<div class="MathJax_Preview"> = P(y|\mu,\sigma)P(\mu, \sigma)</div>
<script type="math/tex; mode=display"> = P(y|\mu,\sigma)P(\mu, \sigma)</script>
</div>
<div class="arithmatex">
<div class="MathJax_Preview"> = P(y|\mu, \sigma)P(\mu)P(\sigma)</div>
<script type="math/tex; mode=display"> = P(y|\mu, \sigma)P(\mu)P(\sigma)</script>
</div>
<p>This translates directly into Python code!</p>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">model_prob</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="c1"># Probability of mu under prior.</span>
    <span class="n">normal_prior</span> <span class="o">=</span> <span class="n">Normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
    <span class="n">mu_prob</span> <span class="o">=</span> <span class="n">normal_prior</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">mu</span><span class="p">)</span>

    <span class="c1"># Probability of sigma under prior.</span>
    <span class="n">sigma_prior</span> <span class="o">=</span> <span class="n">Exponential</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">sigma_prob</span> <span class="o">=</span> <span class="n">sigma_prior</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">sigma</span><span class="p">)</span>

    <span class="c1"># Likelihood of data given mu and sigma</span>
    <span class="n">likelihood</span> <span class="o">=</span> <span class="n">Normal</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">)</span>
    <span class="n">likelihood_prob</span> <span class="o">=</span> <span class="n">likelihood</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">prod</span><span class="p">()</span>

    <span class="c1"># Joint likelihood</span>
    <span class="k">return</span> <span class="n">mu_prob</span> <span class="o">*</span> <span class="n">sigma_prob</span> <span class="o">*</span> <span class="n">likelihood_prob</span>
</code></pre></div>
<p>If you remember, multiplying so many probability distributions together
can give us underflow issues when computing,
so it is common to take the log of both sides.</p>
<div class="arithmatex">
<div class="MathJax_Preview">\log(P(H|D)) = log(P(y|\mu, \sigma)) + log(P(\mu)) + log(P(\sigma))</div>
<script type="math/tex; mode=display">\log(P(H|D)) = log(P(y|\mu, \sigma)) + log(P(\mu)) + log(P(\sigma))</script>
</div>
<p>This also translates directly into Python code!</p>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">model_log_prob</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="c1"># log-probability of mu under prior.</span>
    <span class="n">normal_prior</span> <span class="o">=</span> <span class="n">Normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
    <span class="n">mu_log_prob</span> <span class="o">=</span> <span class="n">normal_prior</span><span class="o">.</span><span class="n">logpdf</span><span class="p">(</span><span class="n">mu</span><span class="p">)</span>

    <span class="c1"># log-probability of sigma under prior.</span>
    <span class="n">sigma_prior</span> <span class="o">=</span> <span class="n">Exponential</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">sigma_log_prob</span> <span class="o">=</span> <span class="n">sigma_prior</span><span class="o">.</span><span class="n">logpdf</span><span class="p">(</span><span class="n">sigma</span><span class="p">)</span>

    <span class="c1"># log-likelihood given priors and data</span>
    <span class="n">likelihood</span> <span class="o">=</span> <span class="n">Normal</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">)</span>
    <span class="n">likelihood_log_prob</span> <span class="o">=</span> <span class="n">likelihood</span><span class="o">.</span><span class="n">logpdf</span><span class="p">(</span><span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>

    <span class="c1"># Joint log-likelihood</span>
    <span class="k">return</span> <span class="n">mu_log_prob</span> <span class="o">+</span> <span class="n">sigma_log_prob</span> <span class="o">+</span> <span class="n">likelihood_log_prob</span>
</code></pre></div>
<h2 id="computing-the-posterior-with-sampling">Computing the Posterior with Sampling</h2>
<p>To identify what the values of <span class="arithmatex"><span class="MathJax_Preview">\mu</span><script type="math/tex">\mu</script></span> and <span class="arithmatex"><span class="MathJax_Preview">\sigma</span><script type="math/tex">\sigma</script></span>
should take on given the data and priors,
we can turn to sampling to help us.
I am intentionally skipping over integrals
which are used to compute expectations,
which is what sampling is replacing.</p>
<h3 id="metropolis-hastings-sampling">Metropolis-Hastings Sampling</h3>
<p>An easy-to-understand sampler that we can start with
is the Metropolis-Hastings sampler.
I first learned it in a grad-level computational biology class,
but I expect most statistics undergrads should have
a good working knowledge of the algorithm.</p>
<p>For the rest of us, check out the note below on how the algorithm works.</p>
<details class="note" open="open">
<summary>The Metropolis-Hastings Algorithm</summary>
<p>Shamelessly copied (and modified)
from the <a href="">Wikipedia article</a>:</p>
<ul>
<li>For each parameter <span class="arithmatex"><span class="MathJax_Preview">p</span><script type="math/tex">p</script></span>, do the following.</li>
<li>Initialize an arbitrary point for the parameter (this is <span class="arithmatex"><span class="MathJax_Preview">p_t</span><script type="math/tex">p_t</script></span>, or <span class="arithmatex"><span class="MathJax_Preview">p</span><script type="math/tex">p</script></span> at step <span class="arithmatex"><span class="MathJax_Preview">t</span><script type="math/tex">t</script></span>).</li>
<li>Define a probability density <span class="arithmatex"><span class="MathJax_Preview">P(p_t)</span><script type="math/tex">P(p_t)</script></span>, for which we will draw new values of the parameters. Here, we will use <span class="arithmatex"><span class="MathJax_Preview">P(p) = Normal(p_{t-1}, 1)</span><script type="math/tex">P(p) = Normal(p_{t-1}, 1)</script></span>.</li>
<li>For each iteration:<ul>
<li>Generate candidate new candidate <span class="arithmatex"><span class="MathJax_Preview">p_t</span><script type="math/tex">p_t</script></span> drawn from <span class="arithmatex"><span class="MathJax_Preview">P(p_t)</span><script type="math/tex">P(p_t)</script></span>.</li>
<li>Calculate the likelihood of the data under the previous parameter value(s) <span class="arithmatex"><span class="MathJax_Preview">p_{t-1}</span><script type="math/tex">p_{t-1}</script></span>: <span class="arithmatex"><span class="MathJax_Preview">L(p_{t-1})</span><script type="math/tex">L(p_{t-1})</script></span></li>
<li>Calculate the likelihood of the data under the proposed parameter value(s) <span class="arithmatex"><span class="MathJax_Preview">p_t</span><script type="math/tex">p_t</script></span>: <span class="arithmatex"><span class="MathJax_Preview">L(p_t)</span><script type="math/tex">L(p_t)</script></span></li>
<li>Calculate acceptance ratio <span class="arithmatex"><span class="MathJax_Preview">r = \frac{L(p_t)}{L(p_{t-1})}</span><script type="math/tex">r = \frac{L(p_t)}{L(p_{t-1})}</script></span>.</li>
<li>Generate a new random number on the unit interval: <span class="arithmatex"><span class="MathJax_Preview">s \sim U(0, 1)</span><script type="math/tex">s \sim U(0, 1)</script></span>.</li>
<li>Compare <span class="arithmatex"><span class="MathJax_Preview">s</span><script type="math/tex">s</script></span> to <span class="arithmatex"><span class="MathJax_Preview">r</span><script type="math/tex">r</script></span>.<ul>
<li>If <span class="arithmatex"><span class="MathJax_Preview">s \leq r</span><script type="math/tex">s \leq r</script></span>, accept <span class="arithmatex"><span class="MathJax_Preview">p_t</span><script type="math/tex">p_t</script></span>.</li>
<li>If <span class="arithmatex"><span class="MathJax_Preview">s \gt r</span><script type="math/tex">s \gt r</script></span>, reject <span class="arithmatex"><span class="MathJax_Preview">p_t</span><script type="math/tex">p_t</script></span> and continue sampling again with <span class="arithmatex"><span class="MathJax_Preview">p_{t-1}</span><script type="math/tex">p_{t-1}</script></span>.</li>
</ul>
</li>
</ul>
</li>
</ul>
</details>
<p>In the algorithm described in the note above,
our parameters <span class="arithmatex"><span class="MathJax_Preview">p</span><script type="math/tex">p</script></span> are actually <span class="arithmatex"><span class="MathJax_Preview">(\mu, \sigma)</span><script type="math/tex">(\mu, \sigma)</script></span>.
This means that we have to propose two numbers
and sample two numbers in each loop of the sampler.</p>
<p>To make things simple for us, let's use the normal distribution
centered on <span class="arithmatex"><span class="MathJax_Preview">0</span><script type="math/tex">0</script></span> but with scale <span class="arithmatex"><span class="MathJax_Preview">0.1</span><script type="math/tex">0.1</script></span>
to propose values for each.</p>
<p>We can implement the algorithm in Python code:</p>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 1</span>
<span class="normal"> 2</span>
<span class="normal"> 3</span>
<span class="normal"> 4</span>
<span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="c1"># Metropolis-Hastings Sampling</span>
<span class="n">mu_prev</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">()</span>
<span class="n">sigma_prev</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">()</span>

<span class="c1"># Keep a history of the parameter values and ratio.</span>
<span class="n">mu_history</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
<span class="n">sigma_history</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
<span class="n">ratio_history</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1000</span><span class="p">):</span>
    <span class="n">mu_history</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">mu_prev</span>
    <span class="n">sigma_history</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">sigma_prev</span>
    <span class="n">mu_t</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">mu_prev</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">)</span>
    <span class="n">sigma_t</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">sigma_prev</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">)</span>

    <span class="c1"># Compute joint log likelihood</span>
    <span class="n">LL_t</span> <span class="o">=</span> <span class="n">model_log_prob</span><span class="p">(</span><span class="n">mu_t</span><span class="p">,</span> <span class="n">sigma_t</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">LL_prev</span> <span class="o">=</span> <span class="n">model_log_prob</span><span class="p">(</span><span class="n">mu_prev</span><span class="p">,</span> <span class="n">sigma_prev</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

    <span class="c1"># Calculate the difference in log-likelihoods</span>
    <span class="c1"># (or a.k.a. ratio of likelihoods)</span>
    <span class="n">diff_log_like</span> <span class="o">=</span> <span class="n">LL_t</span> <span class="o">-</span> <span class="n">LL_prev</span>
    <span class="k">if</span> <span class="n">diff_log_like</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">ratio</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># We need to exponentiate to get the correct ratio,</span>
        <span class="c1"># since all of our calculations were in log-space</span>
        <span class="n">ratio</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">diff_log_like</span><span class="p">)</span>

    <span class="c1"># Defensive programming check</span>
    <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">isinf</span><span class="p">(</span><span class="n">ratio</span><span class="p">)</span> <span class="ow">or</span> <span class="n">np</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">ratio</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;LL_t: </span><span class="si">{</span><span class="n">LL_t</span><span class="si">}</span><span class="s2">, LL_prev: </span><span class="si">{</span><span class="n">LL_prev</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="c1"># Ratio comparison step</span>
    <span class="n">ratio_history</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">ratio</span>
    <span class="n">p</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">ratio</span> <span class="o">&gt;=</span> <span class="n">p</span><span class="p">:</span>
        <span class="n">mu_prev</span> <span class="o">=</span> <span class="n">mu_t</span>
        <span class="n">sigma_prev</span> <span class="o">=</span> <span class="n">sigma_t</span>
</code></pre></div></td></tr></table></div>
<p>Because of a desire for convenience,
we chose to use a single normal distribution to sample all values.
However, that distribution choice is going to bite us during sampling,
because the values that we could possibly sample for the <span class="arithmatex"><span class="MathJax_Preview">\sigma</span><script type="math/tex">\sigma</script></span> parameter
can take on negatives,
but when a negative <span class="arithmatex"><span class="MathJax_Preview">\sigma</span><script type="math/tex">\sigma</script></span> is passed
into the normally-distributed likelihood,
we are going to get computation errors!
This is because the scale parameter of a normal distribution
can only be positive, and cannot be negative or zero.
(If it were zero, there would be no randomness.)</p>
<h3 id="transformations-as-a-hack">Transformations as a Hack</h3>
<p>The key problem here is that the support of the Exponential distribution
is bound to be positive real numbers only.
That said, we can get around this problem
simply by sampling amongst the unbounded real number space <span class="arithmatex"><span class="MathJax_Preview">(-\inf, +\inf)</span><script type="math/tex">(-\inf, +\inf)</script></span>,
and then transforming the number by a math function to be in the bounded space.</p>
<p>One way we can transform numbers from an unbounded space
to a positive-bounded space
is to use the exponential transform:</p>
<div class="arithmatex">
<div class="MathJax_Preview">y = e^x</div>
<script type="math/tex; mode=display">y = e^x</script>
</div>
<p>For any given value <span class="arithmatex"><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span>, <span class="arithmatex"><span class="MathJax_Preview">y</span><script type="math/tex">y</script></span> will be guaranteed to be positive.</p>
<p>Knowing this, we can modify our sampling code, specifically, what was before:</p>
<div class="highlight"><pre><span></span><code><span class="c1"># Initialize in unconstrained space</span>
<span class="n">sigma_prev_unbounded</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="c1"># ...</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1000</span><span class="p">):</span>
    <span class="c1"># ...</span>
    <span class="c1"># Propose in unconstrained space</span>
    <span class="n">sigma_t_unbounded</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">sigma_prev_unbounded</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">)</span>

    <span class="c1"># Transform the sampled values to the constrained space</span>
    <span class="n">sigma_prev</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">sigma_prev_unbounded</span><span class="p">)</span>
    <span class="n">sigma_t</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">sigma_t_unbounded</span><span class="p">)</span>

    <span class="c1"># ...</span>

    <span class="c1"># Pass the transformed values into the log-likelihood calculation</span>
    <span class="n">LL_t</span> <span class="o">=</span> <span class="n">model_log_prob</span><span class="p">(</span><span class="n">mu_t</span><span class="p">,</span> <span class="n">sigma_t</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">LL_prev</span> <span class="o">=</span> <span class="n">model_log_prob</span><span class="p">(</span><span class="n">mu_prev</span><span class="p">,</span> <span class="n">sigma_prev</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

    <span class="c1"># ...</span>
</code></pre></div>
<p>And <em>voila</em>!
If you notice, the key trick here was
to <strong>sample in unbounded space</strong>,
but <strong>evalute log-likelihood in bounded space</strong>.
We call the "unbounded" space the <em>transformed</em> space,
while the "bounded" space is the <em>original</em> or <em>untransformed</em> space.
We have implemented the necessary components
to compute posterior distributions on parameters!</p>
<h3 id="samples-from-posterior">Samples from Posterior</h3>
<p>If we simulate 1000 data points from a <span class="arithmatex"><span class="MathJax_Preview">Normal(3, 1)</span><script type="math/tex">Normal(3, 1)</script></span> distribution,
and pass them into the model log probability function defined above,
then after running the sampler,
we get a chain of values that the sampler has picked out
as maximizing the joint likelihood of the data and the model.
This, by the way, is essentially the simplest version of
Markov Chain Monte Carlo sampling that exists
in modern computational Bayesian statistics.</p>
<p>Let's examine the trace from one run:</p>
<p><img alt="" src="../comp-bayes-figures/mcmc-trace.png" /></p>
<p>Notice how it takes about 200 steps before the trace becomes <strong>stationary</strong>,
that is it becomes a flat trend-line.
If we prune the trace to just the values after the 200th iteration,
we get the following trace:</p>
<p><img alt="" src="../comp-bayes-figures/mcmc-trace-burn-in.png" /></p>
<p>The samples drawn are an approximation to
the expected values of <span class="arithmatex"><span class="MathJax_Preview">\mu</span><script type="math/tex">\mu</script></span> and <span class="arithmatex"><span class="MathJax_Preview">\sigma</span><script type="math/tex">\sigma</script></span>
given the data and priors specified.</p>
<details class="note" open="open">
<summary>Random Variables and Sampling</summary>
<p>A piece of wisdom directly quoted from my friend <a href="https://colindcarroll.com/">Colin Carroll</a>,
who is also a PyMC developer:</p>
<blockquote>
<p>Random variables are <em>measures</em>,
and measures are only really defined under an integral sign.
<em>Sampling</em> is usually defined as the act of generating data
according to a certain measure.
This is confusing, because we invert this relationship
when we do computational statistics:
we generate the data,
and use that to approximate an integral or expectation.</p>
</blockquote>
</details>
<h2 id="topics-we-skipped-over">Topics We Skipped Over</h2>
<p>We intentionally skipped over a number of topics.</p>
<p>One of them was why we used a normal distribution with scale of 0.1
to propose a different value, rather than a different scale.
As it turns out the, scale parameter is a tunable hyperparameter,
and in PyMC3 we do perform tuning as well.
If you want to learn more about how tuning happens,
<a href="https://colindcarroll.com/">Colin</a> has a <a href="https://colcarroll.github.io/hmc_tuning_talk/">great essay</a> on that too.</p>
<p>We also skipped over API design,
as that is a topic I will be exploring in a separate essay.
It will also serve as a tour through the PyMC3 API
as I understand it.</p>
<h2 id="an-anchoring-thought-framework-for-learning-computational-bayes">An Anchoring Thought Framework for Learning Computational Bayes</h2>
<p>Having gone through this exercise
has been extremely helpful in deciphering
what goes on behind-the-scenes in PyMC3
(and the in-development PyMC4,
which is built on top of TensorFlow probability).</p>
<p>From digging through everything from scratch,
my thought framework to think about Bayesian modelling
has been updated (pun intended) to the following.</p>
<p>Firstly, we can view a Bayesian model
from the axis of <strong>prior, likelihood, posterior</strong>.
Bayes' rule provides us the equation "glue"
that links those three components together.</p>
<p>Secondly, when doing <em>computational</em> Bayesian statistics,
we should be able to modularly separate <strong>sampling</strong>
from <strong>model definition</strong>.
<strong>Sampling</strong> is computing the posterior distribution of parameters
given the model and data.
<strong>Model definition</strong>, by contrast,
is all about providing the model structure
as well as a function that calculates the joint log likelihood
of the model and data.</p>
<p>In fact, based on the exercise above,
any "sampler" is only concerned with the model log probability
(though some also require the local gradient of the log probability
w.r.t. the parameters to find where to climb next),
and should only be required to accept a <strong>model log probability</strong> function
and a proposed set of initial parameter values,
and return a chain of sampled values.</p>
<p>Finally, I hope the "simplest complex example"
of estimating <span class="arithmatex"><span class="MathJax_Preview">\mu</span><script type="math/tex">\mu</script></span> and <span class="arithmatex"><span class="MathJax_Preview">\sigma</span><script type="math/tex">\sigma</script></span> of a normal distribution
helps further your understanding of the math behind Bayesian statistics.</p>
<p>All in all, I hope this essay helps your learning, as writing it did for me!</p>





                
              </article>
            </div>
          
          
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
        <div class="md-social">
  
    
    
    
    
      
      
    
    <a href="http://www.shortwhale.com/ericmjl" target="_blank" rel="noopener" title="www.shortwhale.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m20 8-8 5-8-5V6l8 5 8-5m0-2H4c-1.11 0-2 .89-2 2v12a2 2 0 0 0 2 2h16a2 2 0 0 0 2-2V6a2 2 0 0 0-2-2Z"/></svg>
    </a>
  
    
    
    
    
      
      
    
    <a href="https://github.com/ericmjl" target="_blank" rel="noopener" title="github.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.4.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
    </a>
  
    
    
    
    
      
      
    
    <a href="https://twitter.com/ericmjl" target="_blank" rel="noopener" title="twitter.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.4.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg>
    </a>
  
    
    
    
    
      
      
    
    <a href="https://linkedin.com/in/ericmjl" target="_blank" rel="noopener" title="linkedin.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.4.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"/></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    <script id="__config" type="application/json">{"base": "../..", "features": ["tabs"], "search": "../../assets/javascripts/workers/search.dfff1995.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../../assets/javascripts/bundle.dff1b7c8.min.js"></script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
      
        <script src="https://unpkg.com/mermaid@7.1.2/dist/mermaid.min.js"></script>
      
    
  </body>
</html>