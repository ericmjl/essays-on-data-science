
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
        <link rel="prev" href="../nngp/">
      
      
        <link rel="next" href="../llm-dev-guide/">
      
      
      <link rel="icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.5.2, mkdocs-material-9.2.8">
    
    
      
        <title>An Attempt at Demystifying Graph Deep Learning - Essays on Data Science</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.046329b4.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.85d0ee34.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="https://unpkg.com/mermaid@7.1.2/dist/mermaid.css">
    
      <link rel="stylesheet" href="../../css/ansi-colours.css">
    
      <link rel="stylesheet" href="../../css/jupyter-cells.css">
    
      <link rel="stylesheet" href="../../css/pandas-dataframe.css">
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="light-blue" data-md-color-accent="light-blue">
  
    
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#an-attempt-at-demystifying-graph-deep-learning" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="Essays on Data Science" class="md-header__button md-logo" aria-label="Essays on Data Science" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Essays on Data Science
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              An Attempt at Demystifying Graph Deep Learning
            
          </span>
        </div>
      </div>
    </div>
    
      
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        <a href="https://github.com/ericmjl/essays-on-data-science" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.4.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    ericmjl/essays-on-data-science
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="Essays on Data Science" class="md-nav__button md-logo" aria-label="Essays on Data Science" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    Essays on Data Science
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/ericmjl/essays-on-data-science" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.4.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    ericmjl/essays-on-data-science
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Home
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" >
        
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Computing
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            Computing
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../computing/recursion/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Recursion
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
    
  
  
    
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" checked>
        
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Machine Learning
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            Machine Learning
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../computational-bayesian-stats/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    An Introduction to Probability and Computational Bayesian Statistics
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../markov-models/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Markov Models From The Bottom Up, with Python
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../generating-markov-chains-dirichlet/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Dirichlet Processes and Hidden Markov Model Transition Matrices
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../message-passing/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Computational Representations of Message Passing
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../reimplementing-models/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Reimplementing and Testing Deep Learning Models
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../differential-computing-jax/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Differential Computing Explained
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../nngp/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Infinitely Wide Neural Networks
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  <span class="md-ellipsis">
    An Attempt at Demystifying Graph Deep Learning
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    An Attempt at Demystifying Graph Deep Learning
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#introduction" class="md-nav__link">
    Introduction
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#graphs" class="md-nav__link">
    Graphs
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#graphs-as-arrays" class="md-nav__link">
    Graphs as arrays
  </a>
  
    <nav class="md-nav" aria-label="Graphs as arrays">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#representing-nodes-as-arrays" class="md-nav__link">
    Representing nodes as arrays
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#representing-edges-as-arrays" class="md-nav__link">
    Representing edges as arrays
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#message-passing" class="md-nav__link">
    Message Passing
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#message-passing-neural-networks" class="md-nav__link">
    Message Passing + Neural Networks
  </a>
  
    <nav class="md-nav" aria-label="Message Passing + Neural Networks">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#message-passing-neural-networks-mpnns" class="md-nav__link">
    Message Passing Neural Networks (MPNNs)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#graph-laplacian-networks" class="md-nav__link">
    Graph Laplacian Networks
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#graph-attention-networks" class="md-nav__link">
    Graph Attention Networks
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#general-form-of-graph-neural-networks" class="md-nav__link">
    General form of graph neural networks
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mpnns-and-convolution-operations" class="md-nav__link">
    MPNNs and convolution operations
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#graph-learning-tasks" class="md-nav__link">
    Graph learning tasks
  </a>
  
    <nav class="md-nav" aria-label="Graph learning tasks">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#graph-level-prediction" class="md-nav__link">
    Graph-level prediction
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#node-labelling" class="md-nav__link">
    Node labelling
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#edge-presenceabsence-prediction" class="md-nav__link">
    Edge presence/absence prediction
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#summary" class="md-nav__link">
    Summary
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#with-thanks" class="md-nav__link">
    With Thanks
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#further-reading" class="md-nav__link">
    Further Reading
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../llm-dev-guide/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    A Developer-First Guide to LLM APIs
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" >
        
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Miscellaneous
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            Miscellaneous
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../miscellaneous/dashboarding-landscape/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    A Review of the Python Data Science Dashboarding Landscape in 2019
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../miscellaneous/learning-to-learn/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    How I Learned to Learn
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../miscellaneous/pydata-landscape/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    An Opinionated and Unofficial Guide to the PyData Ecosystem
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../miscellaneous/static-sites-on-dokku/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Static Sites and Apps On Your Own Dokku Server
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../miscellaneous/code-style-tools/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Code Style Tools
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5" >
        
          <label class="md-nav__link" for="__nav_5" id="__nav_5_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Software Skills
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5">
            <span class="md-nav__icon md-icon"></span>
            Software Skills
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../software-skills/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Importance
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../software-skills/code-formatting/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Formatting your code
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../software-skills/documentation/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Documenting your code
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../software-skills/environment-variables/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    A Data Scientist's Guide to Environment Variables
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../software-skills/refactoring/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Refactoring your code
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../software-skills/testing/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Testing your code
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6" >
        
          <label class="md-nav__link" for="__nav_6" id="__nav_6_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    People Skills
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_6">
            <span class="md-nav__icon md-icon"></span>
            People Skills
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../people-skills/hiring/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Hiring and Interviewing Data Scientists
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_7" >
        
          <label class="md-nav__link" for="__nav_7" id="__nav_7_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Terminal Hacks
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_7_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_7">
            <span class="md-nav__icon md-icon"></span>
            Terminal Hacks
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../terminal/cli-tools/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Tools and Upgrades for your CLI
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../terminal/pre-commits/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Using pre-commit git hooks to automate code checks
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_8" >
        
          <label class="md-nav__link" for="__nav_8" id="__nav_8_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Workflow
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_8_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_8">
            <span class="md-nav__icon md-icon"></span>
            Workflow
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../workflow/code-review/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Practicing Code Review
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../workflow/effective-commit-messages/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Effective Git Commits in Data Science
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../workflow/gitflow/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Principled Git-based Workflow in Collaborative Data Science Projects
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
    <li class="md-nav__item">
      <a href="../../supporters/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Supporters
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#introduction" class="md-nav__link">
    Introduction
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#graphs" class="md-nav__link">
    Graphs
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#graphs-as-arrays" class="md-nav__link">
    Graphs as arrays
  </a>
  
    <nav class="md-nav" aria-label="Graphs as arrays">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#representing-nodes-as-arrays" class="md-nav__link">
    Representing nodes as arrays
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#representing-edges-as-arrays" class="md-nav__link">
    Representing edges as arrays
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#message-passing" class="md-nav__link">
    Message Passing
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#message-passing-neural-networks" class="md-nav__link">
    Message Passing + Neural Networks
  </a>
  
    <nav class="md-nav" aria-label="Message Passing + Neural Networks">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#message-passing-neural-networks-mpnns" class="md-nav__link">
    Message Passing Neural Networks (MPNNs)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#graph-laplacian-networks" class="md-nav__link">
    Graph Laplacian Networks
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#graph-attention-networks" class="md-nav__link">
    Graph Attention Networks
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#general-form-of-graph-neural-networks" class="md-nav__link">
    General form of graph neural networks
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mpnns-and-convolution-operations" class="md-nav__link">
    MPNNs and convolution operations
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#graph-learning-tasks" class="md-nav__link">
    Graph learning tasks
  </a>
  
    <nav class="md-nav" aria-label="Graph learning tasks">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#graph-level-prediction" class="md-nav__link">
    Graph-level prediction
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#node-labelling" class="md-nav__link">
    Node labelling
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#edge-presenceabsence-prediction" class="md-nav__link">
    Edge presence/absence prediction
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#summary" class="md-nav__link">
    Summary
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#with-thanks" class="md-nav__link">
    With Thanks
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#further-reading" class="md-nav__link">
    Further Reading
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  

  
  


<h1 id="an-attempt-at-demystifying-graph-deep-learning">An attempt at demystifying graph deep learning</h1>
<h2 id="introduction">Introduction</h2>
<p>There are a ton of great explainers of what graph neural networks are.
However, I find that a lot of them go pretty deep into the math pretty quickly.
Yet, we still are faced with that age-old problem: where are all the pics??
As such, just as I had attempted with Bayesian deep learning,
I'd like to try to demystify graph deep learning as well,
using every tool I have at my disposal
to minimize the number of equations and maximize intuition using pictures.
Here's my attempt, I hope you find it useful!</p>
<h2 id="graphs">Graphs</h2>
<p>In my <a href="https://ericmjl.github.io/Network-Analysis-Made-Simple/index.html">Network Analysis Made Simple tutorial</a>,
we see that the term "graph" are really nothing more than
a synonym for networks.
Defined strictly, graphs are comprised of <strong>nodes</strong>, i.e.
entities, and <strong>edges</strong> that define <em>relations</em> between nodes.
Examples are social networks (nodes = people, edges = friendship),
and flight networks
(nodes = airports, edges = flights that exist between the two networks).</p>
<p>Pictorially, we'd usually draw something that looks like this:</p>
<p><img alt="" src="../graph-nets-figures/fig1-graphs.png" /></p>
<p>A graph <span class="arithmatex"><span class="MathJax_Preview">G</span><script type="math/tex">G</script></span>, in really concise mathematical notation,
can be represented as <span class="arithmatex"><span class="MathJax_Preview">G = (V, E)</span><script type="math/tex">G = (V, E)</script></span>,
or in plain English,
an unordered collection of vertices (a synonym for nodes)
and an unordered collection of edges.</p>
<h2 id="graphs-as-arrays">Graphs as <em>arrays</em></h2>
<p>One thing that's a really neat property of graphs is that we can actually represent them as <em>arrays</em>.
This is covered in the <a href="https://ericmjl.github.io/Network-Analysis-Made-Simple/04-advanced/02-linalg/">Linear Algebra section</a> of <a href="https://ericmjl.github.io/Network-Analysis-Made-Simple/index.html">Network Analysis Made Simple</a>;
I also wrote about this in the earlier sections of an essay I wrote on <a href="https://ericmjl.github.io/essays-on-data-science/machine-learning/message-passing/">message passing</a>
that I'd encourage you to check it out in its entirety,
but nonetheless here's a simplified version to introduce you to the key ideas.</p>
<p>Because graphs are comprised of nodes and edges,
we need to figure out a way of representing both of them as arrays.
Coming up, we'll explore how this can be done.</p>
<h3 id="representing-nodes-as-arrays">Representing nodes as arrays</h3>
<p>Let's start with nodes, our entities.
Our nodes, being entities, may have certain properties.
Let's use a concrete example of molecules,
which is what I would consider
a minimally complex example for exploring key ideas.
(I use minimally complex examples as my anchoring examples to learn an idea,
I hope this one is useful for you too!)</p>
<p>Imagine we have a molecule of ethanoic acid.
It's comprised of two carbons, two oxygen,
and four hydrogens joined in the following fashion:</p>
<p><img alt="" src="../graph-nets-figures/fig2-ethanoic-acid.png" /></p>
<p>Each atom is a node, and edges are bonds between the atoms.
Each atom, or node, carries <em>properties</em> or <em>features</em>
which can be represented on the array.
For example, there's the atomic mass
(a floating point number, or integer if you desire to round it off).
There's the valence of the atom as well.
And many more!
To keep things simple, I'll stick to just these two for now.
As you can see, you'll end up with a <em>vector of features</em> per node.
If we stack them up together,
we get what we call a node feature matrix, which will become handy later.</p>
<p><img alt="" src="../graph-nets-figures/fig3-ethanoic-acid-features.png" /></p>
<h3 id="representing-edges-as-arrays">Representing edges as arrays</h3>
<p>Similarly, there is an array representation of edges too!
If you imagine lining up all of the atoms
inside the ethanoic acid molecule along a square matrix,
you can fill in the square matrix
based on whether an edge exists between those two atoms.
It'll look something like this:</p>
<p><img alt="" src="../graph-nets-figures/fig4-ethanoic-acid-adjacency.png" /></p>
<p>If we wanted to, though, we could get really granular!
We could have on adjacency matrix that shows us all of the single bonds,
and another adjacency matrix that shows all of the double bonds.
Or we could have an adjacency matrix that is <em>weighted</em>
based on the number of bonds between two atoms.</p>
<p><img alt="" src="../graph-nets-figures/fig5-ethanoic-acid-weighted-adjacency.png" /></p>
<p>Those are all <em>perfectly valid representations</em> of the edges of a graph,
each with their own semantic meaning <em>and</em> tradeoffs
for whatever modelling problem you might have.</p>
<h2 id="message-passing">Message Passing</h2>
<p>It's now time to think about message passing.
Message passing is a very well-known operation
in the network science and machine learning worlds,
but you don't have to be intimidated about it!
At its core, it's actually really simple,
and you'll see that it totally has a linear algebra interpretation that,
in my opinion, is pretty elegant!</p>
<p>Let's start by defining what message passing on a graph is.</p>
<p>Message passing on a graph is kind of what you would intuit it to be:
if I have a message on a node,
we want to pass the message to other nodes on the graph.
Our message can only travel along the edges,
though - otherwise, the structure of the graph would be irrelevant.</p>
<p>Here's an example that is really, really simplified.
(We'll see the exact same ideas in action
on our ethanoic acid example in a short moment.)
This example is drawn from my tutorial on network analysis,
in particular, from the linear algebra section.</p>
<p>Let's say we have a directed chain graph of four nodes.
A "message" lives on the first node,
and we want to pass it through the graph.
In a first message passing iteration,
we take the message on the first node and pass it to the next one.
In the iteration after, we take the message and pass it to the next node.
So on and so forth until we reach the last node.
Here, there's no place the message can go;
regardless of how many more steps we might desire to run message passing,
the message won't go anywhere.</p>
<p>As it turns out, there's a linear algebra interpretation of message passing!
For the four node chain graph,
we can consider the message as being a property of a node.
For a node possessing the message,
or more generally a copy of the message,
we can assign it a value of 1.
If a node doesn't have a copy of the message,
then we assign it a value of 0.
Here's where the magic happens.
When we dot product the adjacency matrix against this message vector
(which is akin to our node feature vector),
the message will <em>move</em> from the first node to the next node.
(Verify this point for yourself by hand!)</p>
<p><img alt="" src="../graph-nets-figures/fig6-message-passing-chain.png" /></p>
<p>So it seems we can pass messages around nodes by simply doing dot products!
Let's see this in action by looking at the same operation in action
for ethanoic acid.
Here, all nodes receive messages from their neighbors simultaneously.
Notice how the graph looks like after performing message passing.
The carbon attached to three hydrogens has
a different value from the carbon attached to the oxygens;
their value is the sum of their neighbors.
"Summing up neighbors" is literally all that we are doing
in this form of message passing,
in other words,
when we do a dot product of the adjacency matrix with a node feature matrix.</p>
<p><img alt="" src="../graph-nets-figures/fig7-message-passing-ethanoic-acid.png" /></p>
<p>At this point, I'd encourage you to pause reading
and ponder over the ideas this far.
It was pretty mind-blowing for me when I first really made the connection
between message passing and linear algebra.
I spent a few good weeks mulling the ideas I just described.</p>
<h2 id="message-passing-neural-networks">Message Passing + Neural Networks</h2>
<p>We're now going to see how message passing
gets embedded inside a neural network -
a network that learns on network.
(Pretty meta, if you ask me!)</p>
<p>To anchor our understanding,
we will start with graph deep learning in a supervised learing setting,
where our learning task is to predict a scalar number
for every graph in a collection of graphs.
One classic example where this has been done before
is in chemical property prediction,
the first of which I encountered being a paper
by my deep learning teacher <a href="https://www.cs.toronto.edu/~duvenaud/">David Duvenaud</a>
on <a href="https://arxiv.org/abs/1509.09292">learning molecular fingerprints</a>.
Here, each input into the neural network is a <em>graph</em>,
rather than a <em>vector</em>.
For comparison, classical deep learning starts with rows of i.i.d. data
that are fed through a neural network.</p>
<p>We know that neural networks are composed of chains of math functions.
(Really, that's all neural network models are at their core!)
Feed forward neural networks chain together dot products;
convolutional neural networks add in n-dimensional convolutions in the mix;
meanwhile, recurrent neural networks have lag as part of its structure.
Each of these provide <em>inductive biases</em>.
By the way, that term, in my opinion,
is nothing more than a fancy way of saying that
we encode prior knowledge of our data generating process
into the mathematical model.
In each case, though, on a per-i.i.d. sample basis,
we pass in one n-dimensional array at a time.</p>
<p><img alt="" src="../graph-nets-figures/fig8-inductive-biases.png" /></p>
<p>To disambiguate a point, a friend of mine Jacob raised the point that
autoregressive time series are not i.i.d.,
for which he is definitely correct.
However, if the definition of a sample is not one measurement in the series
but one entire series,
then we can have a collection of i.i.d. autoregressive timeseries measurements.
My anchoring example is protein sequences;
you can have long-range dependencies between positions in a sequence,
so each position in a single protein sequence is not independent from others,
but you can have a collection of i.i.d. protein sequences nonetheless.</p>
<p>What about graph neural networks, then -
what exactly does the input look like,
and what is the general structure of such networks?
To answer those questions,
we're going to walk through
three different kinds of graph neural network layers,
the three that I'm most familiar with,
as specific examples of the general form of graph neural nets.
By the end of us walking through them,
we should have a much clearer grasp of the math that underlies these models.</p>
<h3 id="message-passing-neural-networks-mpnns">Message Passing Neural Networks (MPNNs)</h3>
<p>Message passing neural networks, as the name implies,
means we inject <em>message passing</em> as an operation in the neural network.
Let's see how we can define a message passing layer.
Instead of accepting just a vector or n-dimensional image,
the message passing layer accepts both the adjacency matrix
and the node feature matrix and performs one round of message passing.
But we know it's part of a <em>neural network</em>,
so where does the <em>neural</em> piece come in?
Well, right after message passing,
we take the message-passed node feature matrix
and apply the equivalent of a feed-forward neural network operation -
linear transform + some activation function!</p>
<p><img alt="" src="../graph-nets-figures/fig9-message-passing-layer.png" /></p>
<p>We can do this as many rounds as we desire -
though it's important to note that
we usually don't want to go for too many rounds.
The long story cut short is that
message passing has the effect of smoothing out node-level information,
which <em>could</em> become undesirable.</p>
<p>Once we have done one or two rounds of message passing,
it might be desirable to have a vector representation of our graphs.
But at the moment, we're stuck with two matrices -
the adjacency matrix and the messaged-passed node feature matrix!
Well, one canonical way of creating graph-level vector representations
is to simply take a node-wise summation (or average)
of the node feature matrix.
Summations express that the size of a graph matters,
while averages express that size should not matter.
What you choose will depend on your particular problem.
By the way, you as long as you have a reductive function
that takes the node feature matrix and reduces it to a vector,
you can satisfy the need to have a vector summary of a graph...
as long as you can explain the semantics :).
Regardless, let's call this layer a "graph summary layer".</p>
<p><img alt="" src="../graph-nets-figures/fig10-graph-summary-layer.png" /></p>
<p>Once we have a vector-level representation of our graph,
we can stick classical feed forward neural network layers
on top of the graph processing layers
to obtain the thing we're trying to predict.</p>
<p><img alt="" src="../graph-nets-figures/fig11-gnn-anatomy.png" /></p>
<p>And that, my friends, is how a classical message passing neural network works!
Here's the framework I use to remember how GNNs work:</p>
<ol>
<li>Message passing operations, followed by</li>
<li>Graph summarization operations, followed by</li>
<li>Feed forward networks to predict output.</li>
</ol>
<p>In the broader machine learning framework of "model, loss, optimizer",
all we've worked on here is to design a <em>model</em>
that fits the kind of data that we have.
You get to keep constant the <em>loss</em> and the <em>optimizer</em>.
Keep this framework memorized,
and use MPNNs as the anchoring minimally complex example,
and everything else we discuss coming will become crystal clear!</p>
<h3 id="graph-laplacian-networks">Graph Laplacian Networks</h3>
<p>So what's up with graph laplacian networks?
Well, really, at its core is all that's been done here
is to replace each graph's adjacency matrix with their graph Laplacian matrix.
That's it!
Every other operation that you see in the pictures above remain the same.</p>
<p>Edit: As Petar Veličković, a research scientist at DeepMind
kindly pointed out to me on Twitter,
in practice there are fancier forms of the graph Laplacian matrix
that get used inside GNNs.
Please see his tweet below.</p>
<blockquote class="twitter-tweet"><p lang="en" dir="ltr">A great overview with very nice figures! Thank you!<br><br>A comment on &#39;graph Laplacian networks&#39; -- their core ideas stretch way deeper than just replacing A with L, imho. Perhaps a better way to phrase it is that such methods _in practice_ often just end up as products with poly(L).</p>&mdash; Petar Veličković (@PetarV_93) <a href="https://twitter.com/PetarV_93/status/1422819126809088000?ref_src=twsrc%5Etfw">August 4, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<p>But what is the graph Laplacian?
At its core, it's a measure of the local <em>derivative</em> of the graph,
and we use it when we want to express that
local differences in nodes matter to our graph-level property of interest.
To explain this idea here any further
would distract from our exploration of GNNs,
so I would refer you to <a href="https://en.wikipedia.org/wiki/Laplacian_matrix">the Wikipedia entry</a>
for those of you who desire a deeper understanding
of this category of graph matrices.</p>
<h3 id="graph-attention-networks">Graph Attention Networks</h3>
<p>What about graph attention networks, then?
Well, we know that the attention mechanism
is nothing more than an operation that uses
a neural network inside a neural network to learn weights of <em>some</em> kind.
With graph attention networks,
we use an embedded neural network to learn
a per-graph adjacency-like matrix operator
that we mask over with the adjacency matrix,
which effectively gives us an edge attention matrix for message passing.
(If you're feeling antsy and want to read an actual paper on this,
check out the <a href="https://arxiv.org/abs/1710.10903">graph attention networks paper</a> paper.)</p>
<p>And that's it! Everything else about the model can stay constant.</p>
<p>Graph attention layers give us the most general message,
data-driven passing operator that we can imagine.
Rather than fix the message passing operator <em>a priori</em>,
we simply learn the operator in a data-driven fashion.</p>
<h3 id="general-form-of-graph-neural-networks">General form of graph neural networks</h3>
<p>Okay, so we've explored three examples of graph neural networks.
What exactly is general here? Here's the answer:</p>
<ol>
<li>We must have a message passing operator, <span class="arithmatex"><span class="MathJax_Preview">A</span><script type="math/tex">A</script></span>.
It can be learned or it can be provided <em>a priori</em>
and remain fixed throughout the neural network.
You basically have the freedom to define any form of <span class="arithmatex"><span class="MathJax_Preview">A</span><script type="math/tex">A</script></span> that you need!</li>
<li>We need to have node feature matrices, <span class="arithmatex"><span class="MathJax_Preview">F</span><script type="math/tex">F</script></span>.</li>
<li>For convenience,
we usually transform our graphs' array representations <span class="arithmatex"><span class="MathJax_Preview">A</span><script type="math/tex">A</script></span> and <span class="arithmatex"><span class="MathJax_Preview">F</span><script type="math/tex">F</script></span>
into a summary vector that then gets processed
by a feed forward neural network.</li>
</ol>
<p>Again, what remains relatively constant is the <em>structure</em> of the model -
some form of generalized message passing
followed by some form of graph summarization followed by a top model.
As long as you have some sort of semantically-meaningful square matrix <span class="arithmatex"><span class="MathJax_Preview">A</span><script type="math/tex">A</script></span>
and a semantically-meaningful node feature matrix <span class="arithmatex"><span class="MathJax_Preview">F</span><script type="math/tex">F</script></span>,
you can follow the message passing -&gt; summarization -&gt; feed forward structure
to build your GNN models.</p>
<h2 id="mpnns-and-convolution-operations">MPNNs and convolution operations</h2>
<p>When David taught me about graph neural networks,
one idea really clicked:
how message passing generalizes the grid convolution to graphs -
which is why the "graph convolution" term shows up
in the deep learning literature.
Let's explore how this is the case
by looking carefully at a simple grid convolution
and a simple graph convolution.</p>
<p>Starting off with the simplest grid convolution,
let's say we have a 5x5 pixel image with a 3x3 convolution operator
comprised of <span class="arithmatex"><span class="MathJax_Preview">1</span><script type="math/tex">1</script></span>s everywhere except the middle cell.
When using the convolution operation (with certain border settings),
we get the following result:</p>
<p><img alt="" src="../graph-nets-figures/fig12-grid-convolution.png" /></p>
<p>We actually can re-represent the grid of values as a
lattice graph and do a message passing operation to obtain the same result:</p>
<p><img alt="" src="../graph-nets-figures/fig13-graph-convolution.png" /></p>
<p>And in doing so,
we thus can see that the message passing operation
using the adjacency matrix as our message passing operator
is equivalent to a convolution
with a filter of 1s surrounding a zero.
Message passing on graphs more generally simply requires that
we relax the condition that our graph be a lattice graph!</p>
<h2 id="graph-learning-tasks">Graph learning tasks</h2>
<p>Thus far, we explored how graph neural networks work
in the context of a supervised machine learning problem
where we want to build a model that links graphs as inputs
to a property of the entire graph.
Beyond this single learning task, though, there are other learning tasks.
From a talk that my friend <a href="https://www.linkedin.com/in/chris522229197/">Chris Lin</a> delivered,
I learned that there are a few primary categories of graph learning tasks.
Here's a short overview of those learning tasks.</p>
<h3 id="graph-level-prediction">Graph-level prediction</h3>
<p>This is the category of problem we leveraged above
to explore the internals of graph neural networks.
Basically we frame the problem as "given graph, predict number".
It assumes that we have a bunch of i.i.d.
data that have a natural graph structure as its representation.</p>
<h3 id="node-labelling">Node labelling</h3>
<p>This category of problems typically deals
with one large graph and its adjacency matrix,
and our desired output is a label (or a suite of labels)
that need to be annotated on nodes.
Some of the nodes have labels while others don't;
message passing could be handy here
because it encodes the <em>homophily</em> assumption -
that is, for problems where we expect
that similar things will be linked together,
we can encode this inductive bias into our models
much more naturally than other structural equations.</p>
<h3 id="edge-presenceabsence-prediction">Edge presence/absence prediction</h3>
<p>This is another class of problems where usually we are given one large graph,
and the desired output is either whether
an edge ought to exist between two nodes.
This can be framed in terms of predicting
the entries of the graph adjacency matrix, for example.</p>
<h2 id="summary">Summary</h2>
<p>Here, we started with array representations of single graphs.
Then, we covered the message passing operation,
which is a key operation in graph neural networks.
We explored how we can compose message passing, graph summarization,
and feed forward neural networks to do graph-level property prediction.
Finally, we did a quick tour through the general categories
of graph learning tasks in which graph neural networks are used.</p>
<p>It's taken me 2-3 months of procrastination, brainstorming,
and more to write all of this down, and even then,
I doubt I have comprehensively covered
all of the fundamentals of graph deep learning.
That said, I believe that in this essay,
we've given ourselves a great starting point -
you might even say, a launchpad -
for delving into the vast swathe of literature out there.</p>
<h2 id="with-thanks">With Thanks</h2>
<p>I would like to give special thanks to my Patreon supporters,
Alejandro, Rafael, Fazal, Brian, Hector, Carol, and Eddie.
With the new kid, switching roles, and more of life hitting me in the face,
it's taken a bit of time for me to get this essay out,
but I'm finally glad to have it done!</p>
<h2 id="further-reading">Further Reading</h2>
<ul>
<li>I wrote an essay on <a href="https://ericmjl.github.io/essays-on-data-science/machine-learning/message-passing/">how to efficiently represent message passing</a>.</li>
<li>Others have also made attempts at explainin GNNs, including <a href="https://theaisummer.com/graph-convolutional-networks/">The AI Summer</a>.</li>
<li>The <a href="https://github.com/GRAND-Lab/Awesome-Graph-Neural-Networks">Awesome GNN Repository</a>.</li>
</ul>
<p>(The literature is expanding way too quickly; it's going to be hard to keep up!)</p>





                
              </article>
            </div>
          
          
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
        <div class="md-social">
  
    
    
    
    
      
      
    
    <a href="http://www.shortwhale.com/ericmjl" target="_blank" rel="noopener" title="www.shortwhale.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m20 8-8 5-8-5V6l8 5 8-5m0-2H4c-1.11 0-2 .89-2 2v12a2 2 0 0 0 2 2h16a2 2 0 0 0 2-2V6a2 2 0 0 0-2-2Z"/></svg>
    </a>
  
    
    
    
    
      
      
    
    <a href="https://github.com/ericmjl" target="_blank" rel="noopener" title="github.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.4.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
    </a>
  
    
    
    
    
      
      
    
    <a href="https://twitter.com/ericmjl" target="_blank" rel="noopener" title="twitter.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.4.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg>
    </a>
  
    
    
    
    
      
      
    
    <a href="https://linkedin.com/in/ericmjl" target="_blank" rel="noopener" title="linkedin.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.4.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"/></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    <script id="__config" type="application/json">{"base": "../..", "features": ["tabs"], "search": "../../assets/javascripts/workers/search.dfff1995.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../../assets/javascripts/bundle.dff1b7c8.min.js"></script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
      
        <script src="https://unpkg.com/mermaid@7.1.2/dist/mermaid.min.js"></script>
      
    
  </body>
</html>