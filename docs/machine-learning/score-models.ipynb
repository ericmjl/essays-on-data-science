{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Score Models\n",
    "\n",
    "Generative models of data are all the rage in the machine learning world.\n",
    "Being a person embedded deeply in both the biology and machine learning fields,\n",
    "one really cool application of generative models\n",
    "is to be able to generate novel biological sequences.\n",
    "To do so, we can turn to a variety of model classes,\n",
    "such as models that leverage autoregression (recurrent neural networks, RNNs),\n",
    "models that use a cat-and-mouse game of trickery \n",
    "to learn the data generating distribution (generative adversarial networks, GANs),\n",
    "and models that try to approximate the data generating distribution\n",
    "using a latent distribution (variational autoencoders, VAEs).\n",
    "\n",
    "Even back in the day when I was still relatively untrained in probabilistic modelling,\n",
    "RNNs, GANs, and VAEs all felt a bit mystical.\n",
    "The reasons? I actually couldn't pin them down back then.\n",
    "Fast-forward a few years, though, \n",
    "and having been embedded in the world of probabilistic modelling and Bayes,\n",
    "the reasons are much clearer.\n",
    "I'd like to explore one collection of work,\n",
    "based heavily of Yang Song's blog post on [score models][score]\n",
    "(this is also his PhD thesis topic),\n",
    "and share what I think is the core of that idea.\n",
    "\n",
    "[score]: https://yang-song.github.io/blog/2021/score/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probability distributions and generative models\n",
    "\n",
    "From what I have seen, the whole premise of score models \n",
    "is to deal with data that are drawn from unknown data-generating distributions.\n",
    "How do we generate new data that looks like existing data\n",
    "when we don't know what the underlying data generating distribution is?\n",
    "That's the key question that score-based models attempt to answer.\n",
    "\n",
    "Let's consider the case of generating new data\n",
    "when we know the data-generating distribution.\n",
    "To anchor our understanding, \n",
    "we'll use what I consider to be the simplest complex example for this topic:\n",
    "a Gaussian distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we have to understand that all probability distributions,\n",
    "including the Gaussian distribution,\n",
    "are capable of generating new data.\n",
    "(In my other essay, [An Introduction to Probability and Computational Bayesian Statistics][compstats]\n",
    "we go over the anatomy of a probability distribution in detail,\n",
    "and I would recommend referencing that essay.)\n",
    "Generating new data is also called \"drawing samples\".\n",
    "As such, probability distributions can be considered a \"data generator\".\n",
    "\n",
    "[compstats]: ./computational-bayesian-stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, all continuous probability distributions have a **probability density function** (PDF).\n",
    "The PDF is a function that controls the propensity of a distribution\n",
    "to draw values on the support of the distribution.\n",
    "For a Gaussian, values around the mean have the highest propensity to be drawn,\n",
    "while values far away from the mean have the lowest propensity to be drawn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| echo: false\n",
    "import jax.numpy as np \n",
    "from jax.scipy.stats import norm\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "x = np.linspace(-3, 3, 1000)\n",
    "y = norm.pdf(x)\n",
    "plt.plot(x, y)\n",
    "plt.xlabel(\"Support\")\n",
    "plt.ylabel(\"Likelihood\")\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, note that the PDF, \n",
    "which returns the likelihood of observing any point on the support,\n",
    "is nothing more than a math function.\n",
    "That means we can take its logarithm and obtain the \"log PDF\"\n",
    "(from here onwards I'll call it the \"logp\" function).\n",
    "The logp function is useful in many settings,\n",
    "not least in computational settings where multiplying likelihoods together\n",
    "can be transformed to adding log-likelihoods.\n",
    "For the Gaussian above, it looks like this:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "y = norm.logpdf(x)\n",
    "plt.plot(x, y)\n",
    "plt.xlabel(\"Support\")\n",
    "plt.ylabel(\"Log Likelihood\")\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logp function is also differentiable!\n",
    "That means we can take its derivative easily (using JAX, for example).\n",
    "This gives us what we call a **score function**.\n",
    "The derivative of a logp function is the score function of a PDF.\n",
    "To help remember, think of the following chain:\n",
    "\n",
    "```text\n",
    "PDF (likelihood) --> logPDF (log likelihood) --> dlogPDF (score)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| echo: false\n",
    "from jax.scipy.stats import norm\n",
    "from jax import grad, vmap\n",
    "\n",
    "fig, axes = plt.subplots(figsize=(8, 4), nrows=1, ncols=2, sharex=True, sharey=True)\n",
    "\n",
    "x = np.linspace(-3, 3)\n",
    "y = norm.logpdf(x)\n",
    "\n",
    "plt.sca(axes[0])\n",
    "plt.plot(x, y, color=\"black\")\n",
    "plt.ylabel(\"logP(x)\")\n",
    "plt.title(\"Log Likelihood\")\n",
    "\n",
    "plt.sca(axes[1])\n",
    "x_pt = -1.5\n",
    "y_pt = norm.logpdf(x_pt)\n",
    "\n",
    "\n",
    "def line(x):\n",
    "    return grad(norm.logpdf)(x_pt) * (x - x_pt) + y_pt\n",
    "xrange = np.linspace(x_pt - 1, x_pt + 1, 10)\n",
    "plt.plot(x, y, color=\"black\")\n",
    "plt.scatter(x_pt, y_pt, color=\"gray\")\n",
    "plt.plot(xrange, vmap(line)(xrange), color=\"gray\", ls=\"--\")\n",
    "plt.title(\"Score\")\n",
    "\n",
    "sns.despine()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, one thing we know about gradients is that \n",
    "they point to us the direction in which to move on the x-axis\n",
    "in order to go upwards on the y-axis.\n",
    "So if $\\frac{d\\log{P(x)}}{dx} = 1$, \n",
    "then it means we need to move in the positive direction \n",
    "in order to approach the maxima of $d\\log{P}$.\n",
    "And if $\\frac{d\\log{P(x)}}{dx} = -2$, \n",
    "then it means we need to move in the negative direction \n",
    "in order to approach the maxima of $d\\log{P}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we know the data generating distribution's log PDF equation,\n",
    "with a bit of calculus, we can easily derive the distribution's score function too.\n",
    "However, what hapens when we _don't_ know the data generating distribution's log PDF?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating the score function\n",
    "\n",
    "As it turns out, there is [a paper][hyvarinen]\n",
    "published by Aapo Hyv√§rinen in the 2005 in the Journal of Machine Learning Research\n",
    "that details how to _estimate_ the score function\n",
    "in the absence of knowledge of the true data generating distribution.\n",
    "When I first heard of the idea, I thought it was crazy --\n",
    "crazy cool that we could even do this!\n",
    "\n",
    "[hyvarinen]: https://jmlr.csail.mit.edu/papers/volume6/hyvarinen05a/old.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One key equation in the paper is equation #4.\n",
    "This equation details how we can use an arbitrary function, $\\psi(x, \\theta)$,\n",
    "to approximate the score function,\n",
    "and the loss function needed to train the parameters of the function $\\theta$\n",
    "to approximate the score function.\n",
    "I've replicated the equation below,\n",
    "alongside a bullet-point explanation of what each of the terms are:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$J(\\theta) = \\frac{1}{T} \\sum_{t=1}^{T} \\sum_{i=1}^{n} [\\delta_i \\psi_i(x(t); \\theta) + \\frac{1}{2} \\psi_i(x(t); \\theta)^2 ] + \\text{const}$$\n",
    "\n",
    "Here:\n",
    "\n",
    "- $J(\\theta)$ is the loss function that we wish to minimize w.r.t. the parameters $\\theta$\n",
    "- $\\theta$ are the parameters of the function $\\psi_i$\n",
    "- $\\psi_i(x(t); \\theta)$ are the score functions for each dimension $i$ in $x$\n",
    "- $x(t)$ are the i.i.d. samples from the unknown data-generating distribution.\n",
    "- $\\delta_i$ refers to the partial derivative w.r.t. dimension $i$ in $x$.\n",
    "- $\\text{const}$ is a constant term that effectively can be ignored.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can implement this loss function in JAX!\n",
    "This is how it would look like for a univariate distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import grad, vmap\n",
    "from functools import partial \n",
    "\n",
    "def score_matching_loss(params, score_func, batch):\n",
    "    score_func = partial(score_func, params)\n",
    "    dscore_func = grad(score_func)\n",
    "\n",
    "    term1 = vmap(dscore_func)(batch)\n",
    "    term2 = (0.5 * vmap(score_func)(batch) ** 2)\n",
    "\n",
    "    inner_term = term1 + term2\n",
    "    return np.mean(inner_term).squeeze()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And more generally, for multivariate data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import jacfwd \n",
    "from typing import Callable \n",
    "\n",
    "def score_matching_loss(params, score_func: Callable, batch: np.ndarray) -> float:\n",
    "    \"\"\"Score matching loss function.\n",
    "\n",
    "    This is taken from (Hyv√§rinen, 2005) (JMLR)\n",
    "    and https://yang-song.github.io/blog/2019/ssm/.\n",
    "\n",
    "    :param params: The parameters to the score function.\n",
    "    :param score_func: Score function with signature `func(params, batch)`,\n",
    "        which returns a scalar.\n",
    "    :param batch: A batch of data. Should be of shape (batch, :),\n",
    "        where `:` refers to at least 1 more dimension.\n",
    "    :returns: Score matching loss, a float.\n",
    "    \"\"\"\n",
    "    score_func = partial(score_func, params)\n",
    "    dscore_func = jacfwd(score_func)\n",
    "\n",
    "    # Jacobian of score function (i.e. dlogp estimator function).\n",
    "    # This is also the Hessian (2nd derivative) of the logp.\n",
    "    # The Jacobian shape is: `(i, i)`,\n",
    "    # where `i` is the number of dimensions of the input data,\n",
    "    # or the number of random variables.\n",
    "    # Here, we want the diagonals instead, which is of shape (i,)\n",
    "    term1 = vmap(dscore_func)(batch)\n",
    "    term1 = vmap(np.diagonal)(term1)\n",
    "\n",
    "    # Discretized integral of score function.\n",
    "    term2 = 0.5 * vmap(score_func)(batch) ** 2\n",
    "    term2 = np.reshape(term2, term1.shape)\n",
    "\n",
    "    # Summation over the inner term, by commutative property of addition,\n",
    "    # automagically gives us the trace of the Jacobian of the score function.\n",
    "    # Yang Song's blog post refers to the trace\n",
    "    # (final equation in the section\n",
    "    # \"Learning unnormalized models with score matching\"),\n",
    "    # while Hyv√§rinen's JMLR paper uses an explicit summation in Equation 4.\n",
    "    inner_term = term1 + term2\n",
    "    summed_by_dims = vmap(np.sum)(inner_term)\n",
    "    return np.mean(summed_by_dims)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b7410733da85e48a7d98359cb1662001d6b4cc6f90e43427714d358e9ea2f663"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 ('essays-on-data-science')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
