{"config":{"lang":["en"],"separator":"[\\s\\-\\.]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Essays on Data Science","text":"<p>In which I put together my thoughts on the practice of data science.</p> <p>This is a curated and edited collection of my blog posts, as well as essays specially written for the broader Python community.</p>"},{"location":"#support-this-project","title":"Support this project","text":"<p>If you find this collection of essays useful, please star the repository on GitHub!</p> <p>If you enjoyed this essay collection, please support me on Patreon! It's a tangible way of telling me that you appreciate my work and find it valuable.</p> <p>Also, I have a free monthly newsletter that I use as an outlet to share programming-oriented data science tips and tools. If you'd like to receive it, sign up on Substack!</p>"},{"location":"supporters/","title":"A big thank you","text":"<p>...to my Patreon supporters!</p> <ol> <li>Eddie</li> <li>Carol</li> <li>Hector</li> <li>Fazal</li> <li>Rafael</li> <li>Robert</li> <li>Sat</li> <li>Daniel</li> <li>Ahmed</li> </ol> <p>Your support keeps me caffeinated, so I can continue to make educational material for the data science and Python communities!</p>"},{"location":"computing/recursion/","title":"Recursion","text":"<p>Recursion is an incredibly useful concept to know. To be clear, it is distinct from looping, but is related. I think it's helpful for data scientists to have recursion as a programming trick in their back pocket. In this essay, let's take an introductory look at recursion, and where it can come in handy.</p>"},{"location":"computing/recursion/#what-recursion-looks-like","title":"What recursion looks like","text":"<p>Recursion happens when we have a function that calls itself by default or else returns a result when some stopping criteria is reached.</p> <p>A classic example of recursion is in finding the root of a tree from a given node. Here, we essentially want to follow every node's predecessor until we reach a node that has no predecessor.</p> <p>In code form, this looks something like this:</p> <pre><code>def find_root(G, n):\n    predecessor = G.predecessor(n)\n    if G.predecessor(n):\n        return find_root(G, predecessor)\n    else:\n        return n\n</code></pre> <p>Generally, we first compute something on the basis of the inputs (line 2). This is usually some form of finding a new substitute input on which we can check a condition (lines 4 and 6). Under one condition, we return the function call with a new input, and under another condition, we return the desired output.</p>"},{"location":"computing/recursion/#why-you-would-use-recursion","title":"Why you would use recursion","text":"<p>Recursion is essentially a neat way to write a loop concisely, and can be useful, say, under circumstances where we do not know the exact number of loop iterations needed before we encounter the stopping condition.</p> <p>While I do find recursion useful in certain applied settings, I will also clarify that I don't use recursion on a daily basis. As such, I recommend this as a back-pocket trick that one should have, but won't necessarily use all the time.</p>"},{"location":"computing/recursion/#where-recursion-shows-up-in-a-real-life-situation","title":"Where recursion shows up in a real-life situation","text":"<p>I can speak to one situation at work where I was benchmarking some deep neural network models, and also testing hyperparameters on a grid. There, I used YAML files to keep track of parameters and experiments, and in order to keep things concise, I implemented a very lightweight YAML inheritance scheme, where I would have a master \"template\" experiment, but use child YAML files that inherited from the \"master\" template in which certain parts of the experiment parameters were changed. (An example might be one where the master template specified the use of the Adam optimizer with a particular learning rate, while the child templates simply modified the learning rate.)</p> <p>As the experiments got deeper and varied more parameters, things became more tree-like, and so I had to navigate the parameter tree from the child templates up till the root template, which by definition had no parents. After finding the root template, I could then travel back down from the root template, iteratively updating the parameters until I reached the child template of interest.</p> <p>The more general scenario to look out for is in graph traversal problems. If your problem can be cast in terms of a graph data structure that you need to program your computer to take a walk over, then that is a prime candidate for trying your hand at recursion.</p>"},{"location":"machine-learning/computational-bayesian-stats/","title":"An Introduction to Probability and Computational Bayesian Statistics","text":"<p>In Bayesian statistics, we often say that we are \"sampling\" from a posterior distribution to estimate what parameters could be, given a model structure and data. What exactly is happening here?</p> <p>Examples that I have seen on \"how sampling happens\" tends to focus on an overly-simple example of sampling from a single distribution with known parameters. I was wondering if I could challenge myself to come up with a \"simplest complex example\" that would illuminate ideas that were obscure to me before. In this essay, I would like to share that knowledge with you, and hopefully build up your intuition behind what is happening in computational Bayesian inference.</p>"},{"location":"machine-learning/computational-bayesian-stats/#probability-distributions","title":"Probability Distributions","text":"<p>We do need to have a working understanding of what a probability distribution is before we can go on. Without going down deep technical and philosophical rabbit holes (I hear they are deep), I'll start by proposing that \"a probability distribution is a Python object that has a math function that allocates credibility points onto the number line\".</p> <p>Because we'll be using the normal distribution extensively in this essay, we'll start off by examining that definition in the context of the standard normal distribution.</p>"},{"location":"machine-learning/computational-bayesian-stats/#base-object-implementation","title":"Base Object Implementation","text":"<p>Since the normal distribution is an object, I'm implying here that it can hold state. What might that state be? Well, we know from math that probability distributions have parameters, and that the normal distribution has the \"mean\" and \"variance\" parameters defined. In Python code, we might write it as:</p> <pre><code>class Normal:\n    def __init__(self, mu, sigma):\n        self.mu = mu\n        self.sigma = sigma\n</code></pre>"},{"location":"machine-learning/computational-bayesian-stats/#probability-density-function","title":"Probability Density Function","text":"<p>Now, I also stated that the normal distribution has a math function that we can use to allocate credibility points to the number line. This function also has a name, called a \"probability density function\", or the \"PDF\". Using this, we may then extend extend this object with a method called <code>.pdf(x)</code>, that returns a number giving the number of credibility points assigned to the value of <code>x</code> passed in.</p> <pre><code>import numpy as np\n\nclass Normal:\n    def __init__(self, mu, sigma):\n        self.mu = mu\n        self.sigma = sigma\n\n    def pdf(self, x):\n        return (\n            1 / np.sqrt(2 * self.sigma ** 2 * np.pi)\n            * np.exp(\n                - (x - self.mu) ** 2\n                / 2 * self.sigma ** 2\n            ))\n</code></pre> <p>If we pass in a number <code>x</code> from the number line, we will get back another number that tells us the number of credibility points given to that value <code>x</code>, under the state of the normal distribution instantiated. We'll call this P(x).</p> <p>To simplify the implementation used here, we are going to borrow some machinery already available to us in the Python scientific computing ecosystem, particularly from the SciPy stats module, which gives us reference implementations of probability distributions.</p> <pre><code>from scipy.stats import norm\n\nclass Normal:\n    def __init__(self, mu, sigma):\n        self.mu = mu\n        self.sigma = sigma\n\n        # We instantiate the distribution object here.\n        self.dist = norm(loc=mu, scale=sigma)\n\n    def pdf(self, x):\n        # Now, our PDF class method is simplified to be just a wrapper.\n        return self.dist.pdf(x)\n</code></pre>"},{"location":"machine-learning/computational-bayesian-stats/#log-probability","title":"Log Probability","text":"<p>A common task in Bayesian inference is computing the likelihood of data. Let's assume that the data {X_1, X_2, ... X_i} generated are independent and identically distributed, (the famous i.i.d. term comes from this). This means, then, that the joint probability of the data that was generated is equivalent to the product of the individual probabilities of each datum:</p> P(X_1, X_2, ... X_i) = P(X_1) P(X_2) ... P(X_i) <p>(We have to know the rules of probability to know this result; it is a topic for a different essay.)</p> <p>If you remember the notation above, each P(X_i) is an evaluation of X_i on the distribution's probability density function. It being a probability value means it is bound between 0 and 1. However, multiplying many probabilities together usually will result in issues with underflow computationally, so in evaluating likelihoods, we usually stick with log-likelihoods instead. By the usual rules of math, then:</p> \\log P(X_1, X_2, ..., X_i) = \\sum_{j=1}^{i}\\log P(X_i) <p>To our normal distribution class, we can now add in another class method that computes the sum of log likelihoods evaluated at a bunch of i.i.d. data points.</p> <pre><code>from scipy.stats import norm\n\nclass Normal:\n    def __init__(self, mu, sigma):\n        self.mu = mu\n        self.sigma = sigma\n\n        # We instantiate the distribution object here.\n        self.dist = norm(loc=mu, scale=sigma)\n\n    def pdf(self, x):\n        # Now, our PDF class method is simplified to be just a wrapper.\n        return self.dist.pdf(x)\n\n    def logpdf(self, x):\n        return self.dist.logpdf(x)\n</code></pre>"},{"location":"machine-learning/computational-bayesian-stats/#random-variables","title":"Random Variables","text":""},{"location":"machine-learning/computational-bayesian-stats/#definition","title":"Definition","text":"<p>Informally, a \"random variable\" is nothing more than a variable whose quantity is non-deterministic (hence random) but whose probability of taking on a certain value can be described by a probability distribution.</p> <p>According to the Wikipedia definition of a random variable:</p> <p>A random variable has a probability distribution, which specifies the probability of its values.</p> <p>As such, it may be tempting to conceive of a random variable as an object that has a probability distribution attribute attached to it.</p>"},{"location":"machine-learning/computational-bayesian-stats/#realizations-of-a-random-variable","title":"Realizations of a Random Variable","text":"<p>On the other hand, it can also be convenient to invert that relationship, and claim that a probability distribution can generate realizations of a random variable. The latter is exactly how SciPy distributions are implemented:</p> <pre><code>from scipy.stats import norm\n\n# Normal distribution can generate realizations of an RV\n# The following returns a NumPy array of 10 draws\n# from a standard normal distribution.\nnorm(loc=0, scale=1).rvs(10)\n</code></pre> Realizations of a Random Variable <p>A \"realization\" of a random variable is nothing more than generating a random number whose probability of being generated is defined by the random variable's probability density function.</p> <p>Because the generation of realizations of a random variable is equivalent to sampling from a probability distribution, we can extend our probability distribution definition to include a <code>.sample(n)</code> method:</p> <pre><code>from scipy.stats import norm\n\nclass Normal:\n    def __init__(self, mu, sigma):\n        self.mu = mu\n        self.sigma = sigma\n\n        # We instantiate the distribution object here.\n        self.dist = norm(loc=mu, scale=sigma)\n\n    # ...\n\n    def sample(self, n):\n        return self.dist.rvs(n)\n</code></pre> <p>Now, if we draw 10 realizations of a normally distributed random variable, and the drawing of each realization has no dependence of any kind on the previous draw, then we can claim that each draw is independent and identically distributed. This is where the fabled \"iid\" term in undergraduate statistics classes comes from.</p>"},{"location":"machine-learning/computational-bayesian-stats/#data-generating-process","title":"Data Generating Process","text":"<p>Now that we have covered what probability distributions are, we can now move on to other concepts that are important in Bayesian statistical modelling.</p> <p>Realizations of a random variable, or draws from its probability distribution, are how a Bayesian assumes data are generated. Describing how data are generated using probability distributions, or in other words, writing down the \"data generating process\", is a core activity in Bayesian statistical modelling.</p> <p>Viewed this way, data values generated by a random process depend on the underlying random variable's probability distribution. In other words, the random variable realizations are known, given the probability distribution used to model it. Keep this idea in mind: it is going to be important shortly.</p>"},{"location":"machine-learning/computational-bayesian-stats/#bayes-rule","title":"Bayes' Rule","text":"<p>Now that we've covered probability distributions, we can move on to Bayes' rule. You probably have seen the following equation:</p> P(B|A) = \\frac{P(A|B)P(B)}{P(A)} <p>Bayes' rule states nothing more than the fact that the conditional probability of B given A is equal to the conditional probability of A given B times the probability of B divided by the probability of A.</p> <p>When doing Bayesian statistical inference, we commonly take a related but distinct interpretation:</p> P(H|D) = \\frac{P(D|H)P(H)}{P(D)} <p>It may look weird, but didn't we say before that data are realizations from a random variable? Why are we now treating data as a random variable? Here, we are doing not-so-intuitive but technically correct step of treating the data D as being part of this probabilistic model (hence it \"looks\" like a random variable), alongside our model parameters H. There's a lot of measure theory that goes into this interpretation, which at this point I have not yet mastered, and so will wave my hands in great arcs and propose that this interpretation be accepted for now and move on.</p> Data are random variables? <p>Notes from a chat with Colin gave me a lot to chew on, as usual:</p> <p>The answer is in how you define \"event\" as \"an element of a sigma algebra\". intuitively, an \"event\" is just an abstraction, so one event might be \"the coin is heads\", or in another context the event might be \"the parameters are [0.2, 0.1, 0.2]\". And so analogously, \"the data were configured as [0, 5, 2, 3]\". Notice also that the events are different if the data being ordered vs unordered are different!</p> <p>This was a logical leap that I had been asked about before, but did not previously have the knowledge to respond to. Thanks to Colin, I now do.</p> <p>With the data + hypothesis interpretation of Bayes' rule in hand, the next question arises: What math happens when we calculate posterior densities?</p>"},{"location":"machine-learning/computational-bayesian-stats/#translating-bayes-math-to-python","title":"Translating Bayes' Math to Python","text":""},{"location":"machine-learning/computational-bayesian-stats/#defining-posterior-log-likelihood","title":"Defining Posterior Log-Likelihood","text":"<p>To understand this, let's look at the simplest complex example that I could think of: Estimating the \\mu and \\sigma parameters of a normal distribution conditioned on observing data points y.</p> <p>If we assume a data generating process that looks like the following (with no probability distributions specified yet):</p> graph TD;     \u03bc((\u03bc)) --&gt; y(y);     \u03c3((\u03c3)) --&gt; y(y); <p>We can write out the following probabilistic model (now explicitly specifying probability distributions):</p> \\mu \\sim Normal(0, 10) \\sigma \\sim Exponential(1) y \\sim Normal(\\mu, \\sigma) <p>Let's now map the symbols onto Bayes' rule.</p> <ul> <li>H are the parameters, which are \\mu and \\sigma here.</li> <li>D is the data that I will observe</li> <li>P(H|D) is the posterior, which we would like to compute.</li> <li>P(D|H) is the likelihood, and is given by y's probability distribution Normal(\\mu, \\sigma), or in probability notation, P(y|\\mu, \\sigma).</li> <li>P(H) is the the prior, and is given by P(\\mu, \\sigma).</li> <li>P(D) is a hard quantity to calculate, so we sort of cheat and don't use it, and merely claim that the posterior is proportional to likelihood times prior.</li> </ul> <p>If we look at the probability symbols again, we should notice that P(\\mu, \\sigma) is the joint distribution between \\mu and \\sigma. However, from observing the graphical diagram, we'll notice that \\mu and \\sigma have no bearing on one another: we do not need to know \\mu to know the value of \\sigma, and vice versa. Hence, they are independent of one another, and so by the rules of probability,</p> P(\\mu, \\sigma) = P(\\mu | \\sigma)P(\\sigma) = P(\\mu)P(\\sigma) = P(H) <p>Now, by simply moving symbols around:</p> P(H|D) = P(D|H)P(H)  = P(y|\\mu,\\sigma)P(\\mu, \\sigma)  = P(y|\\mu, \\sigma)P(\\mu)P(\\sigma) <p>This translates directly into Python code!</p> <pre><code>def model_prob(mu, sigma, y):\n    # Probability of mu under prior.\n    normal_prior = Normal(0, 10)\n    mu_prob = normal_prior.pdf(mu)\n\n    # Probability of sigma under prior.\n    sigma_prior = Exponential(1)\n    sigma_prob = sigma_prior.pdf(sigma)\n\n    # Likelihood of data given mu and sigma\n    likelihood = Normal(mu, sigma)\n    likelihood_prob = likelihood.pdf(y).prod()\n\n    # Joint likelihood\n    return mu_prob * sigma_prob * likelihood_prob\n</code></pre> <p>If you remember, multiplying so many probability distributions together can give us underflow issues when computing, so it is common to take the log of both sides.</p> \\log(P(H|D)) = log(P(y|\\mu, \\sigma)) + log(P(\\mu)) + log(P(\\sigma)) <p>This also translates directly into Python code!</p> <pre><code>def model_log_prob(mu, sigma, y):\n    # log-probability of mu under prior.\n    normal_prior = Normal(0, 10)\n    mu_log_prob = normal_prior.logpdf(mu)\n\n    # log-probability of sigma under prior.\n    sigma_prior = Exponential(1)\n    sigma_log_prob = sigma_prior.logpdf(sigma)\n\n    # log-likelihood given priors and data\n    likelihood = Normal(mu, sigma)\n    likelihood_log_prob = likelihood.logpdf(y).sum()\n\n    # Joint log-likelihood\n    return mu_log_prob + sigma_log_prob + likelihood_log_prob\n</code></pre>"},{"location":"machine-learning/computational-bayesian-stats/#computing-the-posterior-with-sampling","title":"Computing the Posterior with Sampling","text":"<p>To identify what the values of \\mu and \\sigma should take on given the data and priors, we can turn to sampling to help us. I am intentionally skipping over integrals which are used to compute expectations, which is what sampling is replacing.</p>"},{"location":"machine-learning/computational-bayesian-stats/#metropolis-hastings-sampling","title":"Metropolis-Hastings Sampling","text":"<p>An easy-to-understand sampler that we can start with is the Metropolis-Hastings sampler. I first learned it in a grad-level computational biology class, but I expect most statistics undergrads should have a good working knowledge of the algorithm.</p> <p>For the rest of us, check out the note below on how the algorithm works.</p> The Metropolis-Hastings Algorithm <p>Shamelessly copied (and modified) from the Wikipedia article:</p> <ul> <li>For each parameter p, do the following.</li> <li>Initialize an arbitrary point for the parameter (this is p_t, or p at step t).</li> <li>Define a probability density P(p_t), for which we will draw new values of the parameters. Here, we will use P(p) = Normal(p_{t-1}, 1).</li> <li>For each iteration:<ul> <li>Generate candidate new candidate p_t drawn from P(p_t).</li> <li>Calculate the likelihood of the data under the previous parameter value(s) p_{t-1}: L(p_{t-1})</li> <li>Calculate the likelihood of the data under the proposed parameter value(s) p_t: L(p_t)</li> <li>Calculate acceptance ratio r = \\frac{L(p_t)}{L(p_{t-1})}.</li> <li>Generate a new random number on the unit interval: s \\sim U(0, 1).</li> <li>Compare s to r.<ul> <li>If s \\leq r, accept p_t.</li> <li>If s \\gt r, reject p_t and continue sampling again with p_{t-1}.</li> </ul> </li> </ul> </li> </ul> <p>In the algorithm described in the note above, our parameters p are actually (\\mu, \\sigma). This means that we have to propose two numbers and sample two numbers in each loop of the sampler.</p> <p>To make things simple for us, let's use the normal distribution centered on 0 but with scale 0.1 to propose values for each.</p> <p>We can implement the algorithm in Python code:</p> <pre><code># Metropolis-Hastings Sampling\nmu_prev = np.random.normal()\nsigma_prev = np.random.normal()\n\n# Keep a history of the parameter values and ratio.\nmu_history = dict()\nsigma_history = dict()\nratio_history = dict()\n\nfor i in range(1000):\n    mu_history[i] = mu_prev\n    sigma_history[i] = sigma_prev\n    mu_t = np.random.normal(mu_prev, 0.1)\n    sigma_t = np.random.normal(sigma_prev, 0.1)\n\n    # Compute joint log likelihood\n    LL_t = model_log_prob(mu_t, sigma_t, y)\n    LL_prev = model_log_prob(mu_prev, sigma_prev, y)\n\n    # Calculate the difference in log-likelihoods\n    # (or a.k.a. ratio of likelihoods)\n    diff_log_like = LL_t - LL_prev\n    if diff_log_like &gt; 0:\n        ratio = 1\n    else:\n        # We need to exponentiate to get the correct ratio,\n        # since all of our calculations were in log-space\n        ratio = np.exp(diff_log_like)\n\n    # Defensive programming check\n    if np.isinf(ratio) or np.isnan(ratio):\n        raise ValueError(f\"LL_t: {LL_t}, LL_prev: {LL_prev}\")\n\n    # Ratio comparison step\n    ratio_history[i] = ratio\n    p = np.random.uniform(0, 1)\n\n    if ratio &gt;= p:\n        mu_prev = mu_t\n        sigma_prev = sigma_t\n</code></pre> <p>Because of a desire for convenience, we chose to use a single normal distribution to sample all values. However, that distribution choice is going to bite us during sampling, because the values that we could possibly sample for the \\sigma parameter can take on negatives, but when a negative \\sigma is passed into the normally-distributed likelihood, we are going to get computation errors! This is because the scale parameter of a normal distribution can only be positive, and cannot be negative or zero. (If it were zero, there would be no randomness.)</p>"},{"location":"machine-learning/computational-bayesian-stats/#transformations-as-a-hack","title":"Transformations as a Hack","text":"<p>The key problem here is that the support of the Exponential distribution is bound to be positive real numbers only. That said, we can get around this problem simply by sampling amongst the unbounded real number space (-\\inf, +\\inf), and then transforming the number by a math function to be in the bounded space.</p> <p>One way we can transform numbers from an unbounded space to a positive-bounded space is to use the exponential transform:</p> y = e^x <p>For any given value x, y will be guaranteed to be positive.</p> <p>Knowing this, we can modify our sampling code, specifically, what was before:</p> <pre><code># Initialize in unconstrained space\nsigma_prev_unbounded = np.random.normal(0, 1)\n# ...\nfor i in range(1000):\n    # ...\n    # Propose in unconstrained space\n    sigma_t_unbounded = np.random.normal(sigma_prev_unbounded, 0.1)\n\n    # Transform the sampled values to the constrained space\n    sigma_prev = np.exp(sigma_prev_unbounded)\n    sigma_t = np.exp(sigma_t_unbounded)\n\n    # ...\n\n    # Pass the transformed values into the log-likelihood calculation\n    LL_t = model_log_prob(mu_t, sigma_t, y)\n    LL_prev = model_log_prob(mu_prev, sigma_prev, y)\n\n    # ...\n</code></pre> <p>And voila! If you notice, the key trick here was to sample in unbounded space, but evalute log-likelihood in bounded space. We call the \"unbounded\" space the transformed space, while the \"bounded\" space is the original or untransformed space. We have implemented the necessary components to compute posterior distributions on parameters!</p>"},{"location":"machine-learning/computational-bayesian-stats/#samples-from-posterior","title":"Samples from Posterior","text":"<p>If we simulate 1000 data points from a Normal(3, 1) distribution, and pass them into the model log probability function defined above, then after running the sampler, we get a chain of values that the sampler has picked out as maximizing the joint likelihood of the data and the model. This, by the way, is essentially the simplest version of Markov Chain Monte Carlo sampling that exists in modern computational Bayesian statistics.</p> <p>Let's examine the trace from one run:</p> <p></p> <p>Notice how it takes about 200 steps before the trace becomes stationary, that is it becomes a flat trend-line. If we prune the trace to just the values after the 200th iteration, we get the following trace:</p> <p></p> <p>The samples drawn are an approximation to the expected values of \\mu and \\sigma given the data and priors specified.</p> Random Variables and Sampling <p>A piece of wisdom directly quoted from my friend Colin Carroll, who is also a PyMC developer:</p> <p>Random variables are measures, and measures are only really defined under an integral sign. Sampling is usually defined as the act of generating data according to a certain measure. This is confusing, because we invert this relationship when we do computational statistics: we generate the data, and use that to approximate an integral or expectation.</p>"},{"location":"machine-learning/computational-bayesian-stats/#topics-we-skipped-over","title":"Topics We Skipped Over","text":"<p>We intentionally skipped over a number of topics.</p> <p>One of them was why we used a normal distribution with scale of 0.1 to propose a different value, rather than a different scale. As it turns out the, scale parameter is a tunable hyperparameter, and in PyMC3 we do perform tuning as well. If you want to learn more about how tuning happens, Colin has a great essay on that too.</p> <p>We also skipped over API design, as that is a topic I will be exploring in a separate essay. It will also serve as a tour through the PyMC3 API as I understand it.</p>"},{"location":"machine-learning/computational-bayesian-stats/#an-anchoring-thought-framework-for-learning-computational-bayes","title":"An Anchoring Thought Framework for Learning Computational Bayes","text":"<p>Having gone through this exercise has been extremely helpful in deciphering what goes on behind-the-scenes in PyMC3 (and the in-development PyMC4, which is built on top of TensorFlow probability).</p> <p>From digging through everything from scratch, my thought framework to think about Bayesian modelling has been updated (pun intended) to the following.</p> <p>Firstly, we can view a Bayesian model from the axis of prior, likelihood, posterior. Bayes' rule provides us the equation \"glue\" that links those three components together.</p> <p>Secondly, when doing computational Bayesian statistics, we should be able to modularly separate sampling from model definition. Sampling is computing the posterior distribution of parameters given the model and data. Model definition, by contrast, is all about providing the model structure as well as a function that calculates the joint log likelihood of the model and data.</p> <p>In fact, based on the exercise above, any \"sampler\" is only concerned with the model log probability (though some also require the local gradient of the log probability w.r.t. the parameters to find where to climb next), and should only be required to accept a model log probability function and a proposed set of initial parameter values, and return a chain of sampled values.</p> <p>Finally, I hope the \"simplest complex example\" of estimating \\mu and \\sigma of a normal distribution helps further your understanding of the math behind Bayesian statistics.</p> <p>All in all, I hope this essay helps your learning, as writing it did for me!</p>"},{"location":"machine-learning/differential-computing-jax/","title":"Differential Computing with JAX","text":"<p>The proliferation of differential computing tooling has been one of the biggest contributions from the deep learning world in the 2010-2020 decade. In this essay, I want to write about JAX, which in my personal opinion has made the biggest strides forward for interoperable differential computing in the Python data science world.</p>"},{"location":"machine-learning/differential-computing-jax/#on-differential-computing","title":"On differential computing","text":"<p>Before we go on, we must first ask: what is differential computing all about?</p> <p>I think at its core, we can think of differential computing as \"computing where derivatives (from calculus) are a first-class citizen\". This is analogous to probabilistic computing, in which probabilistic constructs (such as probability distributions and MCMC samplers) are given \"first class\" status in a language. By \"first class\" status, I mean that the relevant computing constructs are a well-developed category of constructs in the language, with clearly defined interfaces.</p> <p>In differential computing, being able to evaluate gradients of a math function are at the heart of the language. (How the gradients are used is a matter of application.) The objective of a differential computing system is to write a program that gets a computer to automatically evaluate gradients of a math function that we have written in that language. This is where automatic differentiation (AD) systems come into play. They take a program, perhaps written as a Python function, and automatically transform the program into, perhaps, another Python function, which can be used to evaluate the derivative of the original function.</p> <p>All of this falls under the paradigm of AD systems, as I mentioned earlier. Symbolic differentiation can be considered one subclass of AD systems; this is a point PyMC3 developer Brandon Willard would make. Packages such as <code>autograd</code> and now JAX (also by the <code>autograd</code> authors) are another subclass of AD systems, which leverage the chain rule and a recorder tape of math operations called in the program to automatically construct the gradient function.</p>"},{"location":"machine-learning/differential-computing-jax/#where-differential-computing-gets-used","title":"Where differential computing gets used","text":"<p>In case you've found yourself living like a digital hermit for the past decade (no judgment, sometimes I do fantasize about going offline for a year), deep learning has been the place where automatic differentiation has been most utilized. With deep learning, the core technical problem that needs to be solved is optimizing parameters of a model to minimize some loss function. It's here where the full set of partial derivatives of the loss function w.r.t. each parameter in the model can be automatically calculated using an AD system, and these partial derivatives can be used to update their respective model parameters in the direction that minimizes loss.</p> <p>Because deep learning models and their applications proliferated in the 2010-2020 decade, AD systems were most commonly associated with neural networks and deep learning. However, that is not the only place where AD systems show up.</p> <p>For example, AD is used in the Bayesian statistical modelling world. Hamiltonian Monte Carlo samplers use AD to help the sampler program identify the direction in which its next MCMC step should be taken. AD systems can also be used to optimize parameters of non-neural network models of the world against data, such as Gaussian Mixture Models and Hidden Markov Models. We can even use AD in a class of problems called \"input design\" problems, where we try to optimize not the parameters of the model w.r.t. some output, but the inputs (assuming we know how to cast the inputs into some continuous numerical space.)</p>"},{"location":"machine-learning/differential-computing-jax/#the-landscape-of-ad-systems","title":"The landscape of AD systems","text":"<p>So where do AD systems live? Firstly, they definitely live inside deep learning frameworks such as PyTorch and TensorFlow, and other deep learning frameworks. Without an AD system, these two deep learning frameworks would not work.</p> <p>Secondly, they also live in independent packages. In Julia, there are two AD packages: one called <code>Zygote.jl</code>, and the other called <code>AutoGrad.jl</code>; both of them are actively developed. <code>autograd</code>, which was the reference Python package that <code>AutoGrad.jl</code> was written against, is also the precursor to JAX, which I think of as automatic differentiation on steroids.</p>"},{"location":"machine-learning/differential-computing-jax/#what-makes-jax-special","title":"What makes JAX special?","text":"<p>With all of that said, the focus of this essay is JAX. I wanted to bring a bit of focus to JAX as I think its developers have been doing all the right things thus far in its development, and I wanted to highlight these as reasons why you might want to use JAX in your next project.</p>"},{"location":"machine-learning/differential-computing-jax/#api-compatible-differentiable-computing","title":"API-compatible differentiable computing","text":"<p>The Python scientific computing stack, also known as the PyData or SciPy stack in the Python world, provides a large library of numerical programs that can be composed together into higher order programs. What JAX provides is a fully API-compatible reimplementation of the stack's differentiable functions, with what I think is a near-complete coverage of functions.</p> <p>As such, users familiar with NumPy and SciPy can, with minimal changes to lines of code, write automatically differentiable versions of their existing programs, and develop new programs that are also automatically differentiable.</p> <p>How does this work? I have written a longer-form collection of teaching materials on this, but here is a quick example. If I have a silly program like the following one:</p> <pre><code>from jax.scipy.special import cholesky\nimport jax.numpy as np\n\ndef some_function(params, data):\n    # do stuff with params, for example, a cholesky decomposition:\n    U = cholesky(params)\n    # followed by a sum of sine transform, for whatever reason it might be needed\n    return np.sum(np.sin(U))\n</code></pre> <p>I can get the gradient function easily, using JAX's provided <code>grad</code> function:</p> <pre><code>from jax import grad\n\ndsome_function = grad(some_function)\n</code></pre> <p><code>dsome_function</code> has the same function signature as <code>some_function</code>, but instead of returning a scalar value, it returns the derivative of <code>some_function</code> w.r.t. <code>params</code> (the first argument), in the same (and possibly nested) data structure as <code>params</code>. That is to say, if <code>params</code> were a tuple, or a dictionary, or a list, or any other native Python construct, <code>dsome_function</code> would return the same structure.</p> <p><code>grad</code> can do many more fancy things, such as differentiating through loops and flow control, and second through nth-order derivatives, and I'd encourage you to check out the docs to learn more. (That is out of scope for this essay, as I'm focusing on high level points.)</p> <p>Providing <code>grad</code> as a first-class citizen in an API-compatible fashion with the scientific Python computing stack makes it very easy to adopt differential computing tooling in one's programs.</p>"},{"location":"machine-learning/differential-computing-jax/#api-compatibility-with-the-rest-of-the-pydata-stack","title":"API-compatibility with the rest of the PyData stack","text":"<p>A design choice made early on by the JAX developers was full NumPy and SciPy API compatibility, with minimal differences (mostly in the realm of random number generation) that are very well-documented. Incidentally, this practice is also adopted by Dask and CuPy, which give us distributed and GPU-backed arrays respectively. This practice reflects a healthy dose of respect for what already exists and for end-users as well.</p> <p>I think a contrasting example best illustrates this point. Consider a PyTorch or TensorFlow array vs. a JAX NumPy array.</p> <p>To plot a PyTorch array's values in <code>matplotlib</code>, one must first convert it to a NumPy array:</p> <pre><code>a = torch.tensor([1, 2, 3])\nplt.plot(a.numpy())\n</code></pre> <p>On the other hand, with JAX:</p> <pre><code>import jax.numpy as np\na = np.array([1, 2, 3])\nplt.plot(a)\n</code></pre> <p>The syntax with JAX is identical to what one might write with vanilla NumPy. This is, I think, in part because JAX's developers have also strived to adhere to NEP18. Small API differences introduce micro-friction in programming, which compound frustration over time; JAX effectively eliminates that friction by adhering to an existing API and not playing smartypants with it.</p>"},{"location":"machine-learning/differential-computing-jax/#composable-program-transforms","title":"Composable program transforms","text":"<p>JAX's <code>grad</code> function is merely the \"gateway drug\" to this bigger idea of \"composable program transforms\". <code>grad</code> is one example of a composable program transform: that is transforming one program into another in a composable fashion. Other transforms include <code>vmap</code>, <code>lax.scan</code>, <code>jit</code>, and more. These all accept Python functions and return Python functions. <code>jit</code>, in particular, can accelerate a program anywhere from 2-100 fold on a CPU, depending on what your reasonable baseline comparison is.</p> <p>In particular, the latter three of the aformentioned transforms allow for highly performant loop code, written without loops, that can also be composed together. In this differential learning workshop that I have been developing, I provide further details in there, which you can take a look at.</p> <p>There are other automatic program transformations that are in active development, and one exciting realm I see is in the probabilistic programming world. In PyMC3, for example, an automated transform happens when we take our PyMC3 syntax, which is written in a domain specific language (DSL) implemented in Python, and transform/compile it into a compute graph that gives us a likelihood function. It's as if PyMC3 gives us a <code>likelihood(func)</code> analogy to JAX's <code>grad</code> func. If you've tried writing probabilistic model likelihoods by hand, you'll know how much of a convenience this is!</p>"},{"location":"machine-learning/differential-computing-jax/#what-jax-enables-and-doesnt-enable","title":"What JAX enables and doesn't enable","text":"<p>JAX as a package doesn't pretend to be a replacement for established deep learning frameworks. That is because JAX doesn't provide the deep learning abstractions as a first-class citizen; its focus is on the much more generally useful idea of composable program transformations. To compare it against a deep learning framework is a bit of a red herring - a distraction away from what JAX enables.</p> <p>What JAX actually enables is for us to write numerical programs using the NumPy API that are performant and automatically differentiable. <code>vmap</code> and <code>lax.scan</code> help us eliminate Python loop overhead in our code; <code>jit</code> just-in-time compiles code to accelerate it; <code>grad</code> gives us differentiability, thus opening the door for us to write performant optimization routines that solve real world problems.</p> <p>At work, I have used JAX productively in both neural network and non-neural network settings, with the unifying theme being gradient-based optimization of model parameters. With JAX, I can seamlessly move between problem classes while using the PyData community's idiomatic NumPy API. We have used JAX to implement Hierarchical Dirichlet Process autoregressive multivariate Gaussian hidden Markov models (what a mouthful!), LSTM recurrent neural networks, graph neural networks, simple feed-forward neural networks, linear models, and more... and train them using the same gradient descent tooling available to us in JAX.</p> <p>The upside here is that we could hand-craft each model and tailor it to each problem encountered. The code was written in a very explicit fashion that exposed the many layers of abstractions that were sometimes needed. Noe that this may also be viewed as the downside of writing JAX code -- we had to write a lot of code, partially because the abstractions we needed weren't already implemented in some cases, and partially because they aren't easily available in JAX in other cases.</p> <p>One thing I wanted to highlight though: leveraging simple tricks learned from the neural network and probabilistic programming worlds (such as optimizing in unbounded rather than bounded space), we were able to train covariance matrices in our multivariate Gaussian HMMs using gradient descent rather than expectation-maximization, and it just worked. I found it amazing to see in action.</p> <p>Now, the lack of deep learning abstractions in JAX doesn't mean that JAX as a backend to other computing frameworks isn't available! A flurry of development after JAX's initial release led to a suite of deep learning libraries and probabilistic programming languages targeting JAX as an array backend, because of its provision of a library of Python-compatible composable program transformations.</p> <p>For deep learning libraries, an experimental <code>stax</code> module exists inside JAX; my intern Arkadij Kummer and myself used it productively in a JAX-based reimplementation of an LSTM model used for protein engineering. <code>flax</code>, also developed by Googlers, exists, and provides a PyTorch-like API that builds on top of the functional programming paradigm encouraged by JAX. The Neural Tangents package for infinitely wide, Bayesian neural networks follows <code>stax</code>'s  idioms, with well-documented differences (though without reasons given).</p> <p>For probabilistic programming languages, even TensorFlow Probability has a JAX backend as an alternative to the TensorFlow backend. PyMC3, which is built on top of Theano, is getting a JAX-ified Theano backend too, while <code>mcx</code>, written by a French software developer Rem\u00ed Louf, is a pedagogical PPL written entirely using JAX as a backend too. Not to forget NumPyro, which is another JAX-based implementation of the Pyro probabilistic programming language.</p>"},{"location":"machine-learning/differential-computing-jax/#recent-developments","title":"Recent developments","text":"<p>JAX has been actively developed for over two years now, and as a project, it continues to attract talent to the project. The originators were Dougal Maclaurin, Matt Johnson, Alex Wiltschko and David Duvenaud while they were at all at Harvard, and has since grown to include many prominent Pythonistas including Jake Vanderplas and Stephan Hoyer on the team. (There are many more, whose names I don't know very well, so my apologies in advance if I have left your name out. For a full list of code contributors, the repository contributors page is the most definitive.)</p>"},{"location":"machine-learning/differential-computing-jax/#learning-more","title":"Learning more","text":"<p>I invested one of my vacation weeks crystallizing my learnings from working with JAX over the past year and a half, and it's been extremely educational. If you're interested in reading it, you can find it at my <code>dl-workshop</code> repository on GitHub. In there, in addition to the original content, which was a workshop on deep learning, I also try to provide \"simple complex examples\" of how to use JAX idioms in solving modelling problems.</p> <p>Besides that, JAX's documentation is quite well-written, and you can find it at jax.readthedocs.io. In particular, they have a very well-documented suite of \"The Sharp Bits\" to look out for when using JAX, geared towards both power users of vanilla NumPy and beginners. If you're using JAX and run into unexpected behaviour, I'd strongly encourage you to check out the post - it'll clear up many misconceptions you might have!</p> <p>In terms of introductory material, a blog post by Colin Raffel, titled \"You don't know JAX\", is a very well-written introduction on how to use JAX. Eric Jang also has a blog post on implementing meta-learning in JAX, which I found very educational for both JAX syntax and meta-learning.</p> <p>While the most flashy advances of the deep learning world came from 2010-2020, personally think that the most exciting foundational advance of that era was the development of a general purpose automatic differentiation package like <code>autograd</code> and JAX. At least for the Python world, it's enabled the writing of arbitrary models in a highly compatible fashion with the rest of the PyData stack, with differentiation and native compilation as first class program transformations. The use of gradients is varied, with much room for creativity; I'd definitely encourage you to try it out!</p>"},{"location":"machine-learning/generating-markov-chains-dirichlet/","title":"Generating markov chains dirichlet","text":"<pre><code>%load_ext autoreload\n%autoreload 2\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\n%load_ext watermark\n</code></pre> <pre><code>from jax import random, vmap, jit, grad, lax\nfrom jax.scipy import stats\nimport jax.numpy as np\nimport matplotlib.pyplot as plt\nfrom functools import partial\nimport seaborn as sns\n</code></pre> <p>To generate a transition matrix with this desired property, we can turn to the GEM distribution. The GEM distribution is one way to generate a random vector from a Dirichlet distribution (which is the generalization of a Beta distribution).</p> <p>You can think of it as stick-breaking, basically. We take a stick of unit length 1, and break it in two according to a draw from a Beta distribution. We record the length of the left part of the stick, and then break the right stick into two according to a draw from a Beta distribution. We then record the new length of the left side and break the right one again and again, ad infinitum or until we have reached a predefined (but finite) number of breaks. The vector of recorded lengths becomes a \"weighting\" vector. One thing to keep in mind: this weighting vector doesn't necessarily sum to 1, so in order to use the weighting vector in a transition matrix, we do have to normalize it to sum to 1, or we append the remainder of the stick to the end to get it to sum to 1.</p> <p>Enough said, let's dig in and try simulating this process.</p> <p>Firstly, we generate a vector of i.i.d. draws from a Beta distribution with parameters \\alpha = 1 and \\beta = 1.</p> <pre><code>key = random.PRNGKey(45)  # for reproducibility\nbeta_draws = random.beta(key, a=1, b=1, shape=(10,))\nplt.plot(beta_draws, marker=\"o\")\nsns.despine()\n</code></pre> <p>Now, we take this and begin our stick-breaking process. Because it is effectively a for-loop in which each loop iteration uses carryover from the previous loop iteration, I have written it taking advantage of JAX's <code>lax.scan</code> function.</p> <pre><code>def stick_breaking_weights(beta_draws):\n\"\"\"Return weights from a stick breaking process.\n\n    :param beta_draws: i.i.d draws from a Beta distribution.\n        This should be a row vector.\n    \"\"\"\n    def weighting(occupied_probability, beta_i):\n\"\"\"\n        :param occupied_probability: The cumulative occupied probability taken up.\n        :param beta_i: Current value of beta to consider.\n        \"\"\"\n        weight = (1 - occupied_probability) * beta_i\n        return occupied_probability + weight, weight\n\n    occupied_probability, weights = lax.scan(weighting, np.array(0.), beta_draws)\n\n    weights = weights / np.sum(weights)\n    return occupied_probability, weights\n</code></pre> <pre><code>occupied_prob, weights = stick_breaking_weights(beta_draws)\nplt.plot(weights, marker=\"o\")\nplt.ylim(-0.1, 1.1)\nsns.despine()\n</code></pre> <p>Really cool! We now have a vector of weights, normalized to a probability distribution.</p> <p>It's worth at this point exploring the effect of varying the b parameter in the Beta distribution:</p> <pre><code>fig, axes = plt.subplots(nrows=1, ncols=5, figsize=(20, 4), sharey=True)\n\nbvals = np.array([1, 3, 5, 10, 20])\nbeta_draws = vmap(partial(random.beta, key, 1, shape=(10,)))(bvals)\noccupied_probs, weights = vmap(stick_breaking_weights)(beta_draws)\n\nfor ax, weight, bval in zip(axes, weights, bvals):\n    ax.plot(weight, marker=\"o\")\n    ax.set_title(f\"b = {bval}\")\nsns.despine()\n</code></pre> <p>As should be visible, when we increase the <code>b</code> value, we get a less concentrated and flatter distribution compared to when we use a smaller <code>b</code> value. Thus, <code>b</code> acts as a \"concentration\" parameter. Smaller values means probability mass is concentrated on a smaller number of slots, while larger values means probability mass is diffused across a larger number of slots.</p> <p>How does this relate to transition matrices in hidden Markov models? Well, a potentially desirable property is that we wish to express is that most of the states tend to move into a certain smaller number of states, thereby concentrating the number of occupied states into a smaller set. This is equivalent to concentrating the transition matrix to a subset of columns. Let's see how we can generate this kind of transition matrix.</p> <p>Firstly, since transition matrices are square, let's start with a 15x15 transition matrix, i.e. one with 15 states. </p> <pre><code>N_STATES = 15\nCONCENTRATION = 2\n\nbeta_draws = random.beta(key, a=1, b=CONCENTRATION, shape=(N_STATES, N_STATES))\n</code></pre> <p>To visualize these i.i.d. Beta-distributed draws, let's use a heatmap.</p> <pre><code>sns.heatmap(beta_draws);\n</code></pre> <p>Keep in mind, this is not the transition matrix just yet. It is the precursor to one!</p> <p>Next up, on a row-wise basis, we convert each row to a weighting vector, thereby getting back a transition matrix:</p> <pre><code>def compute_transition_matrix(beta_draws):\n    _, transition_matrix = vmap(stick_breaking_weights)(beta_draws)\n    return _, transition_matrix\n\n_, transition_matrix = compute_transition_matrix(beta_draws)\n</code></pre> <p>And visualizing the transition_matrix...</p> <pre><code>sns.heatmap(transition_matrix);\n</code></pre> <p>Voil\u00e0! We have a transition matrix that has most of the probability mass concentrated in just a few states. Let's calculate the equilibrium distribution:</p> <pre><code>def equilibrium_distribution(p_transition):\n    n_states = p_transition.shape[0]\n    A = np.append(\n        arr=p_transition.T - np.eye(n_states),\n        values=np.ones(n_states).reshape(1, -1),\n        axis=0\n    )\n    # Moore-Penrose pseudoinverse = (A^TA)^{-1}A^T\n    pinv = np.linalg.pinv(A)\n    # Return last row\n    return pinv.T[-1]\n</code></pre> <pre><code>eq_distribution = equilibrium_distribution(transition_matrix)\nplt.plot(eq_distribution, marker=\"o\")\nsns.despine()\n</code></pre> <p>As should be visible, we spend the majority of time in just a few states, and not too many more.</p> <p>At this point, it's worth exploring how the \"concentration\" parameter affects the transition matrix, and hence the equilibrium distribution.</p> <pre><code>CONCENTRATIONS = np.array([1, 3, 5, 10, 20])\nN_DIMS = 30\nbeta_draws = vmap(partial(random.beta, key, 1, shape=(N_DIMS, N_DIMS)))(CONCENTRATIONS)\nbeta_draws.shape\n</code></pre> <pre><code>_, transition_matrices = vmap(compute_transition_matrix)(beta_draws)\neq_distributions = vmap(equilibrium_distribution)(transition_matrices)\n\nfig, axes = plt.subplots(nrows=2, ncols=5, figsize=(20, 8))\n\nfor ax, conc, tmat in zip(axes[0, :], CONCENTRATIONS, transition_matrices):\n    sns.heatmap(tmat, ax=ax)\n    ax.set_title(f\"concentration = {conc}\")\n\nfor ax, conc, eq_dist in zip(axes[1, :], CONCENTRATIONS, eq_distributions):\n    ax.plot(eq_dist, marker=\"o\")\n    ax.set_title(f\"concentration = {conc}\")\n    ax.set_ylim(-0.1, 1.1)\nsns.despine()\n</code></pre> <p>As you can see, when the value of <code>b</code> goes up, the more diffuse the transition matrix, and the more evenly spread-out the equilibrium states will be.</p> <pre><code>from jax import random\nfrom jax.scipy.special import logit\n</code></pre> <p>Firstly, let's try writing the function that generates a Markov sequence given a transition matrix.</p> <pre><code>from typing import List\nfrom jax import jit\n\ndef markov_sequence(key, p_transition: np.array, sequence_length: int) -&amp;gt; List[int]:\n\"\"\"\n    Generate a Markov sequence based on p_init and p_transition.\n\n    Strategy: leverage categorical distribution.\n    We need to vmap over split PRNGKeys, which will give us the desired number of draws.\n    \"\"\"\n    p_eq = equilibrium_distribution(p_transition)\n    logit_p_eq = logit(p_eq)\n    initial_state = random.categorical(key, logits=logit_p_eq, shape=(1,))\n\n    def draw_state(prev_state, key):\n        logits = logit(p_transition[prev_state])\n        state = random.categorical(key, logits=logits, shape=(1,))\n        return state, state\n\n    keys = random.split(key, sequence_length)\n    final_state, states = lax.scan(draw_state, initial_state, keys)\n    return final_state, np.squeeze(states)\n\nmarkov_sequence = jit(markov_sequence, static_argnums=(2,))\n\nfinal, sequence = markov_sequence(key, transition_matrices[0], 100)\nfinal, sequence\n</code></pre> <p>Now, I think we can generate a bunch of Markov chain sequences using <code>vmap</code>.</p> <pre><code>def markov_seq_vmappable(key, transition_matrix):\n    sequence_length = 500\n    return markov_sequence(key, transition_matrix, sequence_length)\n\n_, sequences = vmap(markov_seq_vmappable)(random.split(key, 5), transition_matrices)\n</code></pre> <p>Let's plot them out!</p> <pre><code>import seaborn as sns\n\nfig, axes = plt.subplots(figsize=(16, 20), nrows=5, ncols=1)\n\nfor ax, seq in zip(axes, sequences):\n    ax.plot(range(len(seq)), seq, marker=\"o\")\nsns.despine()\n</code></pre> <p>As should be visible, as we increase the concentration parameter (really, I think this should be renamed as a \"diffusion\" parameter), the number of states that we would typically\u00a0occupy increases. At smaller values of the concentration parameter, the number of states we would typically occupy decreases.</p> <pre><code>def diagonal_draws(key, bias_factor, shape):\n    return random.beta(key, a=bias_factor, b=1, shape=shape)\n</code></pre> <pre><code>dd = diagonal_draws(key, bias_factor=50, shape=(N_DIMS,))\ndom_diag_transition_matrix = transition_matrices[0] + np.diagflat(dd)\n\ndef normalize_prob_vect(v):\n    return v / np.sum(v)\n\n\ndef normalize_transition_matrix(transition_matrix):\n    return vmap(normalize_prob_vect)(transition_matrix)\n\ndom_diag_transition_matrix = normalize_transition_matrix(dom_diag_transition_matrix)\nsns.heatmap(dom_diag_transition_matrix);\n</code></pre> <p>Now given this transition matrix, let's generate new sequences:</p> <pre><code>_, seq = markov_sequence(key, dom_diag_transition_matrix, 500)\nplt.plot(seq, marker=\"o\")\nsns.despine()\n</code></pre> <p>As is visible from the plot above, we get sequences that tend to stay inside a state, and when they do venture out to unfavoured states (e.g. state 5), they quickly return back to a favoured state.</p> <p>Now, let's see what kind of sequences we get when we use the same dominant diagonal with different concentration parameters.</p> <p>Firstly, we generate a bunch of dominant diagonal matrices:</p> <pre><code>keys = random.split(key, 5)\ndiagonals = vmap(partial(diagonal_draws, bias_factor=50, shape=(N_DIMS,)))(keys)\n\ndef create_dominant_diagonal(p_transition, diagonal):\n    p_transition = p_transition + np.diagflat(diagonal)\n    return normalize_transition_matrix(p_transition)\n\ndom_diag_transition_matrices = vmap(create_dominant_diagonal)(transition_matrices, diagonals)\n\nfig, axes = plt.subplots(nrows=1, ncols=5, figsize=(20, 3))\n\nfor ax, mat in zip(axes, dom_diag_transition_matrices):\n    sns.heatmap(mat, ax=ax)\n</code></pre> <p>Now, we generate a bunch of sequences.</p> <pre><code>_, sequences = vmap(markov_seq_vmappable)(random.split(key, 5), dom_diag_transition_matrices)\n\nfig, axes = plt.subplots(figsize=(16, 20), nrows=5, ncols=1)\n\nfor ax, seq, conc in zip(axes, sequences, CONCENTRATIONS):\n    ax.plot(range(len(seq)), seq, marker=\"o\")\n    ax.set_title(f\"concentration = {conc}\")\nsns.despine()\n</code></pre> <p>As should be visible from here, we now generate sequences that have a much higher propensity to stay within their own state, rather than jump around. Additionally, when there are more states \"available\" (i.e. concentration runs higher), we also see them stay within their own state rather than jump back down to the favoured states.</p> <pre><code>sns.heatmap(transition_matrices[0]);\n</code></pre> <p>Each row of the transition matrix is generated by running a stick breaking process forward from Beta distributed draws. We can run the process backwards to get back our Beta-distributed matrix. Because there's division involved, I have opted to operate in logarithmic space instead, to avoid over/under-flow issues.</p> <pre><code>from jax import lax\ndef beta_draw_from_weights(weights):\n    def beta_from_w(accounted_probability, weights_i):\n\"\"\"\n        :param accounted_probability: The cumulative probability acounted for.\n        :param weights_i: Current value of weights to consider.\n        \"\"\"\n        denominator = 1 - accounted_probability\n        log_denominator = np.log(denominator)\n\n        log_beta_i = np.log(weights_i) - log_denominator\n\n        newly_accounted_probability = accounted_probability + weights_i\n\n        return newly_accounted_probability, np.exp(log_beta_i)\n    final, betas = lax.scan(beta_from_w, np.array(0.), weights)\n    return final, betas\n</code></pre> <p>And now, to sanity-check that it works:</p> <pre><code>beta_draw = random.beta(key, a=1, b=3, shape=(15,))\n_, weights = stick_breaking_weights(beta_draw)\n_, beta_draw_hat = beta_draw_from_weights(weights)\nplt.plot(beta_draw, label=\"original\")\nplt.plot(beta_draw_hat, label=\"recovered\")\nplt.legend()\nsns.despine()\n</code></pre> <p>Up till the last few values, we are basically able to recover the beta distributed draw that generated the matrix. The fundamental problem we're facing here is that when we are faced with a probability vector, we're still missing the \"last stick\" which would give us an accurate estimate of the originals. As such, only the first few are really accurate, and the accuracy of beta draw recovery goes down as we go across the vector.</p> <p>Let's now apply the function to every row in the transition matrix.</p> <pre><code>def recover_beta(transition_matrix):\n    return vmap(beta_draw_from_weights)(transition_matrix)\n</code></pre> <pre><code>_, recovered_betas = vmap(recover_beta)(transition_matrices)\n\nfig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10, 4))\n\nidx = 4\nsns.heatmap(recovered_betas[idx], ax=axes[0])\nsns.heatmap(beta_draws[idx], ax=axes[1])\n</code></pre> <p>Matches what we saw above - we're doing an almost-OK job here.</p> <p>Now, we can evaluate the logpdf of the matrix. Because each entry in the recovered betas matrix is an i.i.d. draw from the Beta distribution, and because row-wise the first 3-5 elements are accurately estimatable backwards from the weights, we will estimate the concentration parameter using only the first three columns of betas from the matrix.</p> <p>Test-driving the syntax below...</p> <pre><code>np.sum(stats.beta.logpdf(recovered_betas[4, :, :3], a=1, b=9))\n</code></pre> <p>Looks good! Let's now define a function for the logpdf.</p> <pre><code>def transition_matrix_logpdf(transition_matrix, concentration, num_cols=2):\n    _, beta_recovered = recover_beta(transition_matrix)\n    logp = stats.beta.logpdf(beta_recovered[:, :num_cols], a=1, b=concentration)\n    return np.sum(logp)\n\ntransition_matrix_logpdf(transition_matrices[1], 5)\n</code></pre> <p>Just to see if this is a gradient optimizable problem, let's plot a range of concentration values, and see what happens.</p> <pre><code>log_conc_range = np.linspace(-3, 3, 10000)\nconc_range = np.exp(log_conc_range)\n\ndef loglike_range(transition_matrix, log_conc_range):\n    conc_range = np.exp(log_conc_range)\n    ll = vmap(partial(transition_matrix_logpdf, transition_matrix))(conc_range)\n    return ll\n\nll = loglike_range(transition_matrices[0], log_conc_range)\n\nlls = vmap(partial(loglike_range, log_conc_range=log_conc_range))(transition_matrices)\n\nfig, axes = plt.subplots(nrows=1, ncols=len(CONCENTRATIONS), figsize=(20, 4))\n\nfor ll, conc, ax in zip(lls, CONCENTRATIONS, axes):\n    ax.plot(conc_range, ll)\n    ax.set_title(f\"concentration = {conc}\")\nsns.despine()\n</code></pre> <p>Not bad! Visually, it appears that we do pretty good in recovering the maximum likelihood value for most of the entries, but for concentration = 20, it's more difficult to do so.</p> <p>Let's confirm by extracting the concentration at which we have maximum log-likelihood.</p> <pre><code>maxes = vmap(np.argmax)(lls)\nmle_estimates = np.take(conc_range, maxes)\nmle_estimates\n</code></pre> <p>Great! Doing this brute-force is all nice and good, but one of the points of JAX is that we get to do gradient descent easily. So now, let's try to perform gradient-based optimization :).</p> <p>We start by first defining a loss as a function of the log of the concentration. (We use the log so that when we do gradient optimization, we can be in an unbounded space.) We also define the gradient of the loss function.</p> <pre><code>def loglike_loss(log_concentration, transition_matrix):\n    concentration = np.exp(log_concentration)\n    ll = transition_matrix_logpdf(transition_matrix, concentration)\n    return -ll\n\nloglike_loss = jit(loglike_loss)\ndloglike_loss = grad(loglike_loss)\n</code></pre> <p>Next up, we do the loop. Instead of writing an explicit for-loop, we are going to some JAX trickery here:</p> <ol> <li>We write the loop taking advantage of <code>lax.scan</code>, which allows us to leverage the previous state to get back parameters to optimize.</li> <li>We also vmap our training loop over all 5 matrices, starting with the same log concentration starting point. This allows us to essentially train five models at one shot. (I could have <code>pmap</code>-ed it, but I really only have one CPU and one GPU on my computer.)</li> <li>Since the result is a vmapped, we have a collection of final states and historical states (which we can post-process post-hoc). Hence, we can vmap <code>get_params</code> over final states to get back the vector of final states (from a constant initial state).</li> </ol> <pre><code>from jax.example_libraries.optimizers import adam\n\ninit, update, get_params = adam(0.05)\nlog_conc_start = random.normal(key)\n\ndef step(prev_state, i, data, dloss):\n\"\"\"One step in the training loop.\"\"\"\n    params = get_params(prev_state)\n    g = dloss(params, data)\n    state = update(i, g, prev_state)\n    return state, state\n\ndef train(transition_matrix, dloss, params, n_steps=200):\n\"\"\"The training loop for one transition matrix.\"\"\"\n    stepfunc = partial(step, data=transition_matrix, dloss=dloss)\n    stepfunc = jit(stepfunc)\n\n    state = init(params)\n    final_state, states = lax.scan(stepfunc, state, np.arange(n_steps))\n    return final_state, states\n\ntrainfunc = partial(train, params=log_conc_start, dloss=dloglike_loss)\ntrainfunc = jit(trainfunc)\n# Train across all transition matrices!\nfinal_states, states_history = vmap(trainfunc)(transition_matrices)\nnp.exp(vmap(get_params)(final_states))\n</code></pre> <p>We can also get the history by <code>vmap</code>-ing <code>get_params</code> over <code>all_states</code>.</p> <pre><code>log_concentration_history = vmap(get_params)(states_history)\n\nfor concentration, history in zip(CONCENTRATIONS, log_concentration_history):\n    plt.plot(np.exp(history), label=concentration)\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"Concentration Value\")\nplt.legend()\nsns.despine()\n</code></pre> <p>Now, if you're wondering how I knew 200 steps would be sufficient for convergence a priori, I didn't :). I had originally tried 1000 steps before staring at the concentration curves, at which point I then knew 200 was sufficient. So... no magic there.</p> <pre><code>%watermark\n</code></pre> <pre><code>%watermark --iversions\n</code></pre>"},{"location":"machine-learning/generating-markov-chains-dirichlet/#dirichlet-processes-and-hidden-markov-model-transition-matrices","title":"Dirichlet Processes and Hidden Markov Model Transition Matrices","text":"<p>How do we construct transition matrices that prioritize re-entry into a constrained set of states? Especially if we don't have perfect knowledge of how many true states there are?</p> <p>From Matt Johnson's thesis, I learned exactly how.</p> <p>Some of us might be used to thinking about transition matrices that have strong diagonals. That's all good for providing stability in a sequence of transitions. But if the goal is to provide a model where the constrained set of states is given priority over the other states, then what we really need is a transition matrix where the first K columns of the transition matrix are given priority over the others.</p>"},{"location":"machine-learning/generating-markov-chains-dirichlet/#lets-generate-markov-sequences-now","title":"Let's generate Markov sequences now","text":"<p>Now that we know how to generate transition matrices, let's step back and try to see whether the generated Markovian sequences from these transition matrices make sense, i.e. whether they display the desired properties that we seek or not.</p>"},{"location":"machine-learning/generating-markov-chains-dirichlet/#generating-markov-sequences-concentrated-on-a-few-stable-states","title":"Generating Markov sequences concentrated on a few stable states","text":"<p>We've thus far generated transition matrices that are biased towards a few states, but they do tend to be jumpy, as the above scatterplots show. In other words, we have not yet generated matrices that allow for stability inside a state. Stability inside a state is generated from strong diagonals. We can engineer this in the generation of the matrix by leveraging (once again) the Beta distribution. Specifically, the generative story here is that each row of the transition matrix is generated by stick-breaking, but the diagonals are replaced by a Beta draw that is biased towards high probabilities. Let's see this in action for one transition matrix.</p>"},{"location":"machine-learning/generating-markov-chains-dirichlet/#inference-of-the-right-concentration-of-states","title":"Inference of the right \"concentration\" of states","text":"<p>Given the transition matrix, can we infer the concentration parameter that best describes it? This is what we're going to try out here.</p> <p>We start with a vanilla transition matrix generated from a <code>concentration = 1</code> setting, with no dominant diagonals. This is the easier setting to begin with:</p>"},{"location":"machine-learning/generating-markov-chains-dirichlet/#summary","title":"Summary","text":"<p>This was a bit of a whirlwind tour of a notebook, in that there were many concepts and ideas tied together into one \"project\". </p> <p>With respect to HMMs, it's super important to have a proper mental model of each of its components. In the case of expressing the idea that we have a restricted number of states, we may engineer this into the model by taking advantage of column-wise heaviness in a restricted subset of states. This is mathematically most naturally incorporated by composint together row-wise Dirichlet-distributed probability arrays. We can also mathematically engineer consistency in states by taking advantage of high Beta-distributed values. Compose the two together, and we get a transition matrix that favours entry into a small number of states with stability in there.</p> <p>Beyond that lesson, we saw the power of composable programs using JAX. <code>vmap</code>, <code>jit</code>, <code>lax.scan</code>, and more from the JAX toolkit gives us the ability to write performant programs that sort of \"just make sense\", once you know what their idioms are. Specifically:</p> <ul> <li><code>vmap</code> is a vanilla for-loop, processing an elementary function over an axis of an array.</li> <li><code>lax.scan</code> is a carry-over for-loop, processing the result of a previous iteration, with accumulation of history as well.</li> <li><code>jit</code> gives you just-in-time compilation of a function.</li> <li><code>grad</code> gives you gradients of any arbitrary function written in a JAX-compatible, pure functional fashion.</li> </ul> <p>In particular, getting used to the <code>lax.scan</code> idioms has been a literal door-opener. We can now write really performant loops that use results from previous iterations, such as a gradient descent training loop or an MCMC sampler. Using <code>lax.scan</code>, we wrote:</p> <ul> <li>A Markov chain state generator</li> <li>A generator of Dirichlet-distributed weights from i.i.d. Beta distribution draws</li> <li>A reverse generator/estimator of Beta distribution draws from Dirichlet-distributed weights (with some inaccuracies of course, due to a lack of information).</li> <li>A fully-compiled gradient descent training loop that ran extremely fast.</li> </ul> <p>And using <code>vmap</code>, we were able to do all sorts of vanilla loops, but the one I want to highlight is our ability to <code>vmap</code> the compiled training loop across multiple transition matrices. The fact that this actually works never has left me in awe. Props to the JAX team here! Opens the door to <code>vmap</code>-ing training loops across random starting points (i.e. a split PRNGKey, much like what we did in the HMM state generator).</p> <p>The trade-off is that we don't get nice progress bars, of course, which require that we break out of the compiled loop to show the current state. But the compilation speedups provided the opportunity to build our compiled tensor program end-to-end. We could verify first that things ran correctly on a moderately-sized number of iterations, before finally estimating how long we would need to go until convergence and letting the program run on its own. I'm sure this little change isn't too hard to adapt to, but will give you access to a whole new world of differential tensor programming that is just cool!</p>"},{"location":"machine-learning/generating-markov-chains-dirichlet/","title":"Dirichlet Processes and Hidden Markov Model Transition Matrices","text":"<p>How do we construct transition matrices that prioritize re-entry into a constrained set of states? Especially if we don't have perfect knowledge of how many true states there are?</p> <p>From Matt Johnson's thesis, I learned exactly how.</p> <p>Some of us might be used to thinking about transition matrices that have strong diagonals. That's all good for providing stability in a sequence of transitions. But if the goal is to provide a model where the constrained set of states is given priority over the other states, then what we really need is a transition matrix where the first K columns of the transition matrix are given priority over the others.</p> <pre><code>%load_ext autoreload\n%autoreload 2\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\n%load_ext watermark\n</code></pre> <pre><code>from jax import random, vmap, jit, grad, lax\nfrom jax.scipy import stats\nimport jax.numpy as np\nimport matplotlib.pyplot as plt\nfrom functools import partial\nimport seaborn as sns\n</code></pre> <p>To generate a transition matrix with this desired property, we can turn to the GEM distribution. The GEM distribution is one way to generate a random vector from a Dirichlet distribution (which is the generalization of a Beta distribution).</p> <p>You can think of it as stick-breaking, basically. We take a stick of unit length 1, and break it in two according to a draw from a Beta distribution. We record the length of the left part of the stick, and then break the right stick into two according to a draw from a Beta distribution. We then record the new length of the left side and break the right one again and again, ad infinitum or until we have reached a predefined (but finite) number of breaks. The vector of recorded lengths becomes a \"weighting\" vector. One thing to keep in mind: this weighting vector doesn't necessarily sum to 1, so in order to use the weighting vector in a transition matrix, we do have to normalize it to sum to 1, or we append the remainder of the stick to the end to get it to sum to 1.</p> <p>Enough said, let's dig in and try simulating this process.</p> <p>Firstly, we generate a vector of i.i.d. draws from a Beta distribution with parameters \\alpha = 1 and \\beta = 1.</p> <pre><code>key = random.PRNGKey(45)  # for reproducibility\nbeta_draws = random.beta(key, a=1, b=1, shape=(10,))\nplt.plot(beta_draws, marker=\"o\")\nsns.despine()\n</code></pre> <p></p> <p>Now, we take this and begin our stick-breaking process. Because it is effectively a for-loop in which each loop iteration uses carryover from the previous loop iteration, I have written it taking advantage of JAX's <code>lax.scan</code> function.</p> <pre><code>def stick_breaking_weights(beta_draws):\n\"\"\"Return weights from a stick breaking process.\n\n    :param beta_draws: i.i.d draws from a Beta distribution.\n        This should be a row vector.\n    \"\"\"\n    def weighting(occupied_probability, beta_i):\n\"\"\"\n        :param occupied_probability: The cumulative occupied probability taken up.\n        :param beta_i: Current value of beta to consider.\n        \"\"\"\n        weight = (1 - occupied_probability) * beta_i\n        return occupied_probability + weight, weight\n\n    occupied_probability, weights = lax.scan(weighting, np.array(0.), beta_draws)\n\n    weights = weights / np.sum(weights)\n    return occupied_probability, weights\n</code></pre> <pre><code>occupied_prob, weights = stick_breaking_weights(beta_draws)\nplt.plot(weights, marker=\"o\")\nplt.ylim(-0.1, 1.1)\nsns.despine()\n</code></pre> <p></p> <p>Really cool! We now have a vector of weights, normalized to a probability distribution.</p> <p>It's worth at this point exploring the effect of varying the b parameter in the Beta distribution:</p> <pre><code>fig, axes = plt.subplots(nrows=1, ncols=5, figsize=(20, 4), sharey=True)\n\nbvals = np.array([1, 3, 5, 10, 20])\nbeta_draws = vmap(partial(random.beta, key, 1, shape=(10,)))(bvals)\noccupied_probs, weights = vmap(stick_breaking_weights)(beta_draws)\n\nfor ax, weight, bval in zip(axes, weights, bvals):\n    ax.plot(weight, marker=\"o\")\n    ax.set_title(f\"b = {bval}\")\nsns.despine()\n</code></pre> <p></p> <p>As should be visible, when we increase the <code>b</code> value, we get a less concentrated and flatter distribution compared to when we use a smaller <code>b</code> value. Thus, <code>b</code> acts as a \"concentration\" parameter. Smaller values means probability mass is concentrated on a smaller number of slots, while larger values means probability mass is diffused across a larger number of slots.</p> <p>How does this relate to transition matrices in hidden Markov models? Well, a potentially desirable property is that we wish to express is that most of the states tend to move into a certain smaller number of states, thereby concentrating the number of occupied states into a smaller set. This is equivalent to concentrating the transition matrix to a subset of columns. Let's see how we can generate this kind of transition matrix.</p> <p>Firstly, since transition matrices are square, let's start with a 15x15 transition matrix, i.e. one with 15 states.</p> <pre><code>N_STATES = 15\nCONCENTRATION = 2\n\nbeta_draws = random.beta(key, a=1, b=CONCENTRATION, shape=(N_STATES, N_STATES))\n</code></pre> <p>To visualize these i.i.d. Beta-distributed draws, let's use a heatmap.</p> <pre><code>sns.heatmap(beta_draws);\n</code></pre> <p></p> <p>Keep in mind, this is not the transition matrix just yet. It is the precursor to one!</p> <p>Next up, on a row-wise basis, we convert each row to a weighting vector, thereby getting back a transition matrix:</p> <pre><code>def compute_transition_matrix(beta_draws):\n    _, transition_matrix = vmap(stick_breaking_weights)(beta_draws)\n    return _, transition_matrix\n\n_, transition_matrix = compute_transition_matrix(beta_draws)\n</code></pre> <p>And visualizing the transition_matrix...</p> <pre><code>sns.heatmap(transition_matrix);\n</code></pre> <p></p> <p>Voil\u00e0! We have a transition matrix that has most of the probability mass concentrated in just a few states. Let's calculate the equilibrium distribution:</p> <pre><code>def equilibrium_distribution(p_transition):\n    n_states = p_transition.shape[0]\n    A = np.append(\n        arr=p_transition.T - np.eye(n_states),\n        values=np.ones(n_states).reshape(1, -1),\n        axis=0\n    )\n    # Moore-Penrose pseudoinverse = (A^TA)^{-1}A^T\n    pinv = np.linalg.pinv(A)\n    # Return last row\n    return pinv.T[-1]\n</code></pre> <pre><code>eq_distribution = equilibrium_distribution(transition_matrix)\nplt.plot(eq_distribution, marker=\"o\")\nsns.despine()\n</code></pre> <p></p> <p>As should be visible, we spend the majority of time in just a few states, and not too many more.</p> <p>At this point, it's worth exploring how the \"concentration\" parameter affects the transition matrix, and hence the equilibrium distribution.</p> <pre><code>CONCENTRATIONS = np.array([1, 3, 5, 10, 20])\nN_DIMS = 30\nbeta_draws = vmap(partial(random.beta, key, 1, shape=(N_DIMS, N_DIMS)))(CONCENTRATIONS)\nbeta_draws.shape\n</code></pre> <pre><code>(5, 30, 30)\n</code></pre> <pre><code>_, transition_matrices = vmap(compute_transition_matrix)(beta_draws)\neq_distributions = vmap(equilibrium_distribution)(transition_matrices)\n\nfig, axes = plt.subplots(nrows=2, ncols=5, figsize=(20, 8))\n\nfor ax, conc, tmat in zip(axes[0, :], CONCENTRATIONS, transition_matrices):\n    sns.heatmap(tmat, ax=ax)\n    ax.set_title(f\"concentration = {conc}\")\n\nfor ax, conc, eq_dist in zip(axes[1, :], CONCENTRATIONS, eq_distributions):\n    ax.plot(eq_dist, marker=\"o\")\n    ax.set_title(f\"concentration = {conc}\")\n    ax.set_ylim(-0.1, 1.1)\nsns.despine()\n</code></pre> <p></p> <p>As you can see, when the value of <code>b</code> goes up, the more diffuse the transition matrix, and the more evenly spread-out the equilibrium states will be.</p>"},{"location":"machine-learning/generating-markov-chains-dirichlet/#lets-generate-markov-sequences-now","title":"Let's generate Markov sequences now","text":"<p>Now that we know how to generate transition matrices, let's step back and try to see whether the generated Markovian sequences from these transition matrices make sense, i.e. whether they display the desired properties that we seek or not.</p> <pre><code>from jax import random\nfrom jax.scipy.special import logit\n</code></pre> <p>Firstly, let's try writing the function that generates a Markov sequence given a transition matrix.</p> <pre><code>from typing import List\nfrom jax import jit\n\ndef markov_sequence(key, p_transition: np.array, sequence_length: int) -&gt; List[int]:\n\"\"\"\n    Generate a Markov sequence based on p_init and p_transition.\n\n    Strategy: leverage categorical distribution.\n    We need to vmap over split PRNGKeys, which will give us the desired number of draws.\n    \"\"\"\n    p_eq = equilibrium_distribution(p_transition)\n    logit_p_eq = logit(p_eq)\n    initial_state = random.categorical(key, logits=logit_p_eq, shape=(1,))\n\n    def draw_state(prev_state, key):\n        logits = logit(p_transition[prev_state])\n        state = random.categorical(key, logits=logits, shape=(1,))\n        return state, state\n\n    keys = random.split(key, sequence_length)\n    final_state, states = lax.scan(draw_state, initial_state, keys)\n    return final_state, np.squeeze(states)\n\nmarkov_sequence = jit(markov_sequence, static_argnums=(2,))\n\nfinal, sequence = markov_sequence(key, transition_matrices[0], 100)\nfinal, sequence\n</code></pre> <pre><code>(Array([0], dtype=int32),\nArray([0, 1, 2, 1, 0, 0, 3, 0, 1, 0, 1, 2, 1, 2, 1, 2, 1, 1, 2, 1, 2, 0,\n0, 1, 0, 4, 1, 0, 7, 0, 1, 0, 1, 2, 0, 2, 4, 0, 0, 1, 2, 1, 2, 1,\n0, 1, 4, 5, 0, 1, 0, 5, 0, 1, 0, 1, 0, 1, 0, 1, 2, 1, 1, 2, 0, 3,\n0, 1, 1, 1, 4, 1, 4, 1, 2, 1, 0, 1, 2, 1, 0, 3, 0, 1, 5, 0, 1, 2,\n1, 0, 1, 0, 1, 0, 3, 0, 1, 0, 1, 0], dtype=int32))\n</code></pre> <p>Now, I think we can generate a bunch of Markov chain sequences using <code>vmap</code>.</p> <pre><code>def markov_seq_vmappable(key, transition_matrix):\n    sequence_length = 500\n    return markov_sequence(key, transition_matrix, sequence_length)\n\n_, sequences = vmap(markov_seq_vmappable)(random.split(key, 5), transition_matrices)\n</code></pre> <p>Let's plot them out!</p> <pre><code>import seaborn as sns\n\nfig, axes = plt.subplots(figsize=(16, 20), nrows=5, ncols=1)\n\nfor ax, seq in zip(axes, sequences):\n    ax.plot(range(len(seq)), seq, marker=\"o\")\nsns.despine()\n</code></pre> <p></p> <p>As should be visible, as we increase the concentration parameter (really, I think this should be renamed as a \"diffusion\" parameter), the number of states that we would typically\u00a0occupy increases. At smaller values of the concentration parameter, the number of states we would typically occupy decreases.</p>"},{"location":"machine-learning/generating-markov-chains-dirichlet/#generating-markov-sequences-concentrated-on-a-few-stable-states","title":"Generating Markov sequences concentrated on a few stable states","text":"<p>We've thus far generated transition matrices that are biased towards a few states, but they do tend to be jumpy, as the above scatterplots show. In other words, we have not yet generated matrices that allow for stability inside a state. Stability inside a state is generated from strong diagonals. We can engineer this in the generation of the matrix by leveraging (once again) the Beta distribution. Specifically, the generative story here is that each row of the transition matrix is generated by stick-breaking, but the diagonals are replaced by a Beta draw that is biased towards high probabilities. Let's see this in action for one transition matrix.</p> <pre><code>def diagonal_draws(key, bias_factor, shape):\n    return random.beta(key, a=bias_factor, b=1, shape=shape)\n</code></pre> <pre><code>dd = diagonal_draws(key, bias_factor=50, shape=(N_DIMS,))\ndom_diag_transition_matrix = transition_matrices[0] + np.diagflat(dd)\n\ndef normalize_prob_vect(v):\n    return v / np.sum(v)\n\n\ndef normalize_transition_matrix(transition_matrix):\n    return vmap(normalize_prob_vect)(transition_matrix)\n\ndom_diag_transition_matrix = normalize_transition_matrix(dom_diag_transition_matrix)\nsns.heatmap(dom_diag_transition_matrix);\n</code></pre> <p></p> <p>Now given this transition matrix, let's generate new sequences:</p> <pre><code>_, seq = markov_sequence(key, dom_diag_transition_matrix, 500)\nplt.plot(seq, marker=\"o\")\nsns.despine()\n</code></pre> <p></p> <p>As is visible from the plot above, we get sequences that tend to stay inside a state, and when they do venture out to unfavoured states (e.g. state 5), they quickly return back to a favoured state.</p> <p>Now, let's see what kind of sequences we get when we use the same dominant diagonal with different concentration parameters.</p> <p>Firstly, we generate a bunch of dominant diagonal matrices:</p> <pre><code>keys = random.split(key, 5)\ndiagonals = vmap(partial(diagonal_draws, bias_factor=50, shape=(N_DIMS,)))(keys)\n\ndef create_dominant_diagonal(p_transition, diagonal):\n    p_transition = p_transition + np.diagflat(diagonal)\n    return normalize_transition_matrix(p_transition)\n\ndom_diag_transition_matrices = vmap(create_dominant_diagonal)(transition_matrices, diagonals)\n\nfig, axes = plt.subplots(nrows=1, ncols=5, figsize=(20, 3))\n\nfor ax, mat in zip(axes, dom_diag_transition_matrices):\n    sns.heatmap(mat, ax=ax)\n</code></pre> <p></p> <p>Now, we generate a bunch of sequences.</p> <pre><code>_, sequences = vmap(markov_seq_vmappable)(random.split(key, 5), dom_diag_transition_matrices)\n\nfig, axes = plt.subplots(figsize=(16, 20), nrows=5, ncols=1)\n\nfor ax, seq, conc in zip(axes, sequences, CONCENTRATIONS):\n    ax.plot(range(len(seq)), seq, marker=\"o\")\n    ax.set_title(f\"concentration = {conc}\")\nsns.despine()\n</code></pre> <p></p> <p>As should be visible from here, we now generate sequences that have a much higher propensity to stay within their own state, rather than jump around. Additionally, when there are more states \"available\" (i.e. concentration runs higher), we also see them stay within their own state rather than jump back down to the favoured states.</p>"},{"location":"machine-learning/generating-markov-chains-dirichlet/#inference-of-the-right-concentration-of-states","title":"Inference of the right \"concentration\" of states","text":"<p>Given the transition matrix, can we infer the concentration parameter that best describes it? This is what we're going to try out here.</p> <p>We start with a vanilla transition matrix generated from a <code>concentration = 1</code> setting, with no dominant diagonals. This is the easier setting to begin with:</p> <pre><code>sns.heatmap(transition_matrices[0]);\n</code></pre> <p></p> <p>Each row of the transition matrix is generated by running a stick breaking process forward from Beta distributed draws. We can run the process backwards to get back our Beta-distributed matrix. Because there's division involved, I have opted to operate in logarithmic space instead, to avoid over/under-flow issues.</p> <pre><code>from jax import lax\ndef beta_draw_from_weights(weights):\n    def beta_from_w(accounted_probability, weights_i):\n\"\"\"\n        :param accounted_probability: The cumulative probability acounted for.\n        :param weights_i: Current value of weights to consider.\n        \"\"\"\n        denominator = 1 - accounted_probability\n        log_denominator = np.log(denominator)\n\n        log_beta_i = np.log(weights_i) - log_denominator\n\n        newly_accounted_probability = accounted_probability + weights_i\n\n        return newly_accounted_probability, np.exp(log_beta_i)\n    final, betas = lax.scan(beta_from_w, np.array(0.), weights)\n    return final, betas\n</code></pre> <p>And now, to sanity-check that it works:</p> <pre><code>beta_draw = random.beta(key, a=1, b=3, shape=(15,))\n_, weights = stick_breaking_weights(beta_draw)\n_, beta_draw_hat = beta_draw_from_weights(weights)\nplt.plot(beta_draw, label=\"original\")\nplt.plot(beta_draw_hat, label=\"recovered\")\nplt.legend()\nsns.despine()\n</code></pre> <p></p> <p>Up till the last few values, we are basically able to recover the beta distributed draw that generated the matrix. The fundamental problem we're facing here is that when we are faced with a probability vector, we're still missing the \"last stick\" which would give us an accurate estimate of the originals. As such, only the first few are really accurate, and the accuracy of beta draw recovery goes down as we go across the vector.</p> <p>Let's now apply the function to every row in the transition matrix.</p> <pre><code>def recover_beta(transition_matrix):\n    return vmap(beta_draw_from_weights)(transition_matrix)\n</code></pre> <pre><code>_, recovered_betas = vmap(recover_beta)(transition_matrices)\n\nfig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10, 4))\n\nidx = 4\nsns.heatmap(recovered_betas[idx], ax=axes[0])\nsns.heatmap(beta_draws[idx], ax=axes[1])\n</code></pre> <pre><code>&lt;AxesSubplot: &gt;\n</code></pre> <p></p> <p>Matches what we saw above - we're doing an almost-OK job here.</p> <p>Now, we can evaluate the logpdf of the matrix. Because each entry in the recovered betas matrix is an i.i.d. draw from the Beta distribution, and because row-wise the first 3-5 elements are accurately estimatable backwards from the weights, we will estimate the concentration parameter using only the first three columns of betas from the matrix.</p> <p>Test-driving the syntax below...</p> <pre><code>np.sum(stats.beta.logpdf(recovered_betas[4, :, :3], a=1, b=9))\n</code></pre> <pre><code>Array(153.58696, dtype=float32)\n</code></pre> <p>Looks good! Let's now define a function for the logpdf.</p> <pre><code>def transition_matrix_logpdf(transition_matrix, concentration, num_cols=2):\n    _, beta_recovered = recover_beta(transition_matrix)\n    logp = stats.beta.logpdf(beta_recovered[:, :num_cols], a=1, b=concentration)\n    return np.sum(logp)\n\ntransition_matrix_logpdf(transition_matrices[1], 5)\n</code></pre> <pre><code>Array(20.504595, dtype=float32)\n</code></pre> <p>Just to see if this is a gradient optimizable problem, let's plot a range of concentration values, and see what happens.</p> <pre><code>log_conc_range = np.linspace(-3, 3, 10000)\nconc_range = np.exp(log_conc_range)\n\ndef loglike_range(transition_matrix, log_conc_range):\n    conc_range = np.exp(log_conc_range)\n    ll = vmap(partial(transition_matrix_logpdf, transition_matrix))(conc_range)\n    return ll\n\nll = loglike_range(transition_matrices[0], log_conc_range)\n\nlls = vmap(partial(loglike_range, log_conc_range=log_conc_range))(transition_matrices)\n\nfig, axes = plt.subplots(nrows=1, ncols=len(CONCENTRATIONS), figsize=(20, 4))\n\nfor ll, conc, ax in zip(lls, CONCENTRATIONS, axes):\n    ax.plot(conc_range, ll)\n    ax.set_title(f\"concentration = {conc}\")\nsns.despine()\n</code></pre> <p></p> <p>Not bad! Visually, it appears that we do pretty good in recovering the maximum likelihood value for most of the entries, but for concentration = 20, it's more difficult to do so.</p> <p>Let's confirm by extracting the concentration at which we have maximum log-likelihood.</p> <pre><code>maxes = vmap(np.argmax)(lls)\nmle_estimates = np.take(conc_range, maxes)\nmle_estimates\n</code></pre> <pre><code>Array([ 0.930245 ,  3.1544516,  5.2817965, 10.019469 , 16.261148 ],      dtype=float32)\n</code></pre> <p>Great! Doing this brute-force is all nice and good, but one of the points of JAX is that we get to do gradient descent easily. So now, let's try to perform gradient-based optimization :).</p> <p>We start by first defining a loss as a function of the log of the concentration. (We use the log so that when we do gradient optimization, we can be in an unbounded space.) We also define the gradient of the loss function.</p> <pre><code>def loglike_loss(log_concentration, transition_matrix):\n    concentration = np.exp(log_concentration)\n    ll = transition_matrix_logpdf(transition_matrix, concentration)\n    return -ll\n\nloglike_loss = jit(loglike_loss)\ndloglike_loss = grad(loglike_loss)\n</code></pre> <p>Next up, we do the loop. Instead of writing an explicit for-loop, we are going to some JAX trickery here:</p> <ol> <li>We write the loop taking advantage of <code>lax.scan</code>, which allows us to leverage the previous state to get back parameters to optimize.</li> <li>We also vmap our training loop over all 5 matrices, starting with the same log concentration starting point. This allows us to essentially train five models at one shot. (I could have <code>pmap</code>-ed it, but I really only have one CPU and one GPU on my computer.)</li> <li>Since the result is a vmapped, we have a collection of final states and historical states (which we can post-process post-hoc). Hence, we can vmap <code>get_params</code> over final states to get back the vector of final states (from a constant initial state).</li> </ol> <pre><code>from jax.example_libraries.optimizers import adam\n\ninit, update, get_params = adam(0.05)\nlog_conc_start = random.normal(key)\n\ndef step(prev_state, i, data, dloss):\n\"\"\"One step in the training loop.\"\"\"\n    params = get_params(prev_state)\n    g = dloss(params, data)\n    state = update(i, g, prev_state)\n    return state, state\n\ndef train(transition_matrix, dloss, params, n_steps=200):\n\"\"\"The training loop for one transition matrix.\"\"\"\n    stepfunc = partial(step, data=transition_matrix, dloss=dloss)\n    stepfunc = jit(stepfunc)\n\n    state = init(params)\n    final_state, states = lax.scan(stepfunc, state, np.arange(n_steps))\n    return final_state, states\n\ntrainfunc = partial(train, params=log_conc_start, dloss=dloglike_loss)\ntrainfunc = jit(trainfunc)\n# Train across all transition matrices!\nfinal_states, states_history = vmap(trainfunc)(transition_matrices)\nnp.exp(vmap(get_params)(final_states))\n</code></pre> <pre><code>Array([ 0.9300743,  3.155341 ,  5.2777195, 10.022282 , 16.264479 ],      dtype=float32)\n</code></pre> <p>We can also get the history by <code>vmap</code>-ing <code>get_params</code> over <code>all_states</code>.</p> <pre><code>log_concentration_history = vmap(get_params)(states_history)\n\nfor concentration, history in zip(CONCENTRATIONS, log_concentration_history):\n    plt.plot(np.exp(history), label=concentration)\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"Concentration Value\")\nplt.legend()\nsns.despine()\n</code></pre> <p></p> <p>Now, if you're wondering how I knew 200 steps would be sufficient for convergence a priori, I didn't :). I had originally tried 1000 steps before staring at the concentration curves, at which point I then knew 200 was sufficient. So... no magic there.</p>"},{"location":"machine-learning/generating-markov-chains-dirichlet/#summary","title":"Summary","text":"<p>This was a bit of a whirlwind tour of a notebook, in that there were many concepts and ideas tied together into one \"project\".</p> <p>With respect to HMMs, it's super important to have a proper mental model of each of its components. In the case of expressing the idea that we have a restricted number of states, we may engineer this into the model by taking advantage of column-wise heaviness in a restricted subset of states. This is mathematically most naturally incorporated by composint together row-wise Dirichlet-distributed probability arrays. We can also mathematically engineer consistency in states by taking advantage of high Beta-distributed values. Compose the two together, and we get a transition matrix that favours entry into a small number of states with stability in there.</p> <p>Beyond that lesson, we saw the power of composable programs using JAX. <code>vmap</code>, <code>jit</code>, <code>lax.scan</code>, and more from the JAX toolkit gives us the ability to write performant programs that sort of \"just make sense\", once you know what their idioms are. Specifically:</p> <ul> <li><code>vmap</code> is a vanilla for-loop, processing an elementary function over an axis of an array.</li> <li><code>lax.scan</code> is a carry-over for-loop, processing the result of a previous iteration, with accumulation of history as well.</li> <li><code>jit</code> gives you just-in-time compilation of a function.</li> <li><code>grad</code> gives you gradients of any arbitrary function written in a JAX-compatible, pure functional fashion.</li> </ul> <p>In particular, getting used to the <code>lax.scan</code> idioms has been a literal door-opener. We can now write really performant loops that use results from previous iterations, such as a gradient descent training loop or an MCMC sampler. Using <code>lax.scan</code>, we wrote:</p> <ul> <li>A Markov chain state generator</li> <li>A generator of Dirichlet-distributed weights from i.i.d. Beta distribution draws</li> <li>A reverse generator/estimator of Beta distribution draws from Dirichlet-distributed weights (with some inaccuracies of course, due to a lack of information).</li> <li>A fully-compiled gradient descent training loop that ran extremely fast.</li> </ul> <p>And using <code>vmap</code>, we were able to do all sorts of vanilla loops, but the one I want to highlight is our ability to <code>vmap</code> the compiled training loop across multiple transition matrices. The fact that this actually works never has left me in awe. Props to the JAX team here! Opens the door to <code>vmap</code>-ing training loops across random starting points (i.e. a split PRNGKey, much like what we did in the HMM state generator).</p> <p>The trade-off is that we don't get nice progress bars, of course, which require that we break out of the compiled loop to show the current state. But the compilation speedups provided the opportunity to build our compiled tensor program end-to-end. We could verify first that things ran correctly on a moderately-sized number of iterations, before finally estimating how long we would need to go until convergence and letting the program run on its own. I'm sure this little change isn't too hard to adapt to, but will give you access to a whole new world of differential tensor programming that is just cool!</p> <pre><code>%watermark\n</code></pre> <pre><code>Last updated: 2022-12-23T14:38:05.144500-05:00\n\nPython implementation: CPython\nPython version       : 3.10.8\nIPython version      : 8.7.0\n\nCompiler    : GCC 10.4.0\nOS          : Linux\nRelease     : 5.15.0-53-generic\nMachine     : x86_64\nProcessor   : x86_64\nCPU cores   : 8\nArchitecture: 64bit\n</code></pre> <pre><code>%watermark --iversions\n</code></pre> <pre><code>jax       : 0.4.1\nseaborn   : 0.12.1\nmatplotlib: 3.6.2\n</code></pre>"},{"location":"machine-learning/graph-nets/","title":"An attempt at demystifying graph deep learning","text":""},{"location":"machine-learning/graph-nets/#introduction","title":"Introduction","text":"<p>There are a ton of great explainers of what graph neural networks are. However, I find that a lot of them go pretty deep into the math pretty quickly. Yet, we still are faced with that age-old problem: where are all the pics?? As such, just as I had attempted with Bayesian deep learning, I'd like to try to demystify graph deep learning as well, using every tool I have at my disposal to minimize the number of equations and maximize intuition using pictures. Here's my attempt, I hope you find it useful!</p>"},{"location":"machine-learning/graph-nets/#graphs","title":"Graphs","text":"<p>In my Network Analysis Made Simple tutorial, we see that the term \"graph\" are really nothing more than a synonym for networks. Defined strictly, graphs are comprised of nodes, i.e. entities, and edges that define relations between nodes. Examples are social networks (nodes = people, edges = friendship), and flight networks (nodes = airports, edges = flights that exist between the two networks).</p> <p>Pictorially, we'd usually draw something that looks like this:</p> <p></p> <p>A graph G, in really concise mathematical notation, can be represented as G = (V, E), or in plain English, an unordered collection of vertices (a synonym for nodes) and an unordered collection of edges.</p>"},{"location":"machine-learning/graph-nets/#graphs-as-arrays","title":"Graphs as arrays","text":"<p>One thing that's a really neat property of graphs is that we can actually represent them as arrays. This is covered in the Linear Algebra section of Network Analysis Made Simple; I also wrote about this in the earlier sections of an essay I wrote on message passing that I'd encourage you to check it out in its entirety, but nonetheless here's a simplified version to introduce you to the key ideas.</p> <p>Because graphs are comprised of nodes and edges, we need to figure out a way of representing both of them as arrays. Coming up, we'll explore how this can be done.</p>"},{"location":"machine-learning/graph-nets/#representing-nodes-as-arrays","title":"Representing nodes as arrays","text":"<p>Let's start with nodes, our entities. Our nodes, being entities, may have certain properties. Let's use a concrete example of molecules, which is what I would consider a minimally complex example for exploring key ideas. (I use minimally complex examples as my anchoring examples to learn an idea, I hope this one is useful for you too!)</p> <p>Imagine we have a molecule of ethanoic acid. It's comprised of two carbons, two oxygen, and four hydrogens joined in the following fashion:</p> <p></p> <p>Each atom is a node, and edges are bonds between the atoms. Each atom, or node, carries properties or features which can be represented on the array. For example, there's the atomic mass (a floating point number, or integer if you desire to round it off). There's the valence of the atom as well. And many more! To keep things simple, I'll stick to just these two for now. As you can see, you'll end up with a vector of features per node. If we stack them up together, we get what we call a node feature matrix, which will become handy later.</p> <p></p>"},{"location":"machine-learning/graph-nets/#representing-edges-as-arrays","title":"Representing edges as arrays","text":"<p>Similarly, there is an array representation of edges too! If you imagine lining up all of the atoms inside the ethanoic acid molecule along a square matrix, you can fill in the square matrix based on whether an edge exists between those two atoms. It'll look something like this:</p> <p></p> <p>If we wanted to, though, we could get really granular! We could have on adjacency matrix that shows us all of the single bonds, and another adjacency matrix that shows all of the double bonds. Or we could have an adjacency matrix that is weighted based on the number of bonds between two atoms.</p> <p></p> <p>Those are all perfectly valid representations of the edges of a graph, each with their own semantic meaning and tradeoffs for whatever modelling problem you might have.</p>"},{"location":"machine-learning/graph-nets/#message-passing","title":"Message Passing","text":"<p>It's now time to think about message passing. Message passing is a very well-known operation in the network science and machine learning worlds, but you don't have to be intimidated about it! At its core, it's actually really simple, and you'll see that it totally has a linear algebra interpretation that, in my opinion, is pretty elegant!</p> <p>Let's start by defining what message passing on a graph is.</p> <p>Message passing on a graph is kind of what you would intuit it to be: if I have a message on a node, we want to pass the message to other nodes on the graph. Our message can only travel along the edges, though - otherwise, the structure of the graph would be irrelevant.</p> <p>Here's an example that is really, really simplified. (We'll see the exact same ideas in action on our ethanoic acid example in a short moment.) This example is drawn from my tutorial on network analysis, in particular, from the linear algebra section.</p> <p>Let's say we have a directed chain graph of four nodes. A \"message\" lives on the first node, and we want to pass it through the graph. In a first message passing iteration, we take the message on the first node and pass it to the next one. In the iteration after, we take the message and pass it to the next node. So on and so forth until we reach the last node. Here, there's no place the message can go; regardless of how many more steps we might desire to run message passing, the message won't go anywhere.</p> <p>As it turns out, there's a linear algebra interpretation of message passing! For the four node chain graph, we can consider the message as being a property of a node. For a node possessing the message, or more generally a copy of the message, we can assign it a value of 1. If a node doesn't have a copy of the message, then we assign it a value of 0. Here's where the magic happens. When we dot product the adjacency matrix against this message vector (which is akin to our node feature vector), the message will move from the first node to the next node. (Verify this point for yourself by hand!)</p> <p></p> <p>So it seems we can pass messages around nodes by simply doing dot products! Let's see this in action by looking at the same operation in action for ethanoic acid. Here, all nodes receive messages from their neighbors simultaneously. Notice how the graph looks like after performing message passing. The carbon attached to three hydrogens has a different value from the carbon attached to the oxygens; their value is the sum of their neighbors. \"Summing up neighbors\" is literally all that we are doing in this form of message passing, in other words, when we do a dot product of the adjacency matrix with a node feature matrix.</p> <p></p> <p>At this point, I'd encourage you to pause reading and ponder over the ideas this far. It was pretty mind-blowing for me when I first really made the connection between message passing and linear algebra. I spent a few good weeks mulling the ideas I just described.</p>"},{"location":"machine-learning/graph-nets/#message-passing-neural-networks","title":"Message Passing + Neural Networks","text":"<p>We're now going to see how message passing gets embedded inside a neural network - a network that learns on network. (Pretty meta, if you ask me!)</p> <p>To anchor our understanding, we will start with graph deep learning in a supervised learing setting, where our learning task is to predict a scalar number for every graph in a collection of graphs. One classic example where this has been done before is in chemical property prediction, the first of which I encountered being a paper by my deep learning teacher David Duvenaud on learning molecular fingerprints. Here, each input into the neural network is a graph, rather than a vector. For comparison, classical deep learning starts with rows of i.i.d. data that are fed through a neural network.</p> <p>We know that neural networks are composed of chains of math functions. (Really, that's all neural network models are at their core!) Feed forward neural networks chain together dot products; convolutional neural networks add in n-dimensional convolutions in the mix; meanwhile, recurrent neural networks have lag as part of its structure. Each of these provide inductive biases. By the way, that term, in my opinion, is nothing more than a fancy way of saying that we encode prior knowledge of our data generating process into the mathematical model. In each case, though, on a per-i.i.d. sample basis, we pass in one n-dimensional array at a time.</p> <p></p> <p>To disambiguate a point, a friend of mine Jacob raised the point that autoregressive time series are not i.i.d., for which he is definitely correct. However, if the definition of a sample is not one measurement in the series but one entire series, then we can have a collection of i.i.d. autoregressive timeseries measurements. My anchoring example is protein sequences; you can have long-range dependencies between positions in a sequence, so each position in a single protein sequence is not independent from others, but you can have a collection of i.i.d. protein sequences nonetheless.</p> <p>What about graph neural networks, then - what exactly does the input look like, and what is the general structure of such networks? To answer those questions, we're going to walk through three different kinds of graph neural network layers, the three that I'm most familiar with, as specific examples of the general form of graph neural nets. By the end of us walking through them, we should have a much clearer grasp of the math that underlies these models.</p>"},{"location":"machine-learning/graph-nets/#message-passing-neural-networks-mpnns","title":"Message Passing Neural Networks (MPNNs)","text":"<p>Message passing neural networks, as the name implies, means we inject message passing as an operation in the neural network. Let's see how we can define a message passing layer. Instead of accepting just a vector or n-dimensional image, the message passing layer accepts both the adjacency matrix and the node feature matrix and performs one round of message passing. But we know it's part of a neural network, so where does the neural piece come in? Well, right after message passing, we take the message-passed node feature matrix and apply the equivalent of a feed-forward neural network operation - linear transform + some activation function!</p> <p></p> <p>We can do this as many rounds as we desire - though it's important to note that we usually don't want to go for too many rounds. The long story cut short is that message passing has the effect of smoothing out node-level information, which could become undesirable.</p> <p>Once we have done one or two rounds of message passing, it might be desirable to have a vector representation of our graphs. But at the moment, we're stuck with two matrices - the adjacency matrix and the messaged-passed node feature matrix! Well, one canonical way of creating graph-level vector representations is to simply take a node-wise summation (or average) of the node feature matrix. Summations express that the size of a graph matters, while averages express that size should not matter. What you choose will depend on your particular problem. By the way, you as long as you have a reductive function that takes the node feature matrix and reduces it to a vector, you can satisfy the need to have a vector summary of a graph... as long as you can explain the semantics :). Regardless, let's call this layer a \"graph summary layer\".</p> <p></p> <p>Once we have a vector-level representation of our graph, we can stick classical feed forward neural network layers on top of the graph processing layers to obtain the thing we're trying to predict.</p> <p></p> <p>And that, my friends, is how a classical message passing neural network works! Here's the framework I use to remember how GNNs work:</p> <ol> <li>Message passing operations, followed by</li> <li>Graph summarization operations, followed by</li> <li>Feed forward networks to predict output.</li> </ol> <p>In the broader machine learning framework of \"model, loss, optimizer\", all we've worked on here is to design a model that fits the kind of data that we have. You get to keep constant the loss and the optimizer. Keep this framework memorized, and use MPNNs as the anchoring minimally complex example, and everything else we discuss coming will become crystal clear!</p>"},{"location":"machine-learning/graph-nets/#graph-laplacian-networks","title":"Graph Laplacian Networks","text":"<p>So what's up with graph laplacian networks? Well, really, at its core is all that's been done here is to replace each graph's adjacency matrix with their graph Laplacian matrix. That's it! Every other operation that you see in the pictures above remain the same.</p> <p>Edit: As Petar Veli\u010dkovi\u0107, a research scientist at DeepMind kindly pointed out to me on Twitter, in practice there are fancier forms of the graph Laplacian matrix that get used inside GNNs. Please see his tweet below.</p> <p>A great overview with very nice figures! Thank you!A comment on 'graph Laplacian networks' -- their core ideas stretch way deeper than just replacing A with L, imho. Perhaps a better way to phrase it is that such methods _in practice_ often just end up as products with poly(L).</p>\u2014 Petar Veli\u010dkovi\u0107 (@PetarV_93) August 4, 2021 <p>But what is the graph Laplacian? At its core, it's a measure of the local derivative of the graph, and we use it when we want to express that local differences in nodes matter to our graph-level property of interest. To explain this idea here any further would distract from our exploration of GNNs, so I would refer you to the Wikipedia entry for those of you who desire a deeper understanding of this category of graph matrices.</p>"},{"location":"machine-learning/graph-nets/#graph-attention-networks","title":"Graph Attention Networks","text":"<p>What about graph attention networks, then? Well, we know that the attention mechanism is nothing more than an operation that uses a neural network inside a neural network to learn weights of some kind. With graph attention networks, we use an embedded neural network to learn a per-graph adjacency-like matrix operator that we mask over with the adjacency matrix, which effectively gives us an edge attention matrix for message passing. (If you're feeling antsy and want to read an actual paper on this, check out the graph attention networks paper paper.)</p> <p>And that's it! Everything else about the model can stay constant.</p> <p>Graph attention layers give us the most general message, data-driven passing operator that we can imagine. Rather than fix the message passing operator a priori, we simply learn the operator in a data-driven fashion.</p>"},{"location":"machine-learning/graph-nets/#general-form-of-graph-neural-networks","title":"General form of graph neural networks","text":"<p>Okay, so we've explored three examples of graph neural networks. What exactly is general here? Here's the answer:</p> <ol> <li>We must have a message passing operator, A. It can be learned or it can be provided a priori and remain fixed throughout the neural network. You basically have the freedom to define any form of A that you need!</li> <li>We need to have node feature matrices, F.</li> <li>For convenience, we usually transform our graphs' array representations A and F into a summary vector that then gets processed by a feed forward neural network.</li> </ol> <p>Again, what remains relatively constant is the structure of the model - some form of generalized message passing followed by some form of graph summarization followed by a top model. As long as you have some sort of semantically-meaningful square matrix A and a semantically-meaningful node feature matrix F, you can follow the message passing -&gt; summarization -&gt; feed forward structure to build your GNN models.</p>"},{"location":"machine-learning/graph-nets/#mpnns-and-convolution-operations","title":"MPNNs and convolution operations","text":"<p>When David taught me about graph neural networks, one idea really clicked: how message passing generalizes the grid convolution to graphs - which is why the \"graph convolution\" term shows up in the deep learning literature. Let's explore how this is the case by looking carefully at a simple grid convolution and a simple graph convolution.</p> <p>Starting off with the simplest grid convolution, let's say we have a 5x5 pixel image with a 3x3 convolution operator comprised of 1s everywhere except the middle cell. When using the convolution operation (with certain border settings), we get the following result:</p> <p></p> <p>We actually can re-represent the grid of values as a lattice graph and do a message passing operation to obtain the same result:</p> <p></p> <p>And in doing so, we thus can see that the message passing operation using the adjacency matrix as our message passing operator is equivalent to a convolution with a filter of 1s surrounding a zero. Message passing on graphs more generally simply requires that we relax the condition that our graph be a lattice graph!</p>"},{"location":"machine-learning/graph-nets/#graph-learning-tasks","title":"Graph learning tasks","text":"<p>Thus far, we explored how graph neural networks work in the context of a supervised machine learning problem where we want to build a model that links graphs as inputs to a property of the entire graph. Beyond this single learning task, though, there are other learning tasks. From a talk that my friend Chris Lin delivered, I learned that there are a few primary categories of graph learning tasks. Here's a short overview of those learning tasks.</p>"},{"location":"machine-learning/graph-nets/#graph-level-prediction","title":"Graph-level prediction","text":"<p>This is the category of problem we leveraged above to explore the internals of graph neural networks. Basically we frame the problem as \"given graph, predict number\". It assumes that we have a bunch of i.i.d. data that have a natural graph structure as its representation.</p>"},{"location":"machine-learning/graph-nets/#node-labelling","title":"Node labelling","text":"<p>This category of problems typically deals with one large graph and its adjacency matrix, and our desired output is a label (or a suite of labels) that need to be annotated on nodes. Some of the nodes have labels while others don't; message passing could be handy here because it encodes the homophily assumption - that is, for problems where we expect that similar things will be linked together, we can encode this inductive bias into our models much more naturally than other structural equations.</p>"},{"location":"machine-learning/graph-nets/#edge-presenceabsence-prediction","title":"Edge presence/absence prediction","text":"<p>This is another class of problems where usually we are given one large graph, and the desired output is either whether an edge ought to exist between two nodes. This can be framed in terms of predicting the entries of the graph adjacency matrix, for example.</p>"},{"location":"machine-learning/graph-nets/#summary","title":"Summary","text":"<p>Here, we started with array representations of single graphs. Then, we covered the message passing operation, which is a key operation in graph neural networks. We explored how we can compose message passing, graph summarization, and feed forward neural networks to do graph-level property prediction. Finally, we did a quick tour through the general categories of graph learning tasks in which graph neural networks are used.</p> <p>It's taken me 2-3 months of procrastination, brainstorming, and more to write all of this down, and even then, I doubt I have comprehensively covered all of the fundamentals of graph deep learning. That said, I believe that in this essay, we've given ourselves a great starting point - you might even say, a launchpad - for delving into the vast swathe of literature out there.</p>"},{"location":"machine-learning/graph-nets/#with-thanks","title":"With Thanks","text":"<p>I would like to give special thanks to my Patreon supporters, Alejandro, Rafael, Fazal, Brian, Hector, Carol, and Eddie. With the new kid, switching roles, and more of life hitting me in the face, it's taken a bit of time for me to get this essay out, but I'm finally glad to have it done!</p>"},{"location":"machine-learning/graph-nets/#further-reading","title":"Further Reading","text":"<ul> <li>I wrote an essay on how to efficiently represent message passing.</li> <li>Others have also made attempts at explainin GNNs, including The AI Summer.</li> <li>The Awesome GNN Repository.</li> </ul> <p>(The literature is expanding way too quickly; it's going to be hard to keep up!)</p>"},{"location":"machine-learning/llm-dev-guide/","title":"A Developer-First Guide to LLM APIs (March 2023)","text":"<p>Large Language Models (LLMs) are having a moment now! We can interact with them programmatically in three ways: OpenAI's official API, LangChain's abstractions, and LlamaIndex. How do we choose among the three? I'd like to use a minimally complex example to showcase how we might make this decision.</p>"},{"location":"machine-learning/llm-dev-guide/#the-problem","title":"The Problem","text":"<p>I blog.</p> <p>(Ok, well, that's quite the understatement, given that you're reading it right now.)</p> <p>As you probably can tell from my blog's design, I need to summarize the blog post for the blog landing page. Because I usually share the post on LinkedIn, I also need to generate a LinkedIn post advertising the blog post. I've heard that emojis are great when sharing posts on LinkedIn, but I can only sometimes remember the names of appropriate emojis. Some automation help would be great here. Finally, if I can make a better title than I initially thought of, that would also constitute an improvement.</p> <p>I've decided that I wanted a tool that can summarize my blog posts, generate appropriately emojified LinkedIn posts for me to advertise those posts, and provide a catchy and engaging title without being clickbait. These are tedious to write! Wouldn't it be great to have a tool that can generate both? That's the use case for LLMs!</p>"},{"location":"machine-learning/llm-dev-guide/#desiderata","title":"Desiderata","text":"<p>Here's the list of requirements I have to build the tool I want.</p> <p>Firstly, I should be able to provide the text of a blog post as input and get back:</p> <ol> <li>A proposed title according to my desired specifications,</li> <li>A summary to put onto my website's blog listing page, and</li> <li>A LinkedIn post that I can use to share with others.</li> </ol> <p>Secondly, the tool should minimize token usage. Tokens equal money spent on this task, so saving on token usage would increase my leverage trading time for money.</p> <p>Finally, because I'm still writing some code to implement this tool, I'd like to use a package that would provide the most easily maintained code. Of course, that means a subjective judgment of how simple the abstractions are.</p> <p>To that end, I will show how to implement this writing tool using three APIs currently available: the official OpenAI Python API, the LangChain API, and the LlamaIndex API. This exercise lets us see which ones are most suitable for this particular use case. We will be using GPT-4 everywhere, as its quality is known to be superior to GPT-3.5. Finally, I will focus on blog post summary generation and LinkedIn post generation when comparing the APIs before choosing one framework to implement the complete program.</p>"},{"location":"machine-learning/llm-dev-guide/#the-test-blog-post","title":"The test blog post","text":"<p>The blog post I'll use for implementing this is my most recent one on the Arc browser. I will be passing in the Lektor raw source without the title. The raw source is available on my website repository.</p> <p>According to <code>tiktoken</code>, this text uses 1436 tokens to encode:</p> <pre><code>blog_text = ... # taken from my raw source.\n\nencoder = tiktoken.get_encoding(\"cl100k_base\")\ntokens = encoder.encode(blog_text)\nlen(tokens)\n</code></pre> <pre><code>1436\n</code></pre> <p>This is a constant throughout the post.</p>"},{"location":"machine-learning/llm-dev-guide/#prompts","title":"Prompts","text":"<p>The prompts that I will use to generate the desired text are as follows. If the three prompts are too long to remember, you can focus on the one for summarization as an anchoring example.</p>"},{"location":"machine-learning/llm-dev-guide/#summary","title":"Summary","text":"<pre><code>summarization_prompt = f\"\"\"You are a blog post summarization bot.\nYour take a blog post and write a summary of it.\nThe summary is intended to hook a reader into the blog post and entice them to read it.\nYou should add in emojis where appropriate.\n\nMy previous summaries sounded like this:\n\n1. I finally figured out how to programmatically create Google Docs using Python. Along the way, I figured out service accounts, special HTML tags, and how to set multi-line environment variables. Does that tickle your brain?\n2. Here is my personal experience creating an app for translating Ark Channel devotionals using OpenAI's GPT-3. In here, I write about the challenges I faced and the lessons I learned! I hope it is informative for you!\n3. I discuss two Twitter threads that outline potential business ideas that could be built on top of ChatGPT3, a chatbot-based language model. What's the tl;dr? As mankind has done over and over, we build machines to solve mundane and repetitive tasks, and ChatGPT3 and other generative models are no exception!\n\nThe summary you generate should match the tone and style of the previously-generated ones.\n\"\"\"\n</code></pre>"},{"location":"machine-learning/llm-dev-guide/#linkedin-post-prompt","title":"LinkedIn Post Prompt","text":"<pre><code>linkedin_prompt = \"\"\"You are a LinkedIn post generator bot.\nYou take a blog post and write a LinkedIn post.\nThe post is intended to hook a reader into reading the blog post.\nThe LinkedIn post should be written with one line per sentence.\nEach sentence should begin with an emoji appropriate to that sentence.\nThe post should be written in professional English and in first-person tone.\n\"\"\"\n</code></pre>"},{"location":"machine-learning/llm-dev-guide/#title-prompt","title":"Title Prompt","text":"<pre><code>title_prompt = \"\"\"You are a blog post title generator.\nYou take a blog post and write a title for it.\nThe title should be short, ideally less than 100 characters.\nIt should be catchy but not click-baity,\nfree from emojis, and should intrigue readers,\nmaking them want to read the summary and the post itself.\nEnsure that the title accurately captures\nthe contents of the post.\n\"\"\"\n</code></pre>"},{"location":"machine-learning/llm-dev-guide/#setup-api-key","title":"Setup API key","text":"<p>Using OpenAI\u2019s API requires we set the OpenAI API key before doing anything; this is also true for using LangChain and LlamaIndex. As always, to adhere to reasonable security practices when developing locally, we should store the API key as an environment variable in a <code>.env</code> file listed in the <code>.gitignore</code>of a repository and then load the API key in our Jupyter notebooks. Here is how we implement it. Firstly, the <code>.env</code> file:</p> <pre><code> # .env\nexport OPENAI_API_KEY=\"...\"\n</code></pre> <p>And then, at the top of our Jupyter notebook or Python script:</p> <pre><code>from dotenv import load_dotenv\nimport openai\nimport os\n\nload_dotenv()\n\nopenai.api_key = os.getenv(\"OPENAI_API_KEY\")\n</code></pre>"},{"location":"machine-learning/llm-dev-guide/#implementation-using-openais-api","title":"Implementation using OpenAI's API","text":""},{"location":"machine-learning/llm-dev-guide/#summarization","title":"Summarization","text":"<p>Let\u2019s start off using the OpenAI Python API. To use GPT-4, we need to provide a chat history of messages that looks something like this:</p> <pre><code>messages = [\n    {\"role\": \"system\", \"content\": summarization_prompt},\n    {\"role\": \"user\", \"content\": f\"Here is my blog post source: {blog_post}\"\n]\n</code></pre> <p>Then, we pass the message history into the OpenAI chat completion class</p> <pre><code>result = openai.ChatCompletion.create(messages=messages, model=\"gpt-4\", temperature=0.0)\n</code></pre> <p>Once the API call has returned, we can see what gets returned:</p> <pre><code>summary = result[\"choices\"][0][\"message\"][\"content\"]\nprint(summary)\n</code></pre> <pre><code>I recently got my hands on the new [Arc browser](https://arc.net/) and I'm loving it! \ud83e\udd29 Arc reimagines the browser as a workspace, helping us manage our chaotic tabs and multiple projects. Some cool features include tabs that expire \u23f3, spaces for grouping tabs \ud83d\uddc2\ufe0f, rapid switching between tabbed and full-screen mode \ud83d\udda5\ufe0f, side-by-side view for multitasking \ud83d\udcd1, and automatic developer mode for locally hosted sites \ud83d\udd27. Arc's focus on UI design is a game-changer for productivity and focus! Check out my first impressions and see if Arc could be your new favorite browser! \ud83d\ude80\n</code></pre> <p>The total number of tokens used here is:</p> <pre><code>len(encoder.encode(summarization_prompt)) \\\n+ len(encoder.encode(human_message[\"content\"])) \\\n+ len(encoder.encode(summary))\n</code></pre> <pre><code>1870\n</code></pre>"},{"location":"machine-learning/llm-dev-guide/#linkedin-post","title":"LinkedIn Post","text":"<p>Now, let's make the LinkedIn post.</p> <pre><code>linkedin_prompt = {\"role\": \"system\", \"content\": linkedin_prompt}\nmessages = [linkedin_prompt, human_message]\n\nresult = openai.ChatCompletion.create(messages=messages, model=\"gpt-4\", temperature=0.0)\n\nlinkedin_post = result[\"choices\"][0][\"message\"][\"content\"]\nprint(linkedin_post)\n</code></pre> <pre><code>\ud83d\ude80 Just tried the new [Arc browser](https://arc.net/) for 24 hours!\n\n\ud83e\udde0 It's designed to fit the modern multitasker's brain.\n\n\u23f3 Love the tabs that expire feature - goodbye clutter!\n\n\ud83c\udf10 Spaces for grouping tabs - perfect for juggling multiple projects.\n\n\ud83d\udd0d Rapid switching between tabbed and full-screen mode for better focus.\n\n\ud83d\udccf Side-by-side view for efficient multitasking.\n\n\ud83d\udc68\u200d\ud83d\udcbb Automatic developer mode for locally hosted sites - a developer's dream!\n\n\ud83c\udf1f Overall, Arc is a game-changer for productivity and focus.\n\n\ud83d\udcd6 Read my full experience in the blog post [here](&lt;blog_post_link&gt;).\n</code></pre> <p>The total number of tokens used here is:</p> <pre><code>len(encoder.encode(linkedin_prompt)) \\\n+ len(encoder.encode(human_message[\"content\"])) \\\n+ len(encoder.encode(linkedin_post))\n</code></pre> <pre><code>1669\n</code></pre>"},{"location":"machine-learning/llm-dev-guide/#cost-accounting","title":"Cost Accounting","text":"<p>Tabulating the total cost of tokens, we have 3c per 1,000 tokens for prompts and 6c per 1,000 token for generated texts. We can do the math easily here:</p> <pre><code># Cost\nprompt_encoding_lengths = (\n    len(encoder.encode(linkedin_prompt)) \\\n    + len(encoder.encode(human_message[\"content\"])) \\\n    + len(encoder.encode(summarization_prompt)) \\\n    + len(encoder.encode(human_message[\"content\"]))\n)\n\ngenerated_encoding_lengths = (\n    len(encoder.encode(linkedin_post)) + \\\n    len(encoder.encode(summary))\n)\n\ncost = (\n    0.03 * prompt_encoding_lengths + 0.06 * generated_encoding_lengths\n) / 1000\n\nprint(cost)\n</code></pre> <pre><code>0.11657999999999999\n</code></pre> <p>Or about 12c to perform this operation.</p> <p>How much ROI do we get here? Excluding benefits, equity, and more, a new Ph.D. grad data scientist is paid about <code>$150,000</code> (give or take) per year in the biomedical industry in 2023. Assuming about 250 days of work per year at an average of 8 hours per day, we're talking about an hourly rate of <code>$75</code>/hr at that salary. If it takes that person 10 minutes to cook up a summary and LinkedIn post (which is about how long I take - excluding figuring out what emojis to put in because that's, for me, bloody time-consuming.), then we're looking at <code>$12.5</code> worth of time put into crafting that post. Looking at just the cold hard numbers, we're looking at a 100X cost improvement by using GPT-4 as a writing aid.</p>"},{"location":"machine-learning/llm-dev-guide/#implementation-using-langchains-api","title":"Implementation using LangChain's API","text":"<p>Now, let's do the same thing, except with the LangChain API. Here, we'll be using LangChain's Chain API.</p>"},{"location":"machine-learning/llm-dev-guide/#summarization_1","title":"Summarization","text":"<p>First off, summarization:</p> <pre><code>from langchain.chains import LLMChain\nfrom langchain.prompts.chat import (\n    ChatPromptTemplate,\n    HumanMessagePromptTemplate,\n    SystemMessagePromptTemplate\n)\nfrom langchain.prompts import PromptTemplate\n\nsystem_message_prompt = SystemMessagePromptTemplate(\n    prompt=PromptTemplate(\n        template=summarization_prompt,\n    input_variables=[],\n    ),\n)\nfrom langchain.chat_models import ChatOpenAI\n\n\nchat = ChatOpenAI(model_name=\"gpt-4\", temperature=0.0)\n\nhuman_message_prompt = HumanMessagePromptTemplate(\n    prompt=PromptTemplate(\n        template=\"Here is my post:\\n\\n {blog_post}?\",\n        input_variables=[\"blog_post\"],\n    )\n)\n\nchat_prompt_template = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\nsummary_chain = LLMChain(llm=chat, prompt=chat_prompt_template)\nsummary = summary_chain.run(blog_post=blog_text)\nprint(summary)\n</code></pre> <pre><code>I recently tried out the Arc browser \ud83c\udf10, which reimagines the browser as a workspace to boost productivity! \ud83d\ude80 After 24 hours of use, I'm loving its features: tabs that expire \u23f3, spaces for organizing tabs \ud83d\uddc2\ufe0f, rapid switching between tabbed and full-screen mode \ud83d\udda5\ufe0f, side-by-side view for multitasking \ud83d\udcd1, and automatic developer mode for locally hosted sites \ud83d\udcbb. Arc's focus on UI design helps us stay focused and organized in our digital lives. Curious to know more? Dive into my first impressions! \ud83d\ude03\n</code></pre> <p>According to tiktoken:</p> <pre><code>len(encoder.encode(summary))\n</code></pre> <p>The generated text was 191 tokens. <pre><code>len(encoder.encode(system_message_prompt.format_messages()[0].content))\n</code></pre></p> <p>The system prompt was 237 tokens.</p> <pre><code>len(encoder.encode(human_message_prompt.format_messages(blog_post=blog_text)[0].content))\n</code></pre> <p>The human message prompt was 1442.</p> <p>We'll keep those numbers in mind when doing the final cost accounting.</p>"},{"location":"machine-learning/llm-dev-guide/#linkedin-post_1","title":"LinkedIn Post","text":"<p>Now, let's make the LinkedIn post.</p> <pre><code>linkedin_system_prompt = SystemMessagePromptTemplate(\n    prompt=PromptTemplate(\n        template=linkedin_prompt,\n    input_variables=[],\n    ),\n)\n\nlinkedin_blog_prompt = HumanMessagePromptTemplate(\n    prompt=PromptTemplate(\n        template=\"Here is the blog post:\\n\\n{blog_post}\",\n        input_variables=[\"blog_post\"],\n    )\n)\n\nlinkedin_chat_prompt_template = ChatPromptTemplate.from_messages([linkedin_system_prompt, linkedin_blog_prompt])\n\nlinkedin_chain = LLMChain(llm=chat, prompt=linkedin_chat_prompt_template)\n\nlinkedin_post = linkedin_chain.run(blog_post=blog_text)\nprint(linkedin_post)\n</code></pre> <pre><code>\ud83c\udf89 Just tried the new [Arc browser](https://arc.net/) and I'm loving it! \ud83e\udde0\n\n\ud83d\udd52 Arc's unique features like tabs that expire and spaces for different projects help me stay focused and organized. \ud83d\udccc\n\n\ud83d\ude80 Check out my blog post for a detailed review and first impressions of this game-changing browser: [Arc Browser: First Impressions](&lt;blog post link&gt;) \ud83c\udf10\n\n\ud83d\udc68\u200d\ud83d\udcbb Are you ready to revolutionize your browsing experience? #ArcBrowser #Productivity #Tools\n</code></pre> <p>Doing an accounting of the tokens once again:</p> <pre><code>len(encoder.encode(linkedin_post))\n</code></pre> <p>The generated LinkedIn post used 151 generated tokens.</p> <pre><code>len(encoder.encode(linkedin_blog_prompt.format_messages(blog_post=blog_text)[0].content))\n</code></pre> <p>The blog post prompt itself used 1441 prompt tokens.</p> <pre><code>len(encoder.encode(linkedin_system_prompt.format_messages()[0].content))\n</code></pre> <p>And the system prompt used 71 tokens.</p>"},{"location":"machine-learning/llm-dev-guide/#cost-accounting_1","title":"Cost Accounting","text":"<p>As usual, let's do the accounting of tokens.</p> <pre><code># Cost Accounting\ngenerated_encodings_length = len(encoder.encode(linkedin_post)) + len(encoder.encode(summary))\n\nprompt_lengths = (\n    len(encoder.encode(human_message_prompt.format_messages(blog_post=blog_text)[0].content)) \\\n    + len(encoder.encode(system_message_prompt.format_messages()[0].content)) \\\n    + len(encoder.encode(linkedin_blog_prompt.format_messages(blog_post=blog_text)[0].content)) \\\n    + len(encoder.encode(linkedin_system_prompt.format_messages()[0].content))\n)\n\ncost = 0.03 * prompt_lengths + 0.06 * generated_encodings_length\nprint(cost / 1000)\n</code></pre> <pre><code>0.11648999999999998\n</code></pre> <p>As we can see, this also costs about 12c for the entire exercise.</p>"},{"location":"machine-learning/llm-dev-guide/#implementation-using-llamaindex","title":"Implementation using LlamaIndex","text":"<p>The way LlamaIndex works is different from the previous two frameworks. With OpenAI's and LangChain's APIs, we stuffed the entire document into the prompt for each task. With LlamaIndex, we can embed and store the text beforehand and then query it with a prompt for our LLM. Here is how it looks:</p>"},{"location":"machine-learning/llm-dev-guide/#embed-text","title":"Embed Text","text":"<p>To do this, we use the <code>GPTSimpleVectorIndex</code> provided by LlamaIndex:</p> <pre><code>from llama_index import Document, GPTSimpleVectorIndex, LLMPredictor\n\n# Index documents using GPT4\nllm_predictor = LLMPredictor(llm=chat)\ndocuments = [Document(blog_text)]\nindex = GPTSimpleVectorIndex(documents=documents, llm_predictor=llm_predictor)\n</code></pre> <p>Thanks to the built-in token accounting capabilities of LlamaIndex, we can see that building the index costed us 1637 tokens.</p> <pre><code>INFO:llama_index.token_counter.token_counter:&gt; [build_index_from_documents] Total LLM token usage: 0 tokens\nINFO:llama_index.token_counter.token_counter:&gt; [build_index_from_documents] Total embedding token usage: 1637 tokens\n</code></pre>"},{"location":"machine-learning/llm-dev-guide/#summarization_2","title":"Summarization","text":"<p>Let's start by creating a summary of the blog post.</p> <pre><code>response = index.query(summarization_prompt)\nprint(response.response)\n</code></pre> <pre><code>Discover the Arc browser, a game-changer that reimagines the browser as a workspace \ud83c\udf10! With features like tabs that expire \u23f3, spaces for grouping tabs \ud83d\udcc1, rapid switching between modes \u26a1, side-by-side view \ud83d\udc40, and automatic developer mode for locally hosted sites \ud83d\udd27, Arc is designed to boost your productivity and focus \ud83d\ude80. Dive into my 24-hour experience with this innovative browser and see how it fits my brain \ud83e\udde0!\n</code></pre> <p>Not bad! The response looks pretty good, though I might edit it a bit further.</p> <p>The token usage here is:</p> <pre><code>INFO:llama_index.token_counter.token_counter:&gt; [query] Total LLM token usage: 2026 tokens\nINFO:llama_index.token_counter.token_counter:&gt; [query] Total embedding token usage: 257 tokens\n</code></pre>"},{"location":"machine-learning/llm-dev-guide/#linkedin-post-generation","title":"LinkedIn Post Generation","text":"<p>Now, let's try generating a LinkedIn post the same way.</p> <pre><code>response = index.query(linkedin_prompt)\nprint(response.response)\n</code></pre> <pre><code>\ud83d\ude80 Just tried the new Arc browser for 24 hours!\n\ud83c\udf1f It reimagines the browser as a workspace, perfect for multitaskers.\n\u23f0 Love the tabs that expire feature, keeping my workspace clutter-free.\n\ud83c\udf08 Spaces help me group tabs by context, boosting focus and productivity.\n\ud83d\udda5\ufe0f Rapid switching between tabbed and full-screen mode is a game-changer.\n\ud83d\udc69\u200d\ud83d\udcbb Developer mode for locally hosted sites makes web development a breeze.\n\ud83d\udd17 Check out my detailed review on my blog and see how Arc can transform your browsing experience!\n</code></pre> <p>Not bad, it followed my instructions as well.</p> <p>Here's the token usage:</p> <pre><code>INFO:llama_index.token_counter.token_counter:&gt; [query] Total LLM token usage: 1874 tokens\nINFO:llama_index.token_counter.token_counter:&gt; [query] Total embedding token usage: 78 tokens\n</code></pre>"},{"location":"machine-learning/llm-dev-guide/#cost-accounting_2","title":"Cost Accounting","text":"<p>LlamaIndex did not break down the difference in prompt tokens vs. generated text tokens for me but split out embedding tokens. In calculating the cost, we will make the following assumptions:</p> <ul> <li>the cost of embedding tokens to be $0.0004 per 1000 tokens,</li> <li>of the LLM token usage budget, the part used for prompting can be calculated by subtracting the number of tokens used to encode the output from the reported LLM token usage.</li> </ul> <p>Crunching the numbers, we get:</p> <pre><code>embedding_tokens = 1637 + 257 + 78\n\nlinkedin_llm_tokens = 1874\nlinkedin_generated_tokens = len(encoder.encode(response.response))\n\nsummary_llm_tokens = 2026\nsummary_generated_tokens = len(encoder.encode(summary_response.response))\n\nllm_tokens_cost = (0.06 * (linkedin_generated_tokens + summary_generated_tokens) + 0.03 * (linkedin_llm_tokens - linkedin_generated_tokens) + 0.03 * (summary_llm_tokens - summary_generated_tokens)) / 1000\n\nembedding_tokens_cost = embedding_tokens * 0.0004 / 1000\n\nprint(f\"Cost: {llm_tokens_cost + embedding_tokens_cost}\")\n</code></pre> <pre><code>Cost: 0.1243588\n</code></pre> <p>Also about 12c for this exercise.</p>"},{"location":"machine-learning/llm-dev-guide/#generated-text-comparison","title":"Generated Text Comparison","text":""},{"location":"machine-learning/llm-dev-guide/#summarization_3","title":"Summarization","text":"<p>Here are the generated texts for summarization side-by-side:</p>"},{"location":"machine-learning/llm-dev-guide/#openais-summarization","title":"OpenAI's Summarization","text":"<pre><code>I recently got my hands on the new [Arc browser](https://arc.net/) and I'm loving it! \ud83e\udd29 Arc reimagines the browser as a workspace, helping us manage our chaotic tabs and multiple projects. Some cool features include tabs that expire \u23f3, spaces for grouping tabs \ud83d\uddc2\ufe0f, rapid switching between tabbed and full-screen mode \ud83d\udda5\ufe0f, side-by-side view for multitasking \ud83d\udcd1, and automatic developer mode for locally hosted sites \ud83d\udd27. Arc's focus on UI design is a game-changer for productivity and focus! Check out my first impressions and see if Arc could be your new favorite browser! \ud83d\ude80\n</code></pre>"},{"location":"machine-learning/llm-dev-guide/#langchains-summarization","title":"LangChain's Summarization","text":"<pre><code>I recently tried out the Arc browser \ud83c\udf10, which reimagines the browser as a workspace to boost productivity! \ud83d\ude80 After 24 hours of use, I'm loving its features: tabs that expire \u23f3, spaces for organizing tabs \ud83d\uddc2\ufe0f, rapid switching between tabbed and full-screen mode \ud83d\udda5\ufe0f, side-by-side view for multitasking \ud83d\udcd1, and automatic developer mode for locally hosted sites \ud83d\udcbb. Arc's focus on UI design helps us stay focused and organized in our digital lives. Curious to know more? Dive into my first impressions! \ud83d\ude03\n</code></pre>"},{"location":"machine-learning/llm-dev-guide/#llamaindexs-summarization","title":"LlamaIndex's Summarization","text":"<pre><code>Discover the Arc browser, a game-changer that reimagines the browser as a workspace \ud83c\udf10! With features like tabs that expire \u23f3, spaces for grouping tabs \ud83d\udcc1, rapid switching between modes \u26a1, side-by-side view \ud83d\udc40, and automatic developer mode for locally hosted sites \ud83d\udd27, Arc is designed to boost your productivity and focus \ud83d\ude80. Dive into my 24-hour experience with this innovative browser and see how it fits my brain \ud83e\udde0!\n</code></pre>"},{"location":"machine-learning/llm-dev-guide/#style-verdict","title":"Style verdict","text":"<p>Of the three, LlamaIndex's style is the furthest from my usual style, though, in some instances, I might choose the style generated by LlamaIndex to change things up on my blog.</p>"},{"location":"machine-learning/llm-dev-guide/#linkedin-post-generation_1","title":"LinkedIn Post Generation","text":""},{"location":"machine-learning/llm-dev-guide/#openais-post","title":"OpenAI's Post","text":"<pre><code>\ud83d\ude80 Just tried the new [Arc browser](https://arc.net/) for 24 hours!\n\n\ud83e\udde0 It's designed to fit the modern multitasker's brain.\n\n\u23f3 Love the tabs that expire feature - goodbye clutter!\n\n\ud83c\udf10 Spaces for grouping tabs - perfect for juggling multiple projects.\n\n\ud83d\udd0d Rapid switching between tabbed and full-screen mode for better focus.\n\n\ud83d\udccf Side-by-side view for efficient multitasking.\n\n\ud83d\udc68\u200d\ud83d\udcbb Automatic developer mode for locally hosted sites - a developer's dream!\n\n\ud83c\udf1f Overall, Arc is a game-changer for productivity and focus.\n\n\ud83d\udcd6 Read my full experience in the blog post [here](&lt;blog_post_link&gt;).\n</code></pre>"},{"location":"machine-learning/llm-dev-guide/#langchains-post","title":"LangChain's Post","text":"<pre><code>\ud83c\udf89 Just tried the new [Arc browser](https://arc.net/) and I'm loving it! \ud83e\udde0\n\n\ud83d\udd52 Arc's unique features like tabs that expire and spaces for different projects help me stay focused and organized. \ud83d\udccc\n\n\ud83d\ude80 Check out my blog post for a detailed review and first impressions of this game-changing browser: [Arc Browser: First Impressions](&lt;blog post link&gt;) \ud83c\udf10\n\n\ud83d\udc68\u200d\ud83d\udcbb Are you ready to revolutionize your browsing experience? #ArcBrowser #Productivity #Tools\n</code></pre>"},{"location":"machine-learning/llm-dev-guide/#llamaindexs-post","title":"LlamaIndex's Post","text":"<pre><code>\ud83d\ude80 Just tried the new Arc browser for 24 hours!\n\ud83c\udf1f It reimagines the browser as a workspace, perfect for multitaskers.\n\u23f0 Love the tabs that expire feature, keeping my workspace clutter-free.\n\ud83c\udf08 Spaces help me group tabs by context, boosting focus and productivity.\n\ud83d\udda5\ufe0f Rapid switching between tabbed and full-screen mode is a game-changer.\n\ud83d\udc69\u200d\ud83d\udcbb Developer mode for locally hosted sites makes web development a breeze.\n\ud83d\udd17 Check out my detailed review on my blog and see how Arc can transform your browsing experience!\n</code></pre>"},{"location":"machine-learning/llm-dev-guide/#style-verdict_1","title":"Style Verdict","text":"<p>Of the three, OpenAI's is furthest from how I usually write my LinkedIn posts, but in fairness, I didn't instruct the model with examples provided. If I were to choose, I would pick LlamaIndex's generated post.</p>"},{"location":"machine-learning/llm-dev-guide/#developer-experience-dx","title":"Developer Experience (DX)","text":"<p>For this simple use case, which would I go with? LlamaIndex, OpenAI's official API, or LangChain?</p> <p>As a lazy programmer, I would go with LlamaIndex. That's because I needed the fewest lines of code to reach my desired endpoints. It is easy to remember the API as well - you only need to remember the pattern:</p> <ol> <li><code>Document</code> to wrap the text that we imported,</li> <li><code>GPTSimpleVectorIndex</code> (or more generically <code>&lt;some&gt;Index</code>), and</li> <li><code>LLMPredictor</code> to wrap around LangChain's LLMs.</li> <li><code>&lt;Index&gt;.query(prompt)</code></li> </ol> <p>The biggest reason why LlamaIndex is best suited to this use case is that querying text is a relatively simple use case. We only need to load the document in a query-able fashion. Furthermore, as you can see from my code above, using LlamaIndex resulted in the least boilerplate code.</p> <p>That said, is LlamaIndex the right choice for every application? Probably not. As of March 2023, LangChain is geared towards much more complex (and autonomous) LLM use cases. So the abstractions that the library creators put in the library may be geared to design larger LLM applications rather than simple ones like we just did. And OpenAI's API is officially supported, which means it would likely have an official \"blessing\" in the long run.</p> <p>I have a penchant/bias for more tightly-controlled LLM applications, which means forgoing a chat interface in favour of a single-text-in-single-text-out interface. Indeed, while I see the usefulness of chat-based interfaces for exploration, I don't think it will become the dominant UI when embedding LLMs in applications. Rather, I predict that we'll see UI/UX researchers designing, on a per-application basis, whether to use a free-flowing chat UI or to use a more tightly controlled ad-lib-style interface, or even crazier interfaces! The factor that should matter the most here is the ROI gained in human time.</p> <p>Additionally, I predict that we will see data science teams use LLMs to bang out dozens of little utility apps much faster than previously possible. These utility apps will likely serve niche-specific but often-repeated use cases, and may serve as the basis of larger applications that get developed. For example, we'll probably see built apps that let users ad-lib a templated prompt. We'll see things like LLMs being the role of data engineer (e.g. structuring data from unstructured text) and domain-specific idea generators prompted by domain experts or domain needs (as we saw here),</p> <p>The short answer is that we'll see more customization to local contexts, with highest ROI use cases being prioritized.</p>"},{"location":"machine-learning/llm-dev-guide/#caveats-to-this-analysis","title":"Caveats to this analysis","text":"<p>There are some caveats here to what I've done that I'd like to point out.</p> <p>Firstly, I've used the most straightforward implementations, stuffing the most text into the prompt. Why? It is because I'm still exploring the libraries and their APIs and partly because so much development has happened quickly. Others will be able to figure out more efficient ways of implementing what we did here with their favourite framework.</p> <p>Secondly, this particular task is quite simple. LangChain, on the other hand, can generate much more complex programs, as we can see from its documentation. Critiquing LangChain's heavy abstractions would be unfair, as it's very likely designed for more complex autonomous applications.</p>"},{"location":"machine-learning/llm-dev-guide/#conclusion","title":"Conclusion","text":"<p>One application built three ways with three different libraries. We have seen how the outputs can differ even with the same prompts and how the developer experience can vary between the three. That said, I caution that these are early days. Libraries evolve. Their developers can introduce more overhead or reduce it. It's probably too early to \"pick a winner\"; as far as I can tell, it's less a competition and more a collaboration between these library developers. However, we can keep abreast of the latest developments and keep experimenting with the libraries on the side, finding out what works and what doesn't and adapting along the way.</p> <p>The near-term value proposition of GPT as a tool lies in its ability to automate low-risk, high-effort writing. We saw through cost calculations that LLMs can represent a 100X ROI in time saved. For us, as data scientists, that should be heartening! Where in your day-to-day work can you find a similar 100X ROI?</p>"},{"location":"machine-learning/markov-models/","title":"Markov Models From The Bottom Up, with Python","text":"<pre><code>%load_ext autoreload\n%autoreload 2\n%config InlineBackend.figure_format = 'retina'\n</code></pre> <pre><code>import pymc3 as pm\n</code></pre>"},{"location":"machine-learning/markov-models/#markov-models-from-the-bottom-up-with-python","title":"Markov Models From The Bottom Up, with Python","text":"<p>Markov models are a useful class of models for sequential-type of data. Before recurrent neural networks (which can be thought of as an upgraded Markov model) came along, Markov Models and their variants were the in thing for processing time series and biological data.</p> <p>Just recently, I was involved in a project with a colleague, Zach Barry, where we thought the use of autoregressive hidden Markov models (AR-HMMs) might be a useful thing. Apart from our hack session one afternoon, it set off a series of self-study that culminated in this essay. By writing this down for my own memory, my hope is that it gives you a resource to refer back to as well.</p> <p>You'll notice that I don't talk about inference (i.e. inferring parameters from data) until the end: this is intentional. As I've learned over the years doing statistical modelling and machine learning, nothing makes sense without first becoming deeply familiar with the \"generative\" story of each model, i.e. the algorithmic steps that let us generate data. It's a very Bayesian-influenced way of thinking that I hope you will become familiar with too.</p>"},{"location":"machine-learning/markov-models/#markov-models-what-they-are-with-mostly-plain-english-and-some-math","title":"Markov Models: What they are, with mostly plain English and some math","text":"<p>The simplest Markov models assume that we have a system that contains a finite set of states, and that the system transitions between these states with some probability at each time step t, thus generating a sequence of states over time. Let's call these states S, where</p> S = \\{s_1, s_2, ..., s_n\\} <p>To keep things simple, let's start with three states:</p> S = \\{s_1, s_2, s_3\\} <p>A Markov model generates a sequence of states, with one possible realization being:</p> \\{s_1, s_1, s_1, s_3, s_3, s_3, s_2, s_2, s_3, s_3, s_3, s_3, s_1, ...\\} <p>And generically, we represent it as a sequence of states x_t, x_{t+1}... x_{t+n}. (We have chosen a different symbol to not confuse the \"generic\" state with the specific realization. Graphically, a plain and simple Markov model looks like the following:</p> <p></p>"},{"location":"machine-learning/markov-models/#initializing-a-markov-chain","title":"Initializing a Markov chain","text":"<p>Every Markov chain needs to be initialized. To do so, we need an initial state probability vector, which tells us what the distribution of initial states will be. Let's call the vector p_S, where the subscript S indicates that it is for the \"states\".</p>  p_{init} = \\begin{pmatrix} p_1 &amp; p_2 &amp; p_3 \\end{pmatrix}  <p>Semantically, they allocate the probabilities of starting the sequence at a given state. For example, we might assume a discrete uniform distribution, which in Python would look like:</p> <pre><code>import numpy as np\np_init = np.array([1/3., 1/3., 1/3.])\n</code></pre> <p>Alternatively, we might assume a fixed starting point, which can be expressed as the p_S array:</p> <pre><code>p_init = np.array([0, 1, 0])\n</code></pre> <p>Alternatively, we might assign non-zero probabilities to each in a non-uniform fashion:</p> <pre><code># State 0: 0.1 probability\n# State 1: 0.8 probability\n# State 2: 0.1 probability\np_init = np.array([0.1, 0.8, 0.1])\n</code></pre> <p>Finally, we might assume that the system was long-running before we started observing the sequence of states, and as such the initial state was drawn as one realization of some equilibrated distribution of states. Keep this idea in your head, as we'll need it later.</p> <p>For now, just to keep things concrete, let's specify an initial distribution as a non-uniform probability vector.</p> <pre><code>import numpy as np\np_init = np.array([0.1, 0.8, 0.1])\n</code></pre>"},{"location":"machine-learning/markov-models/#modelling-transitions-between-states","title":"Modelling transitions between states","text":"<p>To know how a system transitions between states, we now need a transition matrix. The transition matrix describes the probability of transitioning from one state to another. (The probability of staying in the same state is semantically equivalent to transitioning to the same state.)</p> <p>By convention, transition matrix rows correspond to the state at time t, while columns correspond to state at time t+1. Hence, row probabilities sum to one, because the probability of transitioning to the next state depends on only the current state, and all possible states are known and enumerated.</p> <p>Let's call the transition matrix P_{transition}. The symbol etymology, which usually gets swept under the rug in mathematically-oriented papers, are as follows:</p> <ul> <li>transition doesn't refer to time but simply indicates that it is for transitioning states,</li> <li>P is used because it is a probability matrix.</li> </ul>  P_{transition} = \\begin{pmatrix}     p_{11} &amp; p_{12} &amp; p_{13}\\\\     p_{21} &amp; p_{22} &amp; p_{23}\\\\     p_{31} &amp; p_{32} &amp; p_{33}\\\\ \\end{pmatrix}  <p>Using the transition matrix, we can express that the system likes to stay in the state that it enters into, by assigning larger probability mass to the diagonals. Alternatively, we can express that the system likes to transition out of states that it enters into, by assigning larger probability mass to the off-diagonal.</p> <p>Alrighty, enough with that now, let's initialize a transition matrix below.</p> <pre><code>p_transition = np.array(\n    [[0.90, 0.05, 0.05],\n     [0.01, 0.90, 0.09],\n     [0.07, 0.03, 0.9]]\n)\np_transition\n</code></pre> <pre><code>array([[0.9 , 0.05, 0.05],\n       [0.01, 0.9 , 0.09],\n       [0.07, 0.03, 0.9 ]])\n</code></pre> <p>And just to confirm with you that each row sums to one:</p> <pre><code>assert p_transition[0, :].sum() == 1\nassert p_transition[1, :].sum() == 1\nassert p_transition[2, :].sum() == 1\n</code></pre>"},{"location":"machine-learning/markov-models/#equilibrium-or-stationary-distribution","title":"Equilibrium or Stationary Distribution","text":"<p>Now, do you remember how above we talked about the Markov chain being in some \"equilibrated\" state? Well, the stationary or equilibrium distribution of a Markov chain is the distribution of observed states at infinite time.</p> <p>An interesting property is that regardless of what the initial state is, the equilibrium distribution will always be the same, as the equilibrium distribution only depends on the transition matrix.</p> <p>Here's how to think about the equilibrium: if you were to imagine instantiating a thousand Markov chains using the initial distribution</p>  p_{init} = \\begin{pmatrix} 0.1 &amp; 0.8 &amp; 0.1 \\end{pmatrix}  <ul> <li>10% would start out in state 1</li> <li>80% would start out in state 2</li> <li>10% would start out in state 3</li> </ul> <p>However, if you ran each of the systems to a large number of time steps (say, 1 million time steps, to exaggerate the point) then how the states were distributed initially wouldn't matter, as how they transition from time step to time step begins to dominate.</p> <p>We could simulate this explicitly in Python, but as it turns out, there is a mathematical shortcut that involves simple dot products. Let's check it out.</p> <p>Assume we have an initial state and a transition matrix. We're going to reuse <code>p_init</code> from above, but use a different <code>p_transition</code> to make the equilibrium distribution values distinct. This will make it easier for us to plot later.</p> <pre><code>p_transition_example = np.array(\n    [[0.6,  0.2, 0.2],\n     [0.05, 0.9, 0.05],\n     [0.1,  0.2, 0.7]]\n)\n</code></pre> <p>To simulate the distribution of states in the next time step, we take the initial distribution <code>p_init</code> and matrix multiply it against the transition matrix.</p> <pre><code>p_next = p_init @ p_transition_example\np_next\n</code></pre> <pre><code>array([0.11, 0.76, 0.13])\n</code></pre> <p>We can do it again to simulate the distribution of states in the next time step after:</p> <pre><code>p_next = p_next @ p_transition_example\np_next\n</code></pre> <pre><code>array([0.117, 0.732, 0.151])\n</code></pre> <p>Let's now write a for-loop to automate the process.</p> <pre><code>p_state_t = [p_init]\n\nfor i in range(200):  # 200 time steps sorta, kinda, approximates infinite time :)\n    p_state_t.append(p_state_t[-1] @ p_transition_example)\n</code></pre> <p>To make it easier for you to see what we've generated, let's make the <code>p_state_t</code> list into a pandas DataFrame.</p> <pre><code>import pandas as pd\n\nstate_distributions = pd.DataFrame(p_state_t)\nstate_distributions\n</code></pre> 0 1 2 0 0.100000 0.800000 0.10000 1 0.110000 0.760000 0.13000 2 0.117000 0.732000 0.15100 3 0.121900 0.712400 0.16570 4 0.125330 0.698680 0.17599 ... ... ... ... 196 0.133333 0.666667 0.20000 197 0.133333 0.666667 0.20000 198 0.133333 0.666667 0.20000 199 0.133333 0.666667 0.20000 200 0.133333 0.666667 0.20000 <p>201 rows \u00d7 3 columns</p> <p>Now, let's plot what the distributions look like.</p> <pre><code>import matplotlib.pyplot as plt\nstate_distributions.plot();\n</code></pre> <p></p> <p>If you're viewing this notebook on Binder or locally, go ahead and modify the initial state to convince yourself that it doesn't matter what the initial state will be: the equilibrium state distribution, which is the fraction of time the Markov chain is in that state over infinite time, will always be the same as long as the transition matrix stays the same.</p> <pre><code>print(p_state_t[-1])\n</code></pre> <pre><code>[0.13333333 0.66666667 0.2       ]\n</code></pre> <p>As it turns out, there's also a way to solve for the equilibrium distribution analytically from the transition matrix. This involves solving a linear algebra problem, which we can do using Python. (Credit goes to this blog post from which I modified the code to fit the variable naming here.)</p> <pre><code>def equilibrium_distribution(p_transition):\n    n_states = p_transition.shape[0]\n    A = np.append(\n        arr=p_transition.T - np.eye(n_states),\n        values=np.ones(n_states).reshape(1, -1),\n        axis=0\n    )\n    b = np.transpose(np.array([0] * n_states + [1]))\n    p_eq = np.linalg.solve(\n        a=np.transpose(A).dot(A),\n        b=np.transpose(A).dot(b)\n    )\n    return p_eq\n\n# alternative\ndef equilibrium_distribution(p_transition):\n\"\"\"This implementation comes from Colin Carroll, who kindly reviewed the notebook\"\"\"\n    n_states = p_transition.shape[0]\n    A = np.append(\n        arr=p_transition.T - np.eye(n_states),\n        values=np.ones(n_states).reshape(1, -1),\n        axis=0\n    )\n    # Moore-Penrose pseudoinverse = (A^TA)^{-1}A^T\n    pinv = np.linalg.pinv(A)\n    # Return last row\n    return pinv.T[-1]\n\n\nprint(equilibrium_distribution(p_transition_example))\n</code></pre> <pre><code>[0.13333333 0.66666667 0.2       ]\n</code></pre>"},{"location":"machine-learning/markov-models/#generating-a-markov-sequence","title":"Generating a Markov Sequence","text":"<p>Generating a Markov sequence means we \"forward\" simulate the chain by:</p> <p>(1) Optionally drawing an initial state from p_S (let's call that s_{t}). This is done by drawing from a multinomial distribution:</p> s_t \\sim Multinomial(1, p_S) <p>If we assume (and keep in mind that we don't have to) that the system was equilibrated before we started observing its state sequence, then the initial state distribution is equivalent to the equilibrium distribution. All this means that we don't necessarily have to specify the initial distribution explicitly.</p> <p>(2) Drawing the next state by indexing into the transition matrix p_T, and drawing a new state based on the Multinomial distribution:</p> s_{t+1} \\sim Multinomial(1, p_{T_i}) <p>where i is the index of the state.</p> <p>I previously wrote about what probability distributions are, leveraging the SciPy probability distributions library. We're going to use that extensively here, as opposed to NumPy's <code>random</code> module, so that we can practice getting familiar with probability distributions as objects. In Python code:</p> <pre><code>from scipy.stats import multinomial\nfrom typing import List\n\ndef markov_sequence(p_init: np.array, p_transition: np.array, sequence_length: int) -&gt; List[int]:\n\"\"\"\n    Generate a Markov sequence based on p_init and p_transition.\n    \"\"\"\n    if p_init is None:\n        p_init = equilibrium_distribution(p_transition)\n    initial_state = list(multinomial.rvs(1, p_init)).index(1)\n\n    states = [initial_state]\n    for _ in range(sequence_length - 1):\n        p_tr = p_transition[states[-1]]\n        new_state = list(multinomial.rvs(1, p_tr)).index(1)\n        states.append(new_state)\n    return states\n</code></pre> <p>With this function in hand, let's generate a sequence of length 1000.</p> <pre><code>import seaborn as sns\n\nstates = markov_sequence(p_init, p_transition, sequence_length=1000)\nfig, ax = plt.subplots(figsize=(12, 4))\nplt.plot(states)\nplt.xlabel(\"time step\")\nplt.ylabel(\"state\")\nplt.yticks([0, 1, 2])\nsns.despine()\n</code></pre> <p></p> <p>As is pretty evident from the transition probabilities, once this Markov chain enters a state, it tends to maintain its current state rather than transitioning between states.</p> <p>If you've opened up this notebook in Binder or locally, feel free to modify the transition probabilities and initial state probabilities above to see how the Markov sequence changes.</p> <p>If a \"Markov sequence\" feels abstract at this point, one example to help you anchor your understanding would be human motion. The three states can be \"stationary\", \"walking\", and \"running\". We transition between the three states with some probability throughout the day, moving from \"stationary\" (sitting at my desk) to \"walking\" (to get water) to \"stationary\" (because I'm pouring water), to \"walking\" (out the door) to finally \"running\" (for exercise).</p>"},{"location":"machine-learning/markov-models/#emissions-when-markov-chains-not-only-produce-states-but-also-observable-data","title":"Emissions: When Markov chains not only produce \"states\", but also observable data","text":"<p>So as you've seen above, a Markov chain can produce \"states\". If we are given direct access to the \"states\", then a problem that we may have is inferring the transition probabilities given the states.</p> <p>A more common scenario, however, is that the states are latent, i.e. we cannot directly observe them. Instead, the latent states generate data that are given by some distribution conditioned on the state. We call these Hidden Markov Models.</p> <p>That all sounds abstract, so let's try to make it more concrete.</p>"},{"location":"machine-learning/markov-models/#gaussian-emissions-when-markov-chains-emit-gaussian-distributed-data","title":"Gaussian Emissions: When Markov chains emit Gaussian-distributed data.","text":"<p>With a three state model, we might say that the emissions are Gaussian distributed, but the location (\\mu) and scale (\\sigma) vary based on which state we are in. In the simplest case:</p> <ol> <li>State 1 gives us data y_1 \\sim N(\\mu=1, \\sigma=0.2)</li> <li>State 2 gives us data y_2 \\sim N(\\mu=0, \\sigma=0.5)</li> <li>State 3 gives us data y_3 \\sim N(\\mu=-1, \\sigma=0.1)</li> </ol> <p>In terms of a graphical model, it would look something like this:</p> <p></p> <p>Turns out, we can model this in Python code too!</p> <pre><code>from scipy.stats import norm\n\ndef gaussian_emissions(states: List[int], mus: List[float], sigmas: List[float]) -&gt; List[float]:\n    emissions = []\n    for state in states:\n        loc = mus[state]\n        scale = sigmas[state]\n        e = norm.rvs(loc=loc, scale=scale)\n        emissions.append(e)\n    return emissions\n</code></pre> <p>Let's see what the emissions look like.</p> <pre><code>gaussian_ems = gaussian_emissions(states, mus=[1, 0, -1], sigmas=[0.2, 0.5, 0.1])\n\ndef plot_emissions(states, emissions):\n    fig, axes = plt.subplots(figsize=(16, 8), nrows=2, ncols=1, sharex=True)\n\n    axes[0].plot(states)\n    axes[0].set_title(\"States\")\n    axes[1].plot(emissions)\n    axes[1].set_title(\"Emissions\")\n    sns.despine();\n\nplot_emissions(states, gaussian_ems)\n</code></pre> <p></p>"},{"location":"machine-learning/markov-models/#emission-distributions-can-be-any-valid-distribution","title":"Emission Distributions can be any valid distribution!","text":"<p>Nobody said we have to use Gaussian distributions for emissions; we can, in fact, have a ton of fun and start simulating data using other distributions!</p> <p>Let's try Poisson emissions. Here, then, the poisson rate \\lambda is given one per state. In our example below:</p> <ol> <li>State 1 gives us data y_1 \\sim Pois(\\lambda=1)</li> <li>State 2 gives us data y_2 \\sim Pois(\\lambda=10)</li> <li>State 3 gives us data y_3 \\sim Pois(\\lambda=50)</li> </ol> <pre><code>from scipy.stats import poisson\ndef poisson_emissions(states: List[int], lam: List[float]) -&gt; List[int]:\n    emissions = []\n    for state in states:\n        rate = lam[state]\n        e = poisson.rvs(rate)\n        emissions.append(e)\n    return emissions\n</code></pre> <p>Once again, let's observe the emissions:</p> <pre><code>poisson_ems = poisson_emissions(states, lam=[1, 10, 50])\nplot_emissions(states, poisson_ems)\n</code></pre> <p></p> <p>Hope the point is made: Take your favourite distribution and use it as the emission distribution, as long as it can serve as a useful model for the data that you observe!</p>"},{"location":"machine-learning/markov-models/#autoregressive-emissions","title":"Autoregressive Emissions","text":"<p>Autoregressive emissions make things even more interesting and flexible! They show up, for example, when we're trying to model \"motion states\" of people or animals: that's because people and animals don't abruptly change from one state to another, but gradually transition in.</p> <p>The \"autoregressive\" component thus helps us model that the emission value does not only depend on the current state, but also on previous state(s), which is what motion data, for example, might look like.</p> <p>How, though, can we enforce this dependency structure? Well, as implied by the term \"structure\", it means we have some set of equations that relate the parameters of the emission distribution to the value of the previous emission.</p> <p>In terms of a generic graphical model, it is represented as follows:</p> <p></p>"},{"location":"machine-learning/markov-models/#heteroskedastic-autoregressive-emissions","title":"Heteroskedastic Autoregressive Emissions","text":"<p>Here's a \"simple complex\" example, where the location \\mu_t of the emission distribution at time t depends on y_{t-1}, and the scale \\sigma depends only on the current state s_t.</p> <p>A place where this model might be useful is when we believe that noise is the only thing that depends on state, while the location follows a random walk. (Stock markets might be an applicable place for this.)</p> <p>In probabilistic notation:</p> y_t \\sim N(\\mu=k y_{t-1}, \\sigma=\\sigma_{s_t}) <p>Here, k is a multiplicative autoregressive coefficient that scales how the previous emission affects the location \\mu of the current emission. We might also assume that the initial location \\mu=0. Because the scale \\sigma varies with state, the emissions are called heteroskedastic, which means \"of non-constant variance\". In the example below:</p> <ol> <li>State 1 gives us \\sigma=0.5 (kind of small variance).</li> <li>State 2 gives us \\sigma=0.1 (smaller variance).</li> <li>State 3 gives us \\sigma=0.01 (very small varaince).</li> </ol> <p>In Python code, we would model it this way:</p> <pre><code>def ar_gaussian_heteroskedastic_emissions(states: List[int], k: float, sigmas: List[float]) -&gt; List[float]:\n    emissions = []\n    prev_loc = 0\n    for state in states:\n        e = norm.rvs(loc=k * prev_loc, scale=sigmas[state])\n        emissions.append(e)\n        prev_loc = e\n    return emissions\n</code></pre> <pre><code>ar_het_ems = ar_gaussian_heteroskedastic_emissions(states, k=1, sigmas=[0.5, 0.1, 0.01])\nplot_emissions(states, ar_het_ems)\n</code></pre> <p></p> <p>Keep in mind, here, that given the way that we've defined the autoregressive heteroskedastic Gaussian HMM, it is the variance around the heteroskedastic autoregressive emissions that gives us information about the state, not the location. (To see this, notice how every time the system enters into state 2, the chain stops bouncing around much.)</p> <p>Contrast that against vanilla Gaussian emissions that are non-autoregressive:</p> <pre><code>plot_emissions(states, gaussian_ems)\n</code></pre> <p></p>"},{"location":"machine-learning/markov-models/#how-does-the-autoregressive-coefficient-kk-affect-the-markov-chain-emissions","title":"How does the autoregressive coefficient k affect the Markov chain emissions?","text":"<p>As should be visible, the structure of autoregressiveness can really change how things look! What happens as k changes?</p> <pre><code>ar_het_ems = ar_gaussian_heteroskedastic_emissions(states, k=1, sigmas=[0.5, 0.1, 0.01])\nplot_emissions(states, ar_het_ems)\n</code></pre> <p></p> <pre><code>ar_het_ems = ar_gaussian_heteroskedastic_emissions(states, k=0, sigmas=[0.5, 0.1, 0.01])\nplot_emissions(states, ar_het_ems)\n</code></pre> <p></p> <p>Interesting stuff! As k \\rightarrow 0, we approach a Gaussian centered exactly on zero, where only the variance of the observations, rather than the collective average location of the observations, give us information about the state.</p>"},{"location":"machine-learning/markov-models/#homoskedastic-autoregressive-emissions","title":"Homoskedastic Autoregressive Emissions","text":"<p>What if we wanted instead the variance to remain the same, but desired instead that the emission location \\mu gives us information about the state while still being autoregressive? Well, we can bake that into the equation structure!</p> y_t \\sim N(\\mu=k y_{t-1} + \\mu_{s_t}, \\sigma=1) <p>In Python code:</p> <pre><code>def ar_gaussian_homoskedastic_emissions(states: List[int], k: float, mus: List[float]) -&gt; List[float]:\n    emissions = []\n    prev_loc = 0\n    for state in states:\n        e = norm.rvs(loc=k * prev_loc + mus[state], scale=1)\n        emissions.append(e)\n        prev_loc = e\n    return emissions\n</code></pre> <pre><code>ar_hom_ems = ar_gaussian_homoskedastic_emissions(states, k=1, mus=[-10, 0, 10])\nplot_emissions(states, ar_hom_ems)\n</code></pre> <p></p> <p>The variance is too small relative to the scale of the data, so it looks like smooth lines.</p> <p>If we change k, however, we get interesting effects.</p> <pre><code>ar_hom_ems = ar_gaussian_homoskedastic_emissions(states, k=0.8, mus=[-10, 0, 10])\nplot_emissions(states, ar_hom_ems)\n</code></pre> <p></p> <p>Notice how we get \"smoother\" transitions into each state. It's less jumpy. As mentioned earlier, this is extremely useful for modelling motion activity, for example, where people move into and out of states without having jumpy-switching. (We don't go from sitting to standing to walking by jumping frames, we ease into each.)</p>"},{"location":"machine-learning/markov-models/#non-autoregressive-homoskedastic-emissions","title":"Non-Autoregressive Homoskedastic Emissions","text":"<p>With non-autoregressive homoskedastic Gaussian emissions, the mean \\mu depends only on the hidden state at time t, and not on the previous hidden state or the previous emission value.</p> <p>In equations: y_t \\sim N(\\mu=f(x_t), \\sigma), where f(x_t) could be a simple mapping:</p> <ol> <li>If x_t = 1, \\mu = -10,</li> <li>If x_t = 2, \\mu = 0,</li> <li>If x_t = 3, \\mu = 10.</li> </ol> <p>What we can see here is that the mean gives us information about the state, but the scale doesn't.</p> <pre><code>def gaussian_homoskedastic_emissions(states: List[int], mus: List[float]) -&gt; List[float]:\n    emissions = []\n\n    prev_loc = 0\n    for state in states:\n        e = norm.rvs(loc=mus[state], scale=1)\n        emissions.append(e)\n        prev_loc = e\n    return emissions\n</code></pre> <pre><code>hom_ems = gaussian_homoskedastic_emissions(states, mus=[-10, 0, 10])\nplot_emissions(states, hom_ems)\n</code></pre> <p></p> <p>As you might intuit from looking at the equations, this is nothing more than a special case of the Heteroskedastic Gaussian Emissions example shown much earlier above.</p>"},{"location":"machine-learning/markov-models/#the-framework","title":"The Framework","text":"<p>There's the plain old Markov Model, in which we might generate a sequence of states S, which are generated from some initial distribution and transition matrix.</p>  p_S = \\begin{pmatrix} p_1 &amp; p_2 &amp; p_3 \\end{pmatrix}   p_T = \\begin{pmatrix}     p_{11} &amp; p_{12} &amp; p_{13}\\\\     p_{21} &amp; p_{22} &amp; p_{23}\\\\     p_{31} &amp; p_{32} &amp; p_{33}\\\\ \\end{pmatrix}  S = \\{s_t, s_{t+1}, ... s_{t+n}\\} <p>Graphically:</p> <p></p> <p>Then there's the \"Hidden\" Markov Model, in which we don't observe the states but rather the emissions generated from the states (according to some assumed distribution). Now, there's not only the initial distribution and transition matrix to worry about, but also the distribution of the emissions conditioned on the state. The general case is when we have some distribution e.g., the Gaussian or the Poisson or the Chi-Squared - whichever fits the likelihood of your data best. Usually, we would pick a parametric distribution both because of modelling convenience and because we think it would help us interpret our data.</p> y_t|s_t \\sim Dist(\\theta_{t}) <p>Where \\theta_t refers to the parameters for the generic distribution Dist that are indexed by the state s_t. (Think back to \"state 1 gives me N(-10, 1), while state 2 gives me N(0, 1)\", etc...) Your distributions probably generally come from the same family (e.g. \"Gaussians\"), or you can go super complicated and generate them from different distributions.</p> <p>Graphically:</p> <p></p> <p>Here are some special cases of the general framework. Firstly, the parameters of the emission distribution can be held constant (i.e. simple random walks). This is equivalent to when k=1 and neither \\mu nor \\sigma depend on current state. In this case, we get back the Gaussian random walk, where y_t \\sim N(k y_{t-1}, \\sigma)!</p> <p>Secondly, the distribution parameters can depend on the solely on the current state. In this case, you get back basic HMMs!</p> <p>If you make the variance of the likelihood distribution vary based on state, you get heteroskedastic HMMs; conversely, if you keep the variance constant, then you have homoskedastic HMMs.</p> <p>Moving on, there's the \"Autoregressive\" Hidden Markov Models, in which the emissions generated from the states have a dependence on the previous states' emissions (and hence, indirectly, on the previous state). Here, we have the ultimate amount of flexibility to model our processes.</p> y_t|s_t \\sim Dist(f(y_{t-1}, \\theta_t)) <p>Graphically:</p> <p></p> <p>To keep things simple in this essay, we've only considered the case of lag of 1 (which is where the t-1 comes from). However, arbitrary numbers of time lags are possible too!</p> <p>And, as usual, you can make them homoskedastic or heteroskedastic by simply controlling the variance parameter of the Dist distribution.</p> <p>Bonus point: your data don't necessarily have to be single dimensional; they can be multidimensional too! As long as you write the f(y_{t-1}, \\theta_t) in a fashion that handles y that are multidimensional, you're golden! Moreover, you can also write the function f to be any function you like. The function f doesn't have to be a linear function (like we did); it can instead be a neural network if you so choose, thus giving you a natural progression from Markov models to Recurrent Neural Networks. That, however, is out of scope for this essay.</p>"},{"location":"machine-learning/markov-models/#bayesian-inference-on-markov-models","title":"Bayesian Inference on Markov Models","text":"<p>Now that we've gone through the \"data generating process\" for Markov sequences with emissions, we can re-examine the entire class of models in a Bayesian light.</p> <p>If you've been observing the models that we've been \"forward-simulating\" all this while to generate data, you'll notice that there are a few key parameters that seemed like, \"well, if we changed them, then the data would change, right?\" If that's what you've been thinking, then bingo! You're on the right track.</p> <p>Moreover, you'll notice that I've couched everything in the language of probability distributions. The transition probabilities P(s_t | s_{t-1}) are given by a Multinomial distribution. The emissions are given by an arbitrary continuous (or discrete) distribution, depending on what you believe to be the likelihood distribution for the observed data. Given that we're working with probability distributions and data, you probably have been thinking about it already: we need a way to calculate the log-likelihoods of the data that we observe!</p> <p>(Why we use log-likelihoods instead of likelihoods is clarified here.)</p>"},{"location":"machine-learning/markov-models/#markov-chain-log-likelihood-calculation","title":"Markov Chain Log-Likelihood Calculation","text":"<p>Let's examine how we would calculate the log likelihood of state data given the parameters. This will lead us to the Markov chain log-likelihood.</p> <p>The likelihood of a given Markov chain states is:</p> <ol> <li>the probability of the first state given some assumed initial distribution,</li> <li>times the probability of the second state given the first state,</li> <li>times the probability of the third state given the second state,</li> <li>and so on... until the end.</li> </ol> <p>In math notation, given the states S = \\{s_1, s_2, s_3, ..., s_n\\}, this becomes:</p> L(S) = P(s_1) P(s_2|s_1) P(s_3|s_2) ... <p>More explicitly, P(s_1) is nothing more than the probability of observing that state s_1 given an assumed initial (or equilibrium) distribution:</p> <pre><code>s1 = [0, 1, 0]  # assume we start in state 1 of {0, 1, 2}\np_eq = equilibrium_distribution(p_transition)\nprob_s1 = p_eq[s1.index(1)]\nprob_s1\n</code></pre> <pre><code>0.27896995708154565\n</code></pre> <p>Then, P(s_2) is nothing more than the probability of observing that state s_2 given the transition matrix entry for state s_1.</p> <pre><code># assume we enter into state 2 of {0, 1, 2}\ns2 = [0, 0, 1]\ntransition_entry = p_transition[s1.index(1)]\nprob_s2 = transition_entry[s2.index(1)]\nprob_s2\n</code></pre> <pre><code>0.09\n</code></pre> <p>Their joint likelihood is given then by <code>prob_s1</code> times <code>prob_s2</code>.</p> <pre><code>prob_s1 * prob_s2\n</code></pre> <pre><code>0.025107296137339107\n</code></pre> <p>And because we operate in log space to avoid underflow, we do joint log-likelihoods instead:</p> <pre><code>np.log(prob_s1) + np.log(prob_s2)\n</code></pre> <pre><code>-3.6845967923219334\n</code></pre> <p>Let's generalize this in a math function.</p> <p>Since P(s_t|s_{t-1}) is a multinomial distribution, then if we are given the log-likelihood of \\{s_1, s_2, s_3, ..., s_n\\}, we can calculate the log-likelihood over \\{s_2,... s_n\\}, which is given by the sum of the log probabilities:</p> <pre><code>def state_logp(states, p_transition):\n    logp = 0\n\n    # states are 0, 1, 2, but we model them as [1, 0, 0], [0, 1, 0], [0, 0, 1]\n    states_oh = np.eye(len(p_transition))\n    for curr_state, next_state in zip(states[:-1], states[1:]):\n        p_tr = p_transition[curr_state]\n        logp += multinomial(n=1, p=p_tr).logpmf(states_oh[next_state])\n    return logp\n\nstate_logp(states, p_transition)\n</code></pre> <pre><code>-418.65677519562405\n</code></pre> <p>We will also write a vectorized version of <code>state_logp</code>.</p> <pre><code>def state_logp_vect(states, p_transition):\n    states_oh = np.eye(len(p_transition))\n    p_tr = p_transition[states[:-1]]\n    obs = states_oh[states[1:]]\n    return np.sum(multinomial(n=1, p=p_tr).logpmf(obs))\n\nstate_logp_vect(states, p_transition)\n</code></pre> <pre><code>-418.6567751956279\n</code></pre> <p>Now, there is a problem here: we also need the log likelihood of the first state.</p> <p>Remember that if we don't know what the initial distribution is supposed to be, one possible assumption we can make is that the Markov sequence began by drawing from the equilibrium distribution. Here is where equilibrium distribution calculation from before comes in handy!</p> <pre><code>def initial_logp(states, p_transition):\n    initial_state = states[0]\n    states_oh = np.eye(len(p_transition))\n    eq_p = equilibrium_distribution(p_transition)\n    return (\n        multinomial(n=1, p=eq_p)\n        .logpmf(states_oh[initial_state].squeeze())\n    )\n\ninitial_logp(states, p_transition)\n</code></pre> <pre><code>array(-1.16057901)\n</code></pre> <p>Taken together, we get the following Markov chain log-likelihood:</p> <pre><code>def markov_state_logp(states, p_transition):\n    return (\n        state_logp_vect(states, p_transition)\n        + initial_logp(states, p_transition)\n    )\n\nmarkov_state_logp(states, p_transition)\n</code></pre> <pre><code>-419.81735420804523\n</code></pre>"},{"location":"machine-learning/markov-models/#markov-chain-with-gaussian-emissions-log-likelihood-calculation","title":"Markov Chain with Gaussian Emissions Log-Likelihood Calculation","text":"<p>Now that we know how to calculate the log-likelihood for the Markov chain sequence of states, we can move on to the log-likelihood calculation for the emissions.</p> <p>Let's first assume that we have emissions that are non-autoregressive, and have a Gaussian likelihood. For the benefit of those who need it written out explicitly, here's the for-loop version:</p> <pre><code>def gaussian_logp(states, mus, sigmas, emissions):\n    logp = 0\n    for (emission, state) in zip(emissions, states):\n        logp += norm(mus[state], sigmas[state]).logpdf(emission)\n    return logp\n\ngaussian_logp(states, mus=[1, 0, -1], sigmas=[0.2, 0.5, 0.1], emissions=gaussian_ems)\n</code></pre> <pre><code>250.57996114495296\n</code></pre> <p>And we'll also make a vectorized version of it:</p> <pre><code>def gaussian_logp_vect(states, mus, sigmas, emissions):\n    mu = mus[states]\n    sigma = sigmas[states]\n    return np.sum(norm(mu, sigma).logpdf(emissions))\n\ngaussian_logp_vect(states, mus=np.array([1, 0, -1]), sigmas=np.array([0.2, 0.5, 0.1]), emissions=gaussian_ems)\n</code></pre> <pre><code>250.5799611449528\n</code></pre> <p>The joint log likelihood of the emissions and states are then given by their summation.</p> <pre><code>def gaussian_emission_hmm_logp(states, p_transition, mus, sigmas, emissions):\n    return markov_state_logp(states, p_transition) + gaussian_logp_vect(states, mus, sigmas, emissions)\n\ngaussian_emission_hmm_logp(states, p_transition, mus=np.array([1, 0, -1]), sigmas=np.array([0.2, 0.5, 0.1]), emissions=gaussian_ems)\n</code></pre> <pre><code>-169.23739306309244\n</code></pre> <p>If you're in a Binder or local Jupyter session, go ahead and tweak the values of <code>mus</code> and <code>sigmas</code>, and verify for yourself that the current values are the \"maximum likelihood\" values. After all, our Gaussian emission data were generated according to this exact set of parameters!</p>"},{"location":"machine-learning/markov-models/#markov-chain-with-autoregressive-gaussian-emissions-log-likelihood-calculation","title":"Markov Chain with Autoregressive Gaussian Emissions Log-Likelihood Calculation","text":"<p>I hope the pattern is starting to be clear here: since we have Gaussian emissions, we only have to calculate the parameters of the Gaussian to know what the logpdf would be.</p> <p>As an example, I will be using the Gaussian with:</p> <ul> <li>State-varying scale</li> <li>Mean that is dependent on the previously emitted value</li> </ul> <p>This is the AR-HMM with data generated from the <code>ar_gaussian_heteroskedastic_emissions</code> function.</p> <pre><code>def ar_gaussian_heteroskedastic_emissions_logp(states, k, sigmas, emissions):\n    logp = 0\n    initial_state = states[0]\n    initial_emission_logp = norm(0, sigmas[initial_state]).logpdf(emissions[0])\n    for previous_emission, current_emission, state in zip(emissions[:-1], emissions[1:], states[1:]):\n        loc = k * previous_emission\n        scale = sigmas[state]\n        logp += norm(loc, scale).logpdf(current_emission)\n    return logp\n\nar_gaussian_heteroskedastic_emissions_logp(states, k=1.0, sigmas=[0.5, 0.1, 0.01], emissions=ar_het_ems)\n</code></pre> <pre><code>-18605.714303907385\n</code></pre> <p>Now, we can write the full log likelihood of the entire AR-HMM:</p> <pre><code>def ar_gausian_heteroskedastic_hmm_logp(states, p_transition, k, sigmas, emissions):\n    return (\n        markov_state_logp(states, p_transition)\n        + ar_gaussian_heteroskedastic_emissions_logp(states, k, sigmas, emissions)\n    )\n\n\nar_gausian_heteroskedastic_hmm_logp(states, p_transition, k=1.0, sigmas=[0.5, 0.1, 0.01], emissions=ar_het_ems)\n</code></pre> <pre><code>-19025.53165811543\n</code></pre> <p>For those of you who are familiar with Bayesian inference, as soon as we have a joint log likelihood that we can calculate between our model priors and data, using the simple Bayes' rule equation, we can obtain posterior distributions easily through an MCMC sampler.</p> <p>If this looks all foreign to you, then check out my other essay for a first look (or a refresher)!</p>"},{"location":"machine-learning/markov-models/#hmm-distributions-in-pymc3","title":"HMM Distributions in PyMC3","text":"<p>While PyMC4 is in development, PyMC3 remains one of the leading probabilistic programming languages that can be used for Bayesian inference. PyMC3 doesn't have the HMM distribution defined in the library, but thanks to GitHub user @hstrey posting a Jupyter notebook with HMMs defined in there, many PyMC3 users have had a great baseline distribution to study pedagogically and use in their applications, myself included.</p> <p>Side note: I used @hstrey's implementation before setting out to write this essay. Thanks!</p> <p>The key thing to notice in this section is how the <code>logp</code> functions are defined. They will match the log probability functions that we have defined above, except written in Theano.</p>"},{"location":"machine-learning/markov-models/#hmm-states-distribution","title":"HMM States Distribution","text":"<p>Let's first look at the HMM States distribution, which will give us a way to calculate the log probability of the states.</p> <pre><code>import pymc3 as pm\nimport theano.tensor as tt\nimport theano.tensor.slinalg as sla  # theano-wrapped scipy linear algebra\nimport theano.tensor.nlinalg as nla  # theano-wrapped numpy linear algebra\nimport theano\n\ntheano.config.gcc.cxxflags = \"-Wno-c++11-narrowing\"\n\nclass HMMStates(pm.Categorical):\n    def __init__(self, p_transition, p_equilibrium, n_states, *args, **kwargs):\n\"\"\"You can ignore this section for the time being.\"\"\"\n        super(pm.Categorical, self).__init__(*args, **kwargs)\n        self.p_transition = p_transition\n        self.p_equilibrium = p_equilibrium\n        # This is needed\n        self.k = n_states\n        # This is only needed because discrete distributions must define a mode.\n        self.mode = tt.cast(0,dtype='int64')\n\n    def logp(self, x):\n\"\"\"Focus your attention here!\"\"\"\n        p_eq = self.p_equilibrium\n        # Broadcast out the transition probabilities,\n        # so that we can broadcast the calculation\n        # of log-likelihoods\n        p_tr = self.p_transition[x[:-1]]\n\n        # the logp of the initial state evaluated against the equilibrium probabilities\n        initial_state_logp = pm.Categorical.dist(p_eq).logp(x[0])\n\n        # the logp of the rest of the states.\n        x_i = x[1:]\n        ou_like = pm.Categorical.dist(p_tr).logp(x_i)\n        transition_logp = tt.sum(ou_like)\n        return initial_state_logp + transition_logp\n</code></pre> <p>Above, the categorical distribution is used for convenience - it can handle integers, while multinomial requires the one-hot transformation. The categorical distribution is the generalization of the multinomial distribution, but unfortunately, it isn't implemented in the SciPy stats library, which is why we used the multinomial earlier on.</p> <p>Now, we stated earlier on that the transition matrix can be treated as a parameter to tweak, or else a random variable for which we want to infer its parameters. This means there is a natural fit for placing priors on them! Dirichlet distributions are great priors for probability vectors, as they are the generalization of Beta distributions.</p> <pre><code>def solve_equilibrium(n_states, p_transition):\n    A = tt.dmatrix('A')\n    A = tt.eye(n_states) - p_transition + tt.ones(shape=(n_states, n_states))\n    p_equilibrium = pm.Deterministic(\"p_equilibrium\", sla.solve(A.T, tt.ones(shape=(n_states))))\n    return p_equilibrium\n</code></pre> <pre><code>import warnings\nwarnings.simplefilter(action=\"ignore\", category=FutureWarning)\n\nn_states = 3\nwith pm.Model() as model:\n    p_transition = pm.Dirichlet(\n        \"p_transition\",\n        a=tt.ones((n_states, n_states)) * 4,  # weakly informative prior\n        shape=(n_states, n_states))\n\n    # Solve for the equilibrium state\n    p_equilibrium = solve_equilibrium(n_states, p_transition)\n\n    obs_states = HMMStates(\n        \"states\",\n        p_transition=p_transition,\n        p_equilibrium=p_equilibrium,\n        n_states=n_states,\n        observed=np.array(states).astype(\"float\")\n    )\n</code></pre> <p>Now let's fit the model!</p> <pre><code>with model:\n    trace = pm.sample(2000)\n</code></pre> <pre><code>Auto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [p_transition]\nSampling 4 chains, 0 divergences:   0%|          | 0/10000 [00:00&lt;?, ?draws/s]/home/ericmjl/anaconda/envs/bayesian-analysis-recipes/lib/python3.8/site-packages/theano/tensor/slinalg.py:255: LinAlgWarning: Ill-conditioned matrix (rcond=5.89311e-08): result may not be accurate.\nrval = scipy.linalg.solve(A, b)\n/home/ericmjl/anaconda/envs/bayesian-analysis-recipes/lib/python3.8/site-packages/theano/tensor/slinalg.py:255: LinAlgWarning: Ill-conditioned matrix (rcond=5.89311e-08): result may not be accurate.\nrval = scipy.linalg.solve(A, b)\nSampling 4 chains, 0 divergences: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10000/10000 [00:08&lt;00:00, 1192.11draws/s]\n</code></pre> <pre><code>import arviz as az\n\naz.plot_forest(trace, var_names=[\"p_transition\"]);\n</code></pre> <p></p> <p>It looks like we were able to recover the original transitions!</p>"},{"location":"machine-learning/markov-models/#hmm-with-gaussian-emissions","title":"HMM with Gaussian Emissions","text":"<p>Let's try out now an HMM model with Gaussian emissions.</p> <pre><code>class HMMGaussianEmissions(pm.Continuous):\n    def __init__(self, states, mu, sigma, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.states = states\n        # self.rate = rate\n        self.mu = mu\n        self.sigma = sigma\n\n    def logp(self, x):\n\"\"\"\n        x: observations\n        \"\"\"\n        states = self.states\n        # rate = self.rate[states]  # broadcast the rate across the states.\n        mu = self.mu[states]\n        sigma = self.sigma[states]\n        return tt.sum(pm.Normal.dist(mu=mu, sigma=sigma).logp(x))\n</code></pre> <pre><code>n_states = 3\nwith pm.Model() as model:\n    # Priors for transition matrix\n    p_transition = pm.Dirichlet(\"p_transition\", a=tt.ones((n_states, n_states)), shape=(n_states, n_states))\n\n    # Solve for the equilibrium state\n    p_equilibrium = solve_equilibrium(n_states, p_transition)\n\n    # HMM state\n    hmm_states = HMMStates(\n        \"hmm_states\",\n        p_transition=p_transition,\n        p_equilibrium=p_equilibrium,\n        n_states=n_states,\n        shape=(len(gaussian_ems),)\n    )\n\n    # Prior for mu and sigma\n    mu = pm.Normal(\"mu\", mu=0, sigma=1, shape=(n_states,))\n    sigma = pm.Exponential(\"sigma\", lam=2, shape=(n_states,))\n\n    # Observed emission likelihood\n    obs = HMMGaussianEmissions(\n        \"emission\",\n        states=hmm_states,\n        mu=mu,\n        sigma=sigma,\n        observed=gaussian_ems\n    )\n</code></pre> <pre><code>with model:\n    trace = pm.sample(2000)\n</code></pre> <pre><code>Multiprocess sampling (4 chains in 4 jobs)\nCompoundStep\n&gt;NUTS: [sigma, mu, p_transition]\n&gt;CategoricalGibbsMetropolis: [hmm_states]\nSampling 4 chains, 0 divergences: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10000/10000 [11:59&lt;00:00, 13.90draws/s]\nThe rhat statistic is larger than 1.4 for some parameters. The sampler did not converge.\nThe estimated number of effective samples is smaller than 200 for some parameters.\n</code></pre> <pre><code>az.plot_trace(trace, var_names=[\"mu\"]);\n</code></pre> <p></p> <pre><code>az.plot_trace(trace, var_names=[\"sigma\"]);\n</code></pre> <p></p> <pre><code>az.plot_forest(trace, var_names=[\"sigma\"]);\n</code></pre> <p></p> <p>We are able to recover the parameters, but there is significant intra-chain homogeneity. That is fine, though one way to get around this is to explicitly instantiate prior distributions for each of the parameters instead.</p>"},{"location":"machine-learning/markov-models/#autoregressive-hmms-with-gaussian-emissions","title":"Autoregressive HMMs with Gaussian Emissions","text":"<p>Let's now add in the autoregressive component to it. The data we will use is the <code>ar_het_ems</code> data, which were generated by using a heteroskedastic assumption, with Gaussian emissions whose mean depends on the previous value, while variance depends on state.</p> <p>As a reminder of what the data look like:</p> <pre><code>ar_het_ems = ar_gaussian_heteroskedastic_emissions(states, k=0.6, sigmas=[0.5, 0.1, 0.01])\nplot_emissions(states, ar_het_ems)\n</code></pre> <p></p> <p>Let's now define the AR-HMM.</p> <pre><code>class ARHMMGaussianEmissions(pm.Continuous):\n    def __init__(self, states, k, sigma, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.states = states\n        self.sigma = sigma  # variance\n        self.k = k  # autoregressive coefficient.\n\n    def logp(self, x):\n\"\"\"\n        x: observations\n        \"\"\"\n        states = self.states\n        sigma = self.sigma[states]\n        k = self.k\n\n        ar_mean = k * x[:-1]\n        ar_like = tt.sum(pm.Normal.dist(mu=ar_mean, sigma=sigma[1:]).logp(x[1:]))\n\n        boundary_like = pm.Normal.dist(mu=0, sigma=sigma[0]).logp(x[0])\n        return ar_like + boundary_like\n</code></pre> <pre><code>n_states = 3\nwith pm.Model() as model:\n    # Priors for transition matrix\n    p_transition = pm.Dirichlet(\"p_transition\", a=tt.ones((n_states, n_states)), shape=(n_states, n_states))\n\n    # Solve for the equilibrium state\n    p_equilibrium = solve_equilibrium(n_states, p_transition)\n\n    # HMM state\n    hmm_states = HMMStates(\n        \"hmm_states\",\n        p_transition=p_transition,\n        p_equilibrium=p_equilibrium,\n        n_states=n_states,\n        shape=(len(ar_het_ems),)\n    )\n\n    # Prior for sigma and k\n    sigma = pm.Exponential(\"sigma\", lam=2, shape=(n_states,))\n    k = pm.Beta(\"k\", alpha=2, beta=2) # a not-so-weak prior for k\n\n    # Observed emission likelihood\n    obs = ARHMMGaussianEmissions(\n        \"emission\",\n        states=hmm_states,\n        sigma=sigma,\n        k=k,\n        observed=ar_het_ems\n    )\n</code></pre> <pre><code>with model:\n    trace = pm.sample(2000)\n</code></pre> <pre><code>Multiprocess sampling (4 chains in 4 jobs)\nCompoundStep\n&gt;NUTS: [k, sigma, p_transition]\n&gt;CategoricalGibbsMetropolis: [hmm_states]\nSampling 4 chains, 6 divergences: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10000/10000 [12:34&lt;00:00, 13.26draws/s]\nThe acceptance probability does not match the target. It is 0.9096431867898114, but should be close to 0.8. Try to increase the number of tuning steps.\nThere were 6 divergences after tuning. Increase `target_accept` or reparameterize.\nThe rhat statistic is larger than 1.4 for some parameters. The sampler did not converge.\nThe estimated number of effective samples is smaller than 200 for some parameters.\n</code></pre> <p>Let's now take a look at the key parameters we might be interested in estimating:</p> <ul> <li>k: the autoregressive coefficient, or how much previous emissions influence current emissions.</li> <li>\\sigma: the variance that belongs to each state.</li> </ul> <pre><code>az.plot_forest(trace, var_names=[\"k\"]);\n</code></pre> <p></p> <pre><code>az.plot_trace(trace, var_names=[\"k\"]);\n</code></pre> <p></p> <p>It looks like we were able to obtain the value of k correctly!</p> <pre><code>az.plot_trace(trace, var_names=[\"sigma\"]);\n</code></pre> <p></p> <pre><code>az.plot_forest(trace, var_names=[\"sigma\"]);\n</code></pre> <p></p> <p>It also looks like we were able to obtain the correct sigma values too, except that the chains are mixed up. We would do well to take care when calculating means for each parameter on the basis of chains.</p> <p>How about the chain states? Did we get them right?</p> <pre><code>fig, ax = plt.subplots(figsize=(12, 4))\nplt.plot(np.round(trace[\"hmm_states\"].mean(axis=0)), label=\"true\")\nplt.plot(2 - np.array(states), label=\"inferred\")\nplt.legend();\n</code></pre> <p></p> <p>I had to flip the states because they were backwards relative to the original.</p> <p>Qualitatively, not bad! If we wanted to be a bit more rigorous, we would quantify the accuracy of state identification.</p> <p>If the transition probabilities were a bit more extreme, we might have an easier time with the identifiability of the states. As it stands, because the variance is the only thing that changes, and because the variance of two of the three states are quite similar (one is 0.1 and the other is 0.5), distinguishing between these two states may be more difficult.</p>"},{"location":"machine-learning/markov-models/#concluding-notes","title":"Concluding Notes","text":""},{"location":"machine-learning/markov-models/#nothing-in-statistics-makes-sense","title":"Nothing in statistics makes sense...","text":"<p>...unless in light of a \"data generating model\".</p> <p>I initially struggled with the math behind HMMs and its variants, because I had never taken the time to think through the \"data generating process\" carefully. Once we have the data generating process, and in particular, its structure, it becomes trivial to map the structure of the model to the equations that are needed to model it. (I think this is why physicists are such good Bayesians: they are well-trained at thinking about mechanistic, data generating models.)</p> <p>For example, with autoregressive HMMs, until I sat down and thought through the data generating process step-by-step, nothing made sense. Once I wrote out how the mean of the previous observation influenced the mean of the current observation, then things made a ton of sense.</p> <p>In fact, now that I look back on my learning journey in Bayesian statistics, if we can define a likelihood function for our data, we can trivially work backwards and design a data generating process.</p>"},{"location":"machine-learning/markov-models/#model-structure-is-important","title":"Model structure is important","text":"<p>While writing out the PyMC3 implementations and conditioning them on data, I remember times when I mismatched the model to the data, thus generating posterior samples that exhibited pathologies: divergences and more. This is a reminder that getting the structure of the model is very important.</p>"},{"location":"machine-learning/markov-models/#keep-learning","title":"Keep learning","text":"<p>I hope this essay was useful for your learning journey as well. If you enjoyed it, please take a moment to star the repository!</p>"},{"location":"machine-learning/markov-models/#acknowledgements","title":"Acknowledgements","text":"<p>I would like to acknowledge the following colleagues and friends who have helped review the notebook.</p> <ul> <li>My colleagues, Zachary Barry and Balaji Goparaju, both of whom pointed out unclear phrasings in my prose and did some code review.</li> <li>Fellow PyMC developers, Colin Carroll (from whom I never cease to learn things), Alex Andorra (who also did code review), Junpeng Lao, Ravin Kumar, and Osvaldo Martin (also for their comments),</li> <li>Professor Allen Downey (of the Olin College of Engineering) who provided important pedagogical comments throughout the notebook.</li> </ul>"},{"location":"machine-learning/message-passing/","title":"Computational Representations of Message Passing","text":"<p>Abstract: Message passing on graphs, also known as graph convolutions, have become a popular research topic. In this piece, I aim to provide a short technical primer on ways to implement message passing on graphs. The goal is to provide clear pedagogy on what message passing means mathematically, and hopefully point towards cleaner computational implementations of the key algorithmic pieces.</p> <p>Assumed knowledge: We assume our reader has familiarity with elementary graph concepts. More specifically, the terms \u201cgraph\u201d, \u201cnodes\u201d, and \u201cedges\u201d should be familiar terms. Code examples in this technical piece will be written using the Python programming language, specifically using Python 3.7, NumPy 1.17 (in JAX), and NetworkX 2.2.</p>"},{"location":"machine-learning/message-passing/#introduction-to-message-passing","title":"Introduction to Message Passing","text":""},{"location":"machine-learning/message-passing/#functions-on-nodes","title":"Functions on Nodes","text":"<p>Message passing starts with a \u201cfunction defined over nodes\u201d, which we will denote here as f(v) (for \u201cfunction of node/vertex v\u201d). What is this, one might ask? In short, this is nothing more than a numeric value of some kind attached to every node in a graph. This value could be scalar, vector, matrix, or tensor.</p> <p>The semantic meaning of that value is typically defined by the application domain that the graph is being used in. As a concrete example, in molecules, a \u201cfunction\u201d defined over the molecular graph could be the scalar-valued proton number. Carbon would be represented by the function f(v) = 6. Alternatively, it could be a vector of values encompassing both the atomic mass and the number of valence electrons. In this case, carbon would be represented by the function f(v) = (6, 4).</p> <p>Visually, one might represent it as follows:</p> <p></p>"},{"location":"machine-learning/message-passing/#message-passing","title":"Message Passing","text":"<p>What then is message passing, or, as the deep learning community has adopted, \u201cgraph convolution\u201d? At its core, message passing is nothing more than a generic mathematical operation defined between a node\u2019s function value and its neighbors function value.</p> <p>As an example, one may define a message passing operation to be the summation the function evaluated at a node with the function evaluated on its neighbor\u2019s nodes. Here is a simplistic example, shown using a scalar on water:</p> <p></p> <p>Summation is not the only message passing operation that can be defined. In principle, given any node (or vertex) v and its neighbors N(v) values, we may write down a generic function f(v, N(v)) that defines how the function value on each node is to be shared with its neighbors.</p>"},{"location":"machine-learning/message-passing/#computational-implementations-of-message-passing","title":"Computational Implementations of Message Passing","text":"<p>For simplicity, let us stay with the particular case where the message passing operation is defined as the summation of one\u2019s neighbors values with one\u2019s values.</p>"},{"location":"machine-learning/message-passing/#object-oriented-implementation","title":"Object-Oriented Implementation","text":"<p>With this definition in place, we may then define a message passing operation in Python as follows:</p> <pre><code>def message_passing(G):\n\"\"\"Object-oriented message passing operation.\"\"\"\n\n    G_new = G.copy()\n\n    for node, data in G.nodes(data=True):\n        new_value = data[\"value\"]  # assuming the value is stored under this key\n        neighbors = G.neighbors(node)\n        for neighbor in neighbors:\n            new_value += G.nodes[neighbor][\"value\"]\n        G_new.node[node][\"value\"] = new_value\n    return G\n</code></pre> <p>Thinking about computational considerations, we would naturally consider this implementation to be slow, because it involves a for-loop over Python objects. If we had multiple graphs over which we wanted message passing to be performed, the type-checking overhead in Python will naturally accumulate, and may even dominate.</p>"},{"location":"machine-learning/message-passing/#linear-algebra-implementation","title":"Linear Algebra Implementation","text":"<p>How might we speed things up? As it turns out, linear algebra may be useful.</p> <p>We know that every graph may be represented as an adjacency matrix <code>A</code>, whose shape is <code>(n_nodes, n_nodes)</code>. As long as we maintain proper node ordering, we may also define a compatibly-shaped matrix <code>F</code> for node function values, whose shape is <code>(n_nodes, n_features)</code>.</p> <p>Taking advantage of this, in order define the \u201cself plus neighbors\u201d message passing operation in terms of linear algebra operations, we may then modify <code>A</code> by adding to it a diagonal matrix of ones. (In graph terminology, this is equivalent to adding a self-loop to the adjacency matrix.)</p> <p>Then, message passing, as defined above, is trivially the dot product of <code>A</code> and <code>F</code>:</p> <pre><code>def message_passing(A, F):\n\"\"\"\n    Message passing done by linear algebra.\n\n    :param A: Adjacency-like matrix, whose shape is (n_nodes, n_nodes).\n    :param F: Feature matrix, whose shape is (n_nodes, n_features).\n    \"\"\"\n\n    return np.dot(A, F)\n</code></pre> <p>In principle, variants on the adjacency matrix are possible. The only hard requirement for the matrix <code>A</code> is that it has the shape <code>(n_nodes, n_nodes)</code>.</p>"},{"location":"machine-learning/message-passing/#adjacency-variant-1-n-degree-adjacency-matrix","title":"Adjacency Variant 1: N-degree adjacency matrix","text":"<p>The adjacency matrix represents connectivity by degree 1. If we take the second matrix power of the adjacency matrix, we get back the connectivity of nodes at two degrees of separation away. More generically:</p> <pre><code>def n_degree_adjacency(A, n: int):\n\"\"\"\n    Return the n-degree of separation adjacency matrix.\n\n    :param A: Adjacency matrix, of shape (n_nodes, n_nodes)\n    :param n: Number of degrees of separation.\n    \"\"\"\n    return np.linalg.matrix_power(A, n)\n</code></pre> <p>Performing message passing using the N-degree adjacency matrix effectively describes sharing of information between nodes that are N-degrees of separation apart, skipping intermediate neighbors.</p>"},{"location":"machine-learning/message-passing/#adjacency-variant-2-graph-laplacian-matrix","title":"Adjacency Variant 2: Graph laplacian matrix","text":"<p>The graph laplacian matrix is defined as the diagonal degree matrix <code>D</code> (where the diagonal entries are the degree of each node) minus the adjacency matrix <code>A</code>: <code>L = D - A</code>.</p> <p>This matrix is the discrete analog to the Laplacian operator, and can give us information about the discrete gradient between a node and its neighbors.</p>"},{"location":"machine-learning/message-passing/#message-passing-on-multiple-graphs","title":"Message Passing on Multiple Graphs","text":"<p>Thus far, we have seen an efficient implementation of message passing on a single graph using linear algebra.</p> <p>How would one perform message passing on multiple graphs, though?</p> <p>This is a question that has applications in graph neural networks (especially in cheminformatics). For the learning task where one has a batch of graphs, and the supervised learning task is to predict a scalar (or vector) value per graph, knowing how to efficiently message pass over multiple graphs is crucial to developing a performant graph neural network model.</p> <p>The challenge here, though, is that graphs generally are of variable size, hence it is not immediately obvious how to \u201ctensorify\u201d the operations.</p> <p>Let us look at a few alternatives, starting with the most obvious (but also most inefficient), building towards more efficient solutions.</p>"},{"location":"machine-learning/message-passing/#implementation-1-for-loops-over-pairs-of-adjacency-and-feature-matrices","title":"Implementation 1: For-loops over pairs of adjacency and feature matrices","text":"<p>If we multiple graphs, they may be represented as a list of feature matrices and a list of adjacency matrices. The message passing operation, then, may be defined by writing a for-loop over pairs of these matrices.</p> <pre><code>def message_passing(As, Fs):\n    outputs = []\n    for A, F in zip(As, Fs):\n        outputs.append(np.dot(A, F))\n    return outputs\n</code></pre> <p>Because of the for-loop, the obvious downside here is the overhead induced by running a for-loop over pairs of As and Fs.</p>"},{"location":"machine-learning/message-passing/#implementation-2-sparse-matrices","title":"Implementation 2: Sparse Matrices","text":"<p>Sparse matrices are an attractive alternative. Instead of treating graphs as independent samples, we may treat them as a single large graph on which we perform message passing. If we order the nodes in our adjacency matrix and feature matrix correctly, we will end up with a block diagonal adjacency matrix, and vertically stacked feature matrices.</p> <p></p> <p>If we prepare the multiple graphs as a large disconnected graph, then we will have a dense feature matrix of shape <code>(sum(n_nodes), n_feats)</code>, and a sparse adjacency matrix of shape <code>(sum(n_nodes), sum(n_nodes))</code>. Message passing then becomes a sparse-dense dot product:</p> <pre><code>def message_passing(A, F):\n    return sparse.dot(A, F)\n</code></pre> <p>The upside here is that message passing has been returned back to its natural form (a dot product). The downsides here are that the data must be prepared as a single large graph, hence we effectively lose what one would call the \u201csample\u201d (or \u201cbatch\u201d) dimension. Additionally, the most widely used deep learning libraries do not support automatic differentiation on sparse-dense or dense-sparse dot products, hence limiting the use of this implementation in deep learning.</p>"},{"location":"machine-learning/message-passing/#implementation-3-size-batched-matrix-multiplication","title":"Implementation 3: Size-batched matrix multiplication","text":"<p>An alternative way to conceptualize message passing is to think of graphs of the same size as belonging to a \u201csize batch\u201d. We may then vertically stack the feature and adjacency matrices of graphs of the same size together, and perform a batched matrix multiplication, ensuring that we preserve the sample/batch dimension in the final result.</p> <p></p> <p>In terms of Python code, this requires special preparation of the graphs.</p> <pre><code>from collections import defaultdict\nfrom jax.lax import batch_matmul\n\ndef feature_matrix(G):\n    # ...\n    return F\n\ndef prep_data(Gs: list):\n    adjacency_matrices = defaultdict(list)\n    feature_matrices = defaultdict(list)\n    for G in Gs:\n        size = len(G)\n        F = feature_matrix(G)\n        A = nx.adjacency_matrix(G) + np.ones(size)\n        adjacency_matrices[size].append(A)\n        feature_matrices[size].append(A)\n\n    for size, As in adjacency_matrices.items():\n        adjacency_matrices[size] = np.stack(As)\n    for size, Fs in feature_matrices.items():\n        feature_matrices[size] = np.stack(Fs)\n    return adjacency_matrices, feature_matrices\n\ndef message_passing(As, Fs):\n    result = dict()\n    for size in As.keys():\n        F = Fs[size]\n        A = As[size]\n\n        result[size] = batch_matmul(A, F)\n    return result\n</code></pre> <p>In this implementation, we use <code>jax.lax.batch_matmul</code>, which inherently assumes that the first dimension is the sample/batch dimension, and that the matrix multiplication happens on the subsequent dimensions.</p> <p>An advantage here is that the number of loop overhead calls in Python is reduced to the number of unique graph sizes that are present in the graph. The disadvantage, though, is that we have a dictionary data structure that we have to deal with, which makes data handling in Python less natural when dealing with linear algebra libraries.</p>"},{"location":"machine-learning/message-passing/#implementation-4-batched-padded-matrix-multiplication","title":"Implementation 4: Batched padded matrix multiplication","text":"<p>In this implementation, we prepare the data in a different way. Firstly, we must know the size of the largest graph ahead-of-time.</p> <pre><code>size = ...  # largest graph size\n</code></pre> <p>We then pad every graph\u2019s feature matrix with zeros along the node axis until the node axis is as long as the largest graph size.</p> <pre><code>def prep_feats(F, size):\n    # F is of shape (n_nodes, n_feats)\n    return np.pad(\n        F,\n        [\n            (0, size - F.shape[0]),\n            (0, 0)\n        ],\n    )\n</code></pre> <p>We do the same with every adjacency matrix.</p> <pre><code>def prep_adjs(A, size):\n    # A is of shape (n_nodes, n_nodes)\n    return np.pad(\n        A,\n        [\n            (0, size-A.shape[0]),\n            (0, size-A.shape[0]),\n        ],\n    )\n</code></pre> <p>Finally, we simply stack them into the data matrix:</p> <pre><code>As = np.stack([prep_adjs(A, size) for A in As]\nFs = np.stack([prep_feats(F, size) for F in Fs]\n</code></pre> <p>Now, the shapes of our matrices are as follows:</p> <ul> <li><code>F</code> takes on the shape <code>(n_graphs, n_nodes, n_feats)</code></li> <li><code>A</code> takes on the shape <code>(n_graphs, n_nodes, n_nodes)</code></li> </ul> <p>If we desire to be semantically consistent with our shapes, then we might, by convention, assign the first dimension to be the sample/batch dimension.</p> <p>Finally, message passing is now trivially defined as a batch matrix multiply:</p> <pre><code>def message_passing(A, F):\n    return batch_matmul(A, F)\n</code></pre> <p>Visually, this is represented as follows:</p> <p></p> <p>To this author\u2019s best knowledge, this should be the most efficient implementation of batched message passing across multiple graphs that also supports automatic differentiation, while also maintaining parity with the written equation form, hence preserving readability. The problems associated with a for-loop, sparse matrix multiplication, and dictionary carries, are removed. Moreover, the sample/batch dimension is preserved, hence it is semantically easy to map each graph to its corresponding output value. Given the current state of automatic differentiation libraries, no additional machinery is necessary to support sparse matrix products.</p> <p>The only disadvantage that this author can think of is that zero-padding may not be intuitive at first glance, and that the data must still be specially prepared and stacked first.</p>"},{"location":"machine-learning/message-passing/#concluding-words","title":"Concluding Words","text":"<p>This essay was initially motivated by the myriad of difficult-to-read message passing implementations present in the deep learning literature. Frequently, a for-loop of some kind is invoked, or an undocumented list data structure is created, in order to accomplish the message passing operation. Moreover, the model implementation is frequently not separated from the data preparation step, which makes for convoluted and mutually incompatible implementations of message passing in neural networks.</p> <p>It is my hope that while the research field is still in vogue, a technical piece that advises researchers on easily-readable and efficient implementations of message passing on graphs may help advance research practice. In particular, if our code can more closely match the equations listed in papers, that will help facilitate communication and verification of model implementations.</p> <p>To help researchers get started, an example implementation for the full data preparation and batched padded matrix multiplies in JAX is available on GitHub, archived on Zenodo.</p>"},{"location":"machine-learning/message-passing/#acknowledgments","title":"Acknowledgments","text":"<p>I thank Rif. A. Saurous for our discussion at the PyMC4 developer summit in Montreal, QC, where his laser-like focus on \u201ctensorify everything\u201d inspired many new thoughts in my mind.</p> <p>Many thanks to my wife, Nan Li, who first pointed me to the linear algebra equivalents of graphs.</p> <p>I also thank David Duvenaud and Matthew J. Johnson for their pedagogy while they were at Harvard.</p>"},{"location":"machine-learning/message-passing/#appendix","title":"Appendix","text":""},{"location":"machine-learning/message-passing/#equivalence-between-padded-and-non-padded-message-passing","title":"Equivalence between padded and non-padded message passing","text":"<p>To readers who may need an example to be convinced that matrix multiplying the padded matrices is equivalent to matrix multiplying the originals, we show the Python example below.</p> <p>Firstly, without padding:</p> <pre><code>F = np.array([[1, 0], [1, 1]])\nA = np.array([[1, 0], [0, 1]])\nM = np.dot(A, F)\n\n# Value of M\n# DeviceArray([[1, 0],\n            #  [1, 1]], dtype=int32)\n</code></pre> <p>And now, with padding:</p> <pre><code>pad_size = 2\nF_pad = np.pad(\n    F,\n    pad_width=[\n        (0, pad_size),\n        (0, 0),\n    ]\n)\nA_pad = np.pad(\n    A,\n    pad_width=[\n        (0, pad_size),\n        (0, pad_size),\n    ]\n)\n\n# F_pad:\n# DeviceArray([[1, 0],\n#              [1, 1],\n#              [0, 0],\n#              [0, 0]], dtype=int32)\n\n# A_pad:\n# DeviceArray([[1, 0, 0, 0],\n#              [0, 1, 0, 0],\n#              [0, 0, 0, 0],\n#              [0, 0, 0, 0]], dtype=int32)\n\nM_pad = np.dot(A_pad, F_pad)\n# M_pad:\n# DeviceArray([[1, 0],\n#              [1, 1],\n#              [0, 0],\n#              [0, 0]], dtype=int32)\n</code></pre>"},{"location":"machine-learning/nngp/","title":"Nngp","text":"<pre><code>%load_ext autoreload\n%autoreload 2\n%load_ext watermark\n</code></pre> <pre><code>import pandas as pd \nimport janitor\nimport janitor.chemistry\nimport os \n\nfrom jax.config import config\nimport seaborn as sns\n\nsns.set_context(\"talk\")\n\nconfig.update(\"jax_enable_x64\", True)\nos.environ[\"XLA_PYTHON_CLIENT_ALLOCATOR\"] = \"platform\"\n</code></pre> <p>The dataset is already well-cleaned; the column that is the input to our models will be the \"smiles\" column (converted into a 2048-bit Morgan-2 fingerprint), while the target column is the \"measured log solubility in mols per litre\" column.</p> <pre><code># Reset index to guarantee contiguous indexing.\n# Will be useful later.\ndata = pd.read_csv(\"https://deepchemdata.s3-us-west-1.amazonaws.com/datasets/delaney-processed.csv\").smiles2mol(\"smiles\", \"mol\").reset_index(drop=True)\ndata.head()\n</code></pre> <pre><code>import jax.numpy as np\nmorgans = data.morgan_fingerprint(\"mol\", radius=3, nbits=2048)\nmorgans = np.array(morgans)\nmorgans\n</code></pre> <pre><code>task = \"SOLUBILITY\"\ntarget_col_mapping = {\n    \"LIPO\": \"exp\",\n    \"SOLUBILITY\": \"measured log solubility in mols per litre\",\n}\ntargets = data[target_col_mapping[task]]\ntargets = np.array(targets).reshape(-1, 1)\ntargets.shape\n</code></pre> <pre><code>from jax import jit \nimport neural_tangents as nt\nfrom neural_tangents import stax\n\nN_HIDDEN = 4096\n\ninit_fn, apply_fn, kernel_fn = stax.serial(\n    stax.Dense(N_HIDDEN, W_std=1.5, b_std=0.05), stax.Erf(),\n    stax.Dense(N_HIDDEN, W_std=1.5, b_std=0.05), stax.Erf(),\n    stax.Dense(1),\n)\n\napply_fn = jit(apply_fn)\nkernel_fn = jit(kernel_fn, static_argnums=(2,))\n</code></pre> <pre><code>from jax import random as rnd\nkey = rnd.PRNGKey(71)\ninput_shape, params = init_fn(key, input_shape=(2048,))\n</code></pre> <pre><code>from sklearn.preprocessing import StandardScaler\n\n\ndef train_test_split(key, x, y, train_fraction=0.2):\n    indices = rnd.shuffle(key, np.arange(len(x)))\n    n_train = int(np.floor(len(x) * train_fraction))\n    return indices, n_train\n\n\ntrain_fraction = 0.8\nindices, n_train = train_test_split(key, morgans, targets, train_fraction)\nx_train = morgans[indices][:n_train]\ny_train = targets[indices][:n_train]\ntrain_samples = data.loc[indices][:n_train]\n\nx_test = morgans[indices][n_train:]\ny_test = targets[indices][n_train:]\ntest_samples = data.loc[indices][n_train:]\n\nss = StandardScaler()\nx_train = ss.fit_transform(x_train)\nx_test = ss.transform(x_test)\n</code></pre> <pre><code># We generate the prediction function\n# which is conditioned on the training data.\npredict_fn = nt.predict.gradient_descent_mse_ensemble(\n    kernel_fn,\n    x_train,\n    y_train,\n    diag_reg=1e-4,\n    learning_rate=0.1\n)\n\nnngp_mean, nngp_covariance = predict_fn(\n    x_test=x_test, \n    get='nngp', \n    compute_cov=True\n)\n\nnngp_std = np.sqrt(np.diag(nngp_covariance))\n</code></pre> <pre><code>ntk_mean, ntk_covariance = predict_fn(\n    x_test=x_test, \n    get='ntk', \n    compute_cov=True\n)\n\nntk_std = np.sqrt(np.diag(ntk_covariance))\n</code></pre> <pre><code>from sklearn.ensemble import RandomForestRegressor\n\nrfr = RandomForestRegressor(n_estimators=200, n_jobs=-1)\nrfr.fit(x_train, y_train)\n</code></pre> <pre><code>from sklearn.gaussian_process import GaussianProcessRegressor\n\ngpr = GaussianProcessRegressor()\ngpr.fit(x_train, y_train)\n</code></pre> <pre><code>import matplotlib.pyplot as plt\nimport seaborn as sns \n\nfrom sklearn.metrics import r2_score as r2 , explained_variance_score as evs\n\nFIG_ALPHA = 0.1\nSCOREFUNC = evs\n\nfig, axes = plt.subplots(nrows=1, ncols=5, figsize=(20, 4), sharex=True, sharey=True)\naxes = axes.flatten()\ny_preds = apply_fn(params, x_test)\naxes[0].scatter(y_test, y_preds, label=\"untrained\", alpha=FIG_ALPHA)\nscore = SCOREFUNC(y_preds.squeeze(), y_test.squeeze())\naxes[0].set_title(f\"Untrained: {score:.2f}\")\naxes[0].set_xlabel(\"Actual\")\naxes[0].set_ylabel(\"Predicted\")\n\naxes[1].errorbar(y_test.squeeze(), ntk_mean.squeeze(), yerr=ntk_std * 3, marker=\"o\", ls=\"\", alpha=FIG_ALPHA, label=\"ntk\")\nscore = SCOREFUNC(ntk_mean.squeeze(), y_test.squeeze())\naxes[1].set_title(f\"NTK: {score:.2f}\")\naxes[1].set_xlabel(\"Actual\")\n\n\naxes[2].errorbar(y_test.squeeze(), nngp_mean.squeeze(), yerr=nngp_std * 3, marker=\"o\", ls=\"\", alpha=FIG_ALPHA, label=\"nngp\")\nscore = SCOREFUNC(nngp_mean.squeeze(), y_test.squeeze())\naxes[2].set_title(f\"NNGP: {score:.2f}\")\naxes[2].set_xlabel(\"Actual\")\n\ny_preds = rfr.predict(x_test)\naxes[3].scatter(y_test, y_preds, alpha=FIG_ALPHA, label=\"random forest\")\nscore = SCOREFUNC(y_preds.squeeze(), y_test.squeeze())\naxes[3].set_title(f\"sklearn RF: {score:.2f}\")\naxes[3].set_xlabel(\"Actual\")\n\ny_preds = gpr.predict(x_test)\naxes[4].scatter(y_test, y_preds, alpha=FIG_ALPHA, label=\"random forest\")\nscore = SCOREFUNC(y_preds.squeeze(), y_test.squeeze())\naxes[4].set_title(f\"sklearn GP: {score:.2f}\")\naxes[4].set_xlabel(\"Actual\")\n\n\ndef get_x_eq_y(axes):\n    minvals = []\n    maxvals = []\n    for ax in axes.flatten():\n        xlims, ylims = ax.get_xlim(), ax.get_ylim()\n        minval = min(min(xlims), min(ylims))\n        maxval = max(max(xlims), max(ylims))\n        minvals.append(minval)\n        maxvals.append(maxval)\n    return min(minvals), max(maxvals)\n\n\nminval, maxval = get_x_eq_y(axes)\nfor ax in axes.flatten():\n    ax.plot([minval, maxval], [minval, maxval], color=\"black\", ls=\"--\")\n    ax.set_xticks(ax.get_yticks())\n    sns.despine()\nplt.tight_layout()\n</code></pre> <p>In terms of raw performance,  the scikit-learn random forest model performs the best,  while the scikit-learn Gaussian Process model performs the worst. This is all done without any hyperparameter tweaking though. Again, laziness is the primary reason  why I didn't go deeper into optimizing these baseline models, though, you know, using something like pycaret to satisfy my laziness would be a totally valid way to go.</p> <p>Now, at this point, we might be tempted to say  that the NNGP model doesn't compare well vs. the Random Forest model. However, we haven't done any kind of exploration of the parameter space for the neural network models, so such a conclusion would be premature.</p> <pre><code># Some utility functions to help out\n\nimport jax.numpy as np\nimport neural_tangents as nt\nfrom neural_tangents import stax\nfrom jax import jit \n\n\nclass NNGP:\n\"\"\"sklearn-compatible NNGP class.\"\"\"\n    def __init__(self, n_layers:int = 1, nonlinearity=stax.Relu()):\n        # Storing for information only\n        self.n_layers = n_layers\n        self.nonlinearity = nonlinearity\n\n        init_fn, apply_fn, kernel_fn = make_inf_nn(n_layers, 300, nonlinearity=nonlinearity)\n        self.init_fn = init_fn\n        self.apply_fn = apply_fn\n        self.kernel_fn = kernel_fn\n        self.predict_fn = None\n\n    def __repr__(self):\n        return f\"NNGP()\"\n\n    def fit(self, X, y):\n        if len(y.shape) == 1:\n            y = y.reshape(-1, 1)\n        self.predict_fn = nt.predict.gradient_descent_mse_ensemble(\n            self.kernel_fn, X, y, diag_reg=1e-4\n        )\n\n    def predict(self, X):\n        mean = self.predict_fn(x_test=X, get=\"nngp\", compute_cov=False)\n        return mean\n\n\ndef compute_mean_std(kernel_fn, x_train, y_train, x_test):\n\"\"\"Function to compute mean and std function.\"\"\"\n    predict_fn = nt.predict.gradient_descent_mse_ensemble(\n        kernel_fn, x_train, y_train, diag_reg=1e-4\n    )\n\n    nngp_mean, nngp_covariance = predict_fn(x_test=x_test, get=\"nngp\", compute_cov=True)\n\n    nngp_std = np.sqrt(np.diag(nngp_covariance))\n    return nngp_mean, nngp_std\n\n\ndef plot_results(SCOREFUNC, y_test, mean, std, ax, n_layers, n_hidden, FIG_ALPHA):\n\"\"\"Helper plotting function.\"\"\"\n    ax.errorbar(y_test.squeeze(), mean.squeeze(), yerr=std * 3, marker=\"o\", ls=\"\", alpha=FIG_ALPHA)\n    score = SCOREFUNC(mean.squeeze(), y_test.squeeze())\n    ax.set_title(f\"Score: {score:.2f}, {n_layers} layers,\\n{n_hidden} hidden nodes\")\n\n\ndef make_inf_nn(n_layers: int, n_hidden: int, nonlinearity=stax.Erf()):\n\"\"\"Convenience function to make infinitely wide NN of different architectures.\"\"\"\n    layers = []\n    for _ in range(n_layers):\n        layers.append(stax.Dense(n_hidden))\n        if nonlinearity:\n            layers.append(nonlinearity)\n    init_fn, apply_fn, kernel_fn = stax.serial(\n        *layers,\n        stax.Dense(1),\n    )\n    apply_fn = jit(apply_fn)\n    kernel_fn = jit(kernel_fn, static_argnums=(2,))\n    return init_fn, apply_fn, kernel_fn\n</code></pre> <pre><code>from itertools import product \n\nNUM_LAYERS = [1, 10, 20]\nNUM_HIDDEN = [1, 512, 4096]\n\nfig, axes = plt.subplots(nrows=3, ncols=3, figsize=(12, 12), sharex=True, sharey=True)\n\nfor i, (n_layers, n_hidden) in enumerate(product(NUM_LAYERS, NUM_HIDDEN)):\n    init_fn, apply_fn, kernel_fn = make_inf_nn(n_layers, n_hidden, nonlinearity=None)\n    mean, std = compute_mean_std(kernel_fn, x_train, y_train, x_test)\n\n    plot_results(SCOREFUNC, y_test, mean, std, axes.flatten()[i], n_layers, n_hidden, FIG_ALPHA)\n\nminval, maxval = get_x_eq_y(axes)\nfor ax in axes.flatten():\n    ax.plot([minval, maxval], [minval, maxval], color=\"black\", ls=\"--\")\n    sns.despine()\n\nfor ax in axes[:, 0]:\n    ax.set_ylabel(\"Predicted\")\n\nfor ax in axes[-1, :]:\n    ax.set_xlabel(\"Actual\")\n\nplt.tight_layout()\n</code></pre> <p>Apparently, with no pure linear projections, we:</p> <ol> <li>Do much better with the performance as measured by posterior expectation values.</li> <li>See that layer depth and number of hidden nodes have zero effect on performance and uncertainty widths.</li> </ol> <pre><code>fig, axes = plt.subplots(nrows=3, ncols=3, figsize=(12, 12), sharex=True, sharey=True)\n\nfor i, (n_layers, n_hidden) in enumerate(product(NUM_LAYERS, NUM_HIDDEN)):\n    init_fn, apply_fn, kernel_fn = make_inf_nn(n_layers, n_hidden)\n    mean, std = compute_mean_std(kernel_fn, x_train, y_train, x_test)\n\n    plot_results(SCOREFUNC, y_test, mean, std, axes.flatten()[i], n_layers, n_hidden, FIG_ALPHA)\n\nminval, maxval = get_x_eq_y(axes)\nfor ax in axes.flatten():\n    ax.plot([minval, maxval], [minval, maxval], color=\"black\", ls=\"--\")\n    sns.despine()\n\nfor ax in axes[:, 0]:\n    ax.set_ylabel(\"Predicted\")\n\nfor ax in axes[-1, :]:\n    ax.set_xlabel(\"Actual\")\n\nplt.tight_layout()\n</code></pre> <p>We see here that the Erf non-linearity gives us poorer performance when compared to having no non-linearities as the activation function.</p> <p>Additionally, there is a trend w.r.t. network depth. The deeper the network, the poorer the performance  of the corresponding NNGP kernel. I suspect there may be some kind of overfitting that happens. How the overfitting happens, though,  remains something of inquiry for me to learn. (My current hypothesis hinges on the hunch  that deeper networks correspond to stacked GPs, though I would need further confirmation from someone better at math than I am to see if that is correct.)</p> <p>An intriguing thing I see as well is that as we add depth to the network, the resulting NNGP kernel yields smaller uncertainties. Anthropomorphically, we would say that the neural network is more confident of its predictions even as it becomes generally more wrong. We should call these networks Dunning-Kruger networks.</p> <pre><code>fig, axes = plt.subplots(nrows=3, ncols=3, figsize=(12, 12), sharex=True, sharey=True)\n\nfor i, (n_layers, n_hidden) in enumerate(product(NUM_LAYERS, NUM_HIDDEN)):\n    init_fn, apply_fn, kernel_fn = make_inf_nn(n_layers, n_hidden, nonlinearity=stax.Relu())\n    mean, std = compute_mean_std(kernel_fn, x_train, y_train, x_test)\n\n    plot_results(SCOREFUNC, y_test, mean, std, axes.flatten()[i], n_layers, n_hidden, FIG_ALPHA)\n\nminval, maxval = get_x_eq_y(axes)\nfor ax in axes.flatten():\n    ax.plot([minval, maxval], [minval, maxval], color=\"black\", ls=\"--\")\n    sns.despine()\n\nfor ax in axes[:, 0]:\n    ax.set_ylabel(\"Predicted\")\n\nfor ax in axes[-1, :]:\n    ax.set_xlabel(\"Actual\")\n\nplt.tight_layout()\n</code></pre> <p>A really cool result shows up here: we have better performance using the Relu activation function! The same trend w.r.t. number of hidden layers also holds: as we increase the number of hidden layers, the performance decreases, and the uncertainty decreases. The network becomes a Dunning-Kruger network.</p> <pre><code>fig, axes = plt.subplots(nrows=3, ncols=3, figsize=(12, 12), sharex=True, sharey=True)\n\nfor i, (n_layers, n_hidden) in enumerate(product(NUM_LAYERS, NUM_HIDDEN)):\n    init_fn, apply_fn, kernel_fn = make_inf_nn(n_layers, n_hidden, nonlinearity=stax.Gelu())\n    mean, std = compute_mean_std(kernel_fn, x_train, y_train, x_test)\n\n    plot_results(SCOREFUNC, y_test, mean, std, axes.flatten()[i], n_layers, n_hidden, FIG_ALPHA)\n\nminval, maxval = get_x_eq_y(axes)\nfor ax in axes.flatten():\n    ax.plot([minval, maxval], [minval, maxval], color=\"black\", ls=\"--\")\n    sns.despine()\n\nfor ax in axes[:, 0]:\n    ax.set_ylabel(\"Predicted\")\n\nfor ax in axes[-1, :]:\n    ax.set_xlabel(\"Actual\")\n\nplt.tight_layout()\n</code></pre> <p>The Gelu network is a highly interesting one! As the depth increases, the correctness of the network, i.e. performance, doesn't go down, but the uncertainties do become smaller. Anthropomorphically, the network remains just as wrong about predictions, and becomes more confident of them. As usual, the number of hidden nodes affect neither the model's expected performance nor the uncertainties.</p> <pre><code>from neural_tangents import stax\nnonlinearities = {\n    \"none\": None,\n    \"ReLu\": stax.Relu(),\n    \"GeLu\": stax.Gelu(),\n    \"Erf\": stax.Erf(),\n}\n\nfig, ax = plt.subplots(nrows=1, ncols=1, figsize=(4, 4))\n\n\ndef ecdf(data):\n    x, y = np.sort(data), np.arange(1, len(data)+1) / len(data)\n    return x, y\n\n\nfor i, (name, nonlinearity) in enumerate(nonlinearities.items()):\n    init_fn, apply_fn, kernel_fn = make_inf_nn(n_layers=1, n_hidden=1, nonlinearity=nonlinearity)\n    mean, std = compute_mean_std(kernel_fn, x_train, y_train, x_test)\n    x, y = ecdf(std)\n    ax.plot(x, y, label=name)\nax.set_xlabel(\"Std. Dev\")\nax.set_ylabel(\"Cumulative Fraction\")\nax.set_title(\"Distribution\\nof uncertainties\")\nax.legend()\nsns.despine()\n</code></pre> <p>What do we see here?  Well, to begin with, infinitely wide neural networks with no nonlinearities end up being the most uncertain about their predictions. The ReLu and GeLu network end up showing  practically identical distributions of uncertainties in test set predictions. Meanwhile, the Erf network, which was the worst of all of the networks tested, showed a tighter distribution of uncertainties, though roughly at the same scale as the other networks.</p> <pre><code>def evaluate_uncertainty(mean: float, std: float, y_test: float):\n    lt = y_test &amp;lt; mean + std * 3\n    gt = y_test &amp;gt; mean - std * 3\n    return np.logical_and(lt, gt)\n</code></pre> <pre><code>from jax import vmap\n\nresults = dict()\nfor i, (name, nonlinearity) in enumerate(nonlinearities.items()):\n    init_fn, apply_fn, kernel_fn = make_inf_nn(n_layers=1, n_hidden=1, nonlinearity=nonlinearity)\n    mean, std = compute_mean_std(kernel_fn, x_train, y_train, x_test)\n    result = vmap(evaluate_uncertainty)(mean, std, y_test)\n    results[name] = result.squeeze()\n</code></pre> <pre><code>import pandas as pd\n\npd.DataFrame(results).mean().plot(kind=\"bar\", rot=45)\nax = plt.gca()\nax.set_xlabel(\"Activation\")\nax.set_ylabel(\"P(correct)\")\nax.set_ylim(0, 1)\nsns.despine()\n</code></pre> <p>As we can see above, the ReLu function has the best performing uncertainties, with 70% of the posteriors harbouring the correct value. One might say that the ReLu infinite networks  are best calibrated for prediction purposes.</p> <p>You'd think that the linear projection networks would have well-calibrated uncertainties given their widths, and that's certainly something I thought would happen, but that turned out not to be the case.</p> <pre><code>test_indices = indices[n_train:]\n</code></pre> <pre><code>from jax.scipy.stats import norm\ndef prob_above_threshold(mean: float, std: float, threshold: float):\n    return 1 - norm.cdf(x=threshold, loc=mean, scale=std)\n</code></pre> <pre><code>from functools import partial\nprob_thresh = dict()\nfor i, (name, nonlinearity) in enumerate(nonlinearities.items()):\n    init_fn, apply_fn, kernel_fn = make_inf_nn(n_layers=1, n_hidden=1, nonlinearity=nonlinearity)\n    mean, std = compute_mean_std(kernel_fn, x_train, y_train, x_test)\n    result = vmap(partial(prob_above_threshold, threshold=-0.1))(mean, std)\n    prob_thresh[name] = result.squeeze()\n</code></pre> <p>Let's now query for molecules in the test set that are predicted to have greater than 50% probability of having solubility greater than 0. This would be akin to helping a chemist prioritize the selection of molecules where we'd be happy to bet at 1:1 odds of having a desirable property, given the model's predictions.</p> <p>Remember that the uncertainties dataframe is indexed differently, so we'll need to do some fancy indexing to get back to the indexing that is used in the original dataframe.</p> <pre><code>prob_thresh_df = pd.DataFrame(prob_thresh).sort_values(\"ReLu\", ascending=False).query(\"ReLu &amp;gt; 0.5\")\nprob_thresh_df\n</code></pre> <pre><code>test_samples.loc[test_indices[prob_thresh_df.index.values]]\n</code></pre> <p>Hey, not bad! From what I know about chemistry, glycerol is highly soluble in water, and is used by microbiologists to protect frozen bacteria cells from being cracked open upon freeze/thawing cycles. Urea is definitely a molecule that is soluble in water; we make it all the time in our bodies as a waste product, and is carried out of our body in our urine. Raffinose is a sugar, and so with all of the oxygens and hydrogens in there, it'll likely be dissolvable in water. 2-hydroxy-something always sounds like it'll be soluble in water, and Ethanol... well, we know how that one goes :).</p> <pre><code>%watermark\n</code></pre> <pre><code>%watermark --iversions\n</code></pre>"},{"location":"machine-learning/nngp/#how-useful-are-infinitely-wide-neural-networks","title":"How useful are infinitely wide neural networks?","text":"<p>In this notebook, I would like to explore the use of the Neural Tangents package for doing cheminformatics tasks.</p> <p>For the non-cheminformatician reading this, cheminformatics tasks should be treated as a surrogate for complex modelling problems that do not have an obvious input-output relationship structure that can be easily written in an equation, hence necessitating the use of a neural network.</p> <p>The goal here is not to see how we can achieve state-of-the-art (SOTA) performance, but instead empirically explore how infinitely wide neural networks could possibly be leveraged to improve a data scientist's workflow  and solve complex problems. As such, you won't see us try to tweak hyperparameters to squeeze out maximum performance. Rather, you'll see what we hope to be is a systematic exploration of how neural network architectures affect their infinite-width limit performance.</p>"},{"location":"machine-learning/nngp/#motivation-for-this-notebook","title":"Motivation for this notebook","text":"<p>Given the popularity of neural networks  and their productive use in applications, it sort of begs the question,  why would one want to be interested in infinitely-wide neural networks as an applied topic?</p>"},{"location":"machine-learning/nngp/#practical-implications-for-predictive-modelling","title":"Practical implications for predictive modelling","text":"<p>Putting aside the really fascinating theoretical connections between Gaussian Processes and infinitely wide neural networks, I saw some potentially attractive practical implications of infinitely wide neural networks. In particular, having read the original paper, and studying the code, I had this hunch that infinitely wide neural networks could save us deep learners from a wide swathe of Sisyphean hyperparameter tuning tasks, such as tuning learning rate, number of hidden nodes, number of epochs, and more.</p>"},{"location":"machine-learning/nngp/#curiousity-surrounding-kernels","title":"Curiousity surrounding kernels","text":"<p>Having read the paper, I had this hunch that infinitely wide neural networks should give us different Gaussian Process kernels depending on the neural network architecture. As we know from GP literature, the kernel,  which is a function of the distance between data points in input space, dictates the behaviour of a GP. Kernels are compositionally additive, and can thus be used to simultaneously model short-range and long-range behaviour, at least in 1-D examples that I have seen.</p>"},{"location":"machine-learning/nngp/#try-moleculenets-esol","title":"Try MoleculeNet's ESOL","text":"<p>The dataset that we will focus on here is MoleculeNet's solubility dataset.</p>"},{"location":"machine-learning/nngp/#writing-infinitely-wide-neural-networks","title":"Writing infinitely wide neural networks","text":"<p>Infinitely wide neural networks are written using the neural tangents library developed by Google Research. It is based on JAX, and provides a neural network library that lets us analytically obtain the infinite-width kernel  corresponding to the particular neural network architecture specified. This is known as a Neural Network Gaussian Process (NNGP) kernel.</p> <p>I will primarily be concerned with the NNGP kernel rather than the Neural Tangent Kernel (NTK). Mostly for simplicity's sake; being the lazy person I am,  I'll try avoiding adding an orthogonal and combinatorial axis of complexity wherever possible.</p> <p>Here is an example infinitely-wide neural network with two hidden layers, each with 4096 hidden nodes.</p>"},{"location":"machine-learning/nngp/#initial-model-params","title":"Initial model params","text":"<p>For benchmarking purposes, we want to obtain the parameters at initialization in order to compare how the NNGP kernel conditioned on observed data compares against the initialized, unfitted model.</p>"},{"location":"machine-learning/nngp/#preprocessing-of-data","title":"Preprocessing of data","text":"<p>We are going to use a fixed split, again, for simplicity purposes. To take advantage of JAX's deterministic random number generation capabilities, I have written my own train/test split function.</p> <p>We will bring all of our inputs into a standardized Gaussian space, mostly to ensure that our inputs  are nicely scaled to roughly the same magnitudes. (Something something good numerical behaviour.)</p>"},{"location":"machine-learning/nngp/#obtaining-the-nngp-and-ntk-kernels","title":"Obtaining the NNGP and NTK kernels","text":"<p>Following the notebook examples released by the neural tangents team, we now get the NNGP/NTK mean and covariance.</p> <p>But wait, didn't I say I wasn't going to care about NTK kernels? Yeah, I guess I waffled a bit here. I was copying and pasting code at this point.</p>"},{"location":"machine-learning/nngp/#compare-against-standard-models","title":"Compare against standard models","text":"<p>You can't have a benchmarking exercise  without having some models to compare against.  We'll compare the NNGP/NTK means against predictions from a fitted Random Forest Regressor and a Gaussian Process Regressor, taken from our favourite standard machine learning library scikit-learn.</p>"},{"location":"machine-learning/nngp/#drumroll-please","title":"Drumroll please...","text":"<p>Now for our first benchmarking exericse: how do NNGPs and NTKs perform against standard models, and against the initialized neural network itself?</p> <p>We'll be using the explained variance score to measure performance. Why not others?  Well, again, I'm a bit lazy, but there's another actually good reason here. Firstly, explained variance is a measure of \"goodness of fit\"  that is scale-free. It is defined as:</p> 1 - \\frac{var(residuals)}{var(target)} <p>I wrote a blog post before on why I think the variance explained metric is a pretty neat one, even if it's not the most widely understood. The tl;dr of why is that it's a scale-free metric that is easily interpretable.</p>"},{"location":"machine-learning/nngp/#optimizing-the-infinitely-wide-neural-network","title":"Optimizing the infinitely wide neural network","text":"<p>There's still some pieces of the infinitely wide neural network that I think we need to get a practical handle over. In particular, how does the neural network performance, and the uncertainties, change as a function of:</p> <ul> <li>The number of nodes in a hidden layer?</li> <li>The depth of the neural network?</li> <li>The activation function that we use?</li> </ul> <p>That's what we're going to explore in this section of the notebook.</p> <p>It's always good to go into an inquiry with some expected range of answers. Here's mine, stated up front:</p> <p>Number of nodes: The number of nodes shouldn't matter. We're talking about expanding each layer to infinite width, after all.</p> <p>Depth: Depth might affect the structure of the corresponding NNGP kernel. However, I am not well-versed enough in the math to know exactly how.</p> <p>Activation function: I suspect the activation function will also affect the structure of the NNGP kernel. A hunch tells me it may have the weakest effect compared to depth, but I can't back that up with any theory. It's just a hunch.</p> <p>Let me first set up some utility functions below to help out with coding later.</p>"},{"location":"machine-learning/nngp/#experiment-setup","title":"Experiment setup","text":"<p>For number of layers, we're going to explore 1, 10 and 20 layers.</p> <p>For number of nodes per hidden layer,  we'll explore what happens with 1, 512, and 4096 nodes. (I mean, it's gonna go to infinity, so why would I try thousands of hidden nodes, right?)</p> <p>For non-linearities, we'll do:</p> <ul> <li>No non-linearities (i.e. simple linear projections)</li> <li>Erf non-linearity</li> <li>Relu non-linearity</li> <li>Gelu non-linearity</li> </ul> <p>The three non-linearities are available in neural tangents, so we're using them for convenience. At 4x3x3, we're already at 36 combinations, which I think is more than enough to see any patterns.</p>"},{"location":"machine-learning/nngp/#linear-projections-aka-no-non-linearities","title":"Linear projections (a.k.a. no non-linearities)","text":"<p>Let's start with linear projections and see what happens.</p>"},{"location":"machine-learning/nngp/#erf-nonlinearity","title":"Erf nonlinearity","text":"<p>Now let's see what happens when we use the Erf nonlinearity, which was the default activation function  that the neural tangents demo notebooks use.</p>"},{"location":"machine-learning/nngp/#try-with-relu-non-linearity","title":"Try with Relu non-linearity","text":"<p>Let's now try a popular non-linearity, the Relu non-linearity.</p>"},{"location":"machine-learning/nngp/#try-with-gelu-nonlinearity","title":"Try with Gelu nonlinearity","text":""},{"location":"machine-learning/nngp/#summary-of-trends-observed","title":"Summary of Trends observed","text":""},{"location":"machine-learning/nngp/#depth","title":"Depth","text":"<p>The observable trend here is that going with deeper nets  that have non-identity activation functions leads to overfitting on the training set; shallower nets have the best performance. This holds in general, with the Gelu network being the exception of the activation functions tested here.</p>"},{"location":"machine-learning/nngp/#activation-functions","title":"Activation functions","text":"<p>Of the three tested inside the nt.stax library of nonlinearities, the Gelu activation function provided the best performance based on the posterior expectation, with the Relu activation function a close second.</p> <p>The effect of activation functions on model performance was dramatic, and this was something out of expectation based on my stated prior beliefs.</p> <p>What the effect of activation functions are on the width of the posterior uncertainties is something we will investigate next.</p>"},{"location":"machine-learning/nngp/#number-of-hidden-nodes","title":"Number of hidden nodes","text":"<p>From the get-go,  I expected that the number of hidden nodes declared in the network constructor should not play a role with NNGP performance, as after all, we're taking the number of hidden nodes to the infinite limit. As the results show, whether we use 1, 2, or 3 hidden nodes in the hidden layer, the infinite-width performance is identical.</p>"},{"location":"machine-learning/nngp/#comparison-between-non-linearities-effects-on-uncertainties","title":"Comparison between non-linearities' effects on uncertainties","text":"<p>Having seen that neural networks begin to exhibit the Dunning-Kruger effect as they get more powerful (at least in the limit of infinite width), we will limit further probing of our model  to just the single layer depth networks with a single hidden node. I'm playing with fire here, just to prove the point. Take that, overparameterized networks, we're going infinite now.</p> <p>To investigate how the nonlinearities affect uncertainty, we will plot the distribution of standard deviations of the test set predictions. It will be an ECDF plot, so that we can cram everything into one plot and compare directly. (Histograms are so 1950s!)</p>"},{"location":"machine-learning/nngp/#how-do-we-measure-the-performance-of-the-uncertainties","title":"How do we measure the \"performance\" of the uncertainties?","text":"<p>Well, one way to do so is to ask what fraction of the uncertainties' range capture the correct value. We have the standard deviation of predictions from our GP; 3x the standard deviation gives us about 96% of the posterior probability. If the mean \u00b1 3x standard deviation encapsulates the true value, then we know our posterior standard deviation was highly likely to capture the true value, even if the expected value was off.</p>"},{"location":"machine-learning/nngp/#using-the-uncertainties-for-decision-making","title":"Using the uncertainties for decision-making","text":"<p>In a chemistry setting, we're usually concerned with prioritization tasks. A chemist's time for testing molecules is usually finite, so we need some help rank-ordering items in a principled fashion. Well, turns out when you have Gaussian uncertainties, you can do this pretty easily.</p> <p>If you know anything about Bayesian optimization, we need to have an acquisition function that helps us rank-order our predictions. The acquisition function that gets chosen is going to depend on the task at hand. Because we're dealing with solubility as a task, and in medicinal chemistry high solubility is a desirable property, we can rank order our test samples according to the probability that they are above some threshold value. This is effectively evaluating the survival function, or <code>1 - CDF(threshold)</code>, of a Gaussian.</p>"},{"location":"machine-learning/nngp/#conclusions","title":"Conclusions","text":"<p>Overall, this deep dive into infinite-width neural networks tells me that we deep learners have a superb tool at our disposal for modelling complex relationships that don't have an obvious input-output structure. We can write a single layer neural network, expand it out to its infinite width form easily to get a Neural Net Gaussian Process kernel that, when conditioned on training data, could give us good test performance. (No guarantees though, we'd have to benchmark.) For expensive data collection problems like those that we find in the life sciences, this is a superb modelling tool on many levels.</p> <p>Firstly, Gaussian processes are highly flexible and inherently Bayesian modelling tools, but proper kernel design is difficult; neural net design is an easier alternative language to work with to generate GPs for our modelling problems. Infinitely wide neural networks give us the best of both worlds.</p> <p>Secondly, because we get a highly flexible function learner, complex functions are easily modelled without needing to do much work with hyperparameter tuning. Down with the Sisyphean tasks of twiddling with neural networks! (OK, maybe I'm taking the rant a bit too far.) Single layer infinitely wide neural networks did a practically great job on solubility with standard molecular descriptors, were as easy to wield as the cheminformatics-favourite Random Forest model, and had performance metrics effectively on par with Random Forests. We may be able to squeeze out more performance by adopting alternative model architectures that have different inductive biases, such as graph neural networks, but I have to admit, it was pretty easy to get to the performance levels we observed here.</p> <p>Thirdly, because NNGPs are inherently Bayesian, the suite of tools from Bayesian decision science are at our disposal. Prioritizing, ranking, selecting... once you have the posterior likelihood density at hand, you have the principle by which these activities can be done. No more premature thresholds; delay all binarization until the end, when we have the posterior likelihood calculated. No fancy methods needed either; \"likelihood is all you need\".</p>"},{"location":"machine-learning/nngp/#lessons-learned","title":"Lessons Learned","text":"<p>From this exercise, it seems evident to me that the nonlinearity is the biggest thing that affects the kernel, which in turn affects the expected performance (as given by the posterior means) and uncertainty calibration (as measured by the posterior standard deviations). If you remember where my headspace was when we first started, I was completely wrong w.r.t. this point; the NNGP kernel is highly influenced by the choice of activation function. Where I was correct was that the number of hidden nodes per layer didn't matter one bit for the performance of the network, because we are, after all, going to infinity ~~and beyond~~.</p> <p>Knowing that we can get pretty decent expected predictive performance and somewhat nicely calibrated uncertainties without needing to fiddle with layer width, number of layers, number of training epochs, learning rate, and the myriad of other hyperparameters that we might be interested in... is actually kind of a nice result. With the NNGP kernel, we basically eliminate a ton of hyperparameters in training neural nets, and can focus primarily on the non-linear activation function to design the best NNGP kernel. Hyperparameter tuning is the worst Sisyphean task that will bore the crap out of any data scientist.</p> <p>Hence, from the perspective of workflow, using infinitely-wide neural networks is an attractive proposition for modelling problems that do not have an obvious input-output relationship. At the end of the day, by using infinitely wide neural networks, fitting a Gaussian Process with a particular kernel to the problem at hand, and this cuts down on the number of hyperparameters that we have to fiddle with. Pragmatically, I have a hunch that we can mostly stick with single layer NNGPs and leverage the uncertainty instead.</p> <p>Finally, I have to comment on the lack of compute intensity needed  to wield infinitely wide neural networks. Everything I did in this notebook was first prototyped on an M1 MacBook Air, done in emulation mode. No GPUs were used. None! The entire notebook runs from top to bottom in under two minutes. Take that, Transformers.</p>"},{"location":"machine-learning/nngp/","title":"How useful are infinitely wide neural networks?","text":"<p>In this notebook, I would like to explore the use of the Neural Tangents package for doing cheminformatics tasks.</p> <p>For the non-cheminformatician reading this, cheminformatics tasks should be treated as a surrogate for complex modelling problems that do not have an obvious input-output relationship structure that can be easily written in an equation, hence necessitating the use of a neural network.</p> <p>The goal here is not to see how we can achieve state-of-the-art (SOTA) performance, but instead empirically explore how infinitely wide neural networks could possibly be leveraged to improve a data scientist's workflow and solve complex problems. As such, you won't see us try to tweak hyperparameters to squeeze out maximum performance. Rather, you'll see what we hope to be is a systematic exploration of how neural network architectures affect their infinite-width limit performance.</p>"},{"location":"machine-learning/nngp/#motivation-for-this-notebook","title":"Motivation for this notebook","text":"<p>Given the popularity of neural networks and their productive use in applications, it sort of begs the question, why would one want to be interested in infinitely-wide neural networks as an applied topic?</p>"},{"location":"machine-learning/nngp/#practical-implications-for-predictive-modelling","title":"Practical implications for predictive modelling","text":"<p>Putting aside the really fascinating theoretical connections between Gaussian Processes and infinitely wide neural networks, I saw some potentially attractive practical implications of infinitely wide neural networks. In particular, having read the original paper, and studying the code, I had this hunch that infinitely wide neural networks could save us deep learners from a wide swathe of Sisyphean hyperparameter tuning tasks, such as tuning learning rate, number of hidden nodes, number of epochs, and more.</p>"},{"location":"machine-learning/nngp/#curiousity-surrounding-kernels","title":"Curiousity surrounding kernels","text":"<p>Having read the paper, I had this hunch that infinitely wide neural networks should give us different Gaussian Process kernels depending on the neural network architecture. As we know from GP literature, the kernel, which is a function of the distance between data points in input space, dictates the behaviour of a GP. Kernels are compositionally additive, and can thus be used to simultaneously model short-range and long-range behaviour, at least in 1-D examples that I have seen.</p>"},{"location":"machine-learning/nngp/#try-moleculenets-esol","title":"Try MoleculeNet's ESOL","text":"<p>The dataset that we will focus on here is MoleculeNet's solubility dataset.</p> <pre><code>%load_ext autoreload\n%autoreload 2\n%load_ext watermark\n</code></pre> <pre><code>import pandas as pd\nimport janitor\nimport janitor.chemistry\nimport os\n\nfrom jax.config import config\nimport seaborn as sns\n\nsns.set_context(\"talk\")\n\nconfig.update(\"jax_enable_x64\", True)\nos.environ[\"XLA_PYTHON_CLIENT_ALLOCATOR\"] = \"platform\"\n</code></pre> <p>The dataset is already well-cleaned; the column that is the input to our models will be the \"smiles\" column (converted into a 2048-bit Morgan-2 fingerprint), while the target column is the \"measured log solubility in mols per litre\" column.</p> <pre><code># Reset index to guarantee contiguous indexing.\n# Will be useful later.\ndata = pd.read_csv(\"https://deepchemdata.s3-us-west-1.amazonaws.com/datasets/delaney-processed.csv\").smiles2mol(\"smiles\", \"mol\").reset_index(drop=True)\ndata.head()\n</code></pre> Compound ID ESOL predicted log solubility in mols per litre Minimum Degree Molecular Weight Number of H-Bond Donors Number of Rings Number of Rotatable Bonds Polar Surface Area measured log solubility in mols per litre smiles mol 0 Amigdalin -0.974 1 457.432 7 3 7 202.32 -0.77 OCC3OC(OCC2OC(OC(C#N)c1ccccc1)C(O)C(O)C2O)C(O)... &lt;rdkit.Chem.rdchem.Mol object at 0x7febddddaff0&gt; 1 Fenfuram -2.885 1 201.225 1 2 2 42.24 -3.30 Cc1occc1C(=O)Nc2ccccc2 &lt;rdkit.Chem.rdchem.Mol object at 0x7febddddb060&gt; 2 citral -2.579 1 152.237 0 0 4 17.07 -2.06 CC(C)=CCCC(C)=CC(=O) &lt;rdkit.Chem.rdchem.Mol object at 0x7febddddb0d0&gt; 3 Picene -6.618 2 278.354 0 5 0 0.00 -7.87 c1ccc2c(c1)ccc3c2ccc4c5ccccc5ccc43 &lt;rdkit.Chem.rdchem.Mol object at 0x7febddddb140&gt; 4 Thiophene -2.232 2 84.143 0 1 0 0.00 -1.33 c1ccsc1 &lt;rdkit.Chem.rdchem.Mol object at 0x7febddddb1b0&gt; <pre><code>import jax.numpy as np\nmorgans = data.morgan_fingerprint(\"mol\", radius=3, nbits=2048)\nmorgans = np.array(morgans)\nmorgans\n</code></pre> <pre><code>Array([[0., 1., 0., ..., 0., 0., 0.],\n[0., 0., 0., ..., 0., 0., 0.],\n[0., 0., 0., ..., 0., 0., 0.],\n...,\n[0., 0., 0., ..., 0., 0., 0.],\n[0., 1., 0., ..., 0., 0., 0.],\n[0., 0., 0., ..., 0., 0., 0.]], dtype=float64)\n</code></pre> <pre><code>task = \"SOLUBILITY\"\ntarget_col_mapping = {\n    \"LIPO\": \"exp\",\n    \"SOLUBILITY\": \"measured log solubility in mols per litre\",\n}\ntargets = data[target_col_mapping[task]]\ntargets = np.array(targets).reshape(-1, 1)\ntargets.shape\n</code></pre> <pre><code>(1128, 1)\n</code></pre>"},{"location":"machine-learning/nngp/#writing-infinitely-wide-neural-networks","title":"Writing infinitely wide neural networks","text":"<p>Infinitely wide neural networks are written using the neural tangents library developed by Google Research. It is based on JAX, and provides a neural network library that lets us analytically obtain the infinite-width kernel corresponding to the particular neural network architecture specified. This is known as a Neural Network Gaussian Process (NNGP) kernel.</p> <p>I will primarily be concerned with the NNGP kernel rather than the Neural Tangent Kernel (NTK). Mostly for simplicity's sake; being the lazy person I am, I'll try avoiding adding an orthogonal and combinatorial axis of complexity wherever possible.</p> <p>Here is an example infinitely-wide neural network with two hidden layers, each with 4096 hidden nodes.</p> <pre><code>from jax import jit\nimport neural_tangents as nt\nfrom neural_tangents import stax\n\nN_HIDDEN = 4096\n\ninit_fn, apply_fn, kernel_fn = stax.serial(\n    stax.Dense(N_HIDDEN, W_std=1.5, b_std=0.05), stax.Erf(),\n    stax.Dense(N_HIDDEN, W_std=1.5, b_std=0.05), stax.Erf(),\n    stax.Dense(1),\n)\n\napply_fn = jit(apply_fn)\nkernel_fn = jit(kernel_fn, static_argnums=(2,))\n</code></pre> <pre><code>2022-12-23 14:34:25.930704: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n2022-12-23 14:34:25.930781: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n2022-12-23 14:34:25.930789: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n</code></pre>"},{"location":"machine-learning/nngp/#initial-model-params","title":"Initial model params","text":"<p>For benchmarking purposes, we want to obtain the parameters at initialization in order to compare how the NNGP kernel conditioned on observed data compares against the initialized, unfitted model.</p> <pre><code>from jax import random as rnd\nkey = rnd.PRNGKey(71)\ninput_shape, params = init_fn(key, input_shape=(2048,))\n</code></pre>"},{"location":"machine-learning/nngp/#preprocessing-of-data","title":"Preprocessing of data","text":"<p>We are going to use a fixed split, again, for simplicity purposes. To take advantage of JAX's deterministic random number generation capabilities, I have written my own train/test split function.</p> <p>We will bring all of our inputs into a standardized Gaussian space, mostly to ensure that our inputs are nicely scaled to roughly the same magnitudes. (Something something good numerical behaviour.)</p> <pre><code>from sklearn.preprocessing import StandardScaler\n\n\ndef train_test_split(key, x, y, train_fraction=0.2):\n    indices = rnd.shuffle(key, np.arange(len(x)))\n    n_train = int(np.floor(len(x) * train_fraction))\n    return indices, n_train\n\n\ntrain_fraction = 0.8\nindices, n_train = train_test_split(key, morgans, targets, train_fraction)\nx_train = morgans[indices][:n_train]\ny_train = targets[indices][:n_train]\ntrain_samples = data.loc[indices][:n_train]\n\nx_test = morgans[indices][n_train:]\ny_test = targets[indices][n_train:]\ntest_samples = data.loc[indices][n_train:]\n\nss = StandardScaler()\nx_train = ss.fit_transform(x_train)\nx_test = ss.transform(x_test)\n</code></pre> <pre><code>/home/ericmjl/anaconda/envs/essays-on-data-science/lib/python3.10/site-packages/jax/_src/random.py:400: FutureWarning: jax.random.shuffle is deprecated and will be removed in a future release. Use jax.random.permutation with independent=True.\nwarnings.warn(msg, FutureWarning)\n</code></pre>"},{"location":"machine-learning/nngp/#obtaining-the-nngp-and-ntk-kernels","title":"Obtaining the NNGP and NTK kernels","text":"<p>Following the notebook examples released by the neural tangents team, we now get the NNGP/NTK mean and covariance.</p> <p>But wait, didn't I say I wasn't going to care about NTK kernels? Yeah, I guess I waffled a bit here. I was copying and pasting code at this point.</p> <pre><code># We generate the prediction function\n# which is conditioned on the training data.\npredict_fn = nt.predict.gradient_descent_mse_ensemble(\n    kernel_fn,\n    x_train,\n    y_train,\n    diag_reg=1e-4,\n    learning_rate=0.1\n)\n\nnngp_mean, nngp_covariance = predict_fn(\n    x_test=x_test,\n    get='nngp',\n    compute_cov=True\n)\n\nnngp_std = np.sqrt(np.diag(nngp_covariance))\n</code></pre> <pre><code>ntk_mean, ntk_covariance = predict_fn(\n    x_test=x_test,\n    get='ntk',\n    compute_cov=True\n)\n\nntk_std = np.sqrt(np.diag(ntk_covariance))\n</code></pre>"},{"location":"machine-learning/nngp/#compare-against-standard-models","title":"Compare against standard models","text":"<p>You can't have a benchmarking exercise without having some models to compare against. We'll compare the NNGP/NTK means against predictions from a fitted Random Forest Regressor and a Gaussian Process Regressor, taken from our favourite standard machine learning library scikit-learn.</p> <pre><code>from sklearn.ensemble import RandomForestRegressor\n\nrfr = RandomForestRegressor(n_estimators=200, n_jobs=-1)\nrfr.fit(x_train, y_train)\n</code></pre> <pre><code>/tmp/ipykernel_986531/3912822998.py:4: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n  rfr.fit(x_train, y_train)\n</code></pre> <pre>RandomForestRegressor(n_estimators=200, n_jobs=-1)</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomForestRegressor<pre>RandomForestRegressor(n_estimators=200, n_jobs=-1)</pre> <pre><code>from sklearn.gaussian_process import GaussianProcessRegressor\n\ngpr = GaussianProcessRegressor()\ngpr.fit(x_train, y_train)\n</code></pre> <pre>GaussianProcessRegressor()</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GaussianProcessRegressor<pre>GaussianProcessRegressor()</pre>"},{"location":"machine-learning/nngp/#drumroll-please","title":"Drumroll please...","text":"<p>Now for our first benchmarking exericse: how do NNGPs and NTKs perform against standard models, and against the initialized neural network itself?</p> <p>We'll be using the explained variance score to measure performance. Why not others? Well, again, I'm a bit lazy, but there's another actually good reason here. Firstly, explained variance is a measure of \"goodness of fit\" that is scale-free. It is defined as:</p> 1 - \\frac{var(residuals)}{var(target)} <p>I wrote a blog post before on why I think the variance explained metric is a pretty neat one, even if it's not the most widely understood. The tl;dr of why is that it's a scale-free metric that is easily interpretable.</p> <pre><code>import matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.metrics import r2_score as r2 , explained_variance_score as evs\n\nFIG_ALPHA = 0.1\nSCOREFUNC = evs\n\nfig, axes = plt.subplots(nrows=1, ncols=5, figsize=(20, 4), sharex=True, sharey=True)\naxes = axes.flatten()\ny_preds = apply_fn(params, x_test)\naxes[0].scatter(y_test, y_preds, label=\"untrained\", alpha=FIG_ALPHA)\nscore = SCOREFUNC(y_preds.squeeze(), y_test.squeeze())\naxes[0].set_title(f\"Untrained: {score:.2f}\")\naxes[0].set_xlabel(\"Actual\")\naxes[0].set_ylabel(\"Predicted\")\n\naxes[1].errorbar(y_test.squeeze(), ntk_mean.squeeze(), yerr=ntk_std * 3, marker=\"o\", ls=\"\", alpha=FIG_ALPHA, label=\"ntk\")\nscore = SCOREFUNC(ntk_mean.squeeze(), y_test.squeeze())\naxes[1].set_title(f\"NTK: {score:.2f}\")\naxes[1].set_xlabel(\"Actual\")\n\n\naxes[2].errorbar(y_test.squeeze(), nngp_mean.squeeze(), yerr=nngp_std * 3, marker=\"o\", ls=\"\", alpha=FIG_ALPHA, label=\"nngp\")\nscore = SCOREFUNC(nngp_mean.squeeze(), y_test.squeeze())\naxes[2].set_title(f\"NNGP: {score:.2f}\")\naxes[2].set_xlabel(\"Actual\")\n\ny_preds = rfr.predict(x_test)\naxes[3].scatter(y_test, y_preds, alpha=FIG_ALPHA, label=\"random forest\")\nscore = SCOREFUNC(y_preds.squeeze(), y_test.squeeze())\naxes[3].set_title(f\"sklearn RF: {score:.2f}\")\naxes[3].set_xlabel(\"Actual\")\n\ny_preds = gpr.predict(x_test)\naxes[4].scatter(y_test, y_preds, alpha=FIG_ALPHA, label=\"random forest\")\nscore = SCOREFUNC(y_preds.squeeze(), y_test.squeeze())\naxes[4].set_title(f\"sklearn GP: {score:.2f}\")\naxes[4].set_xlabel(\"Actual\")\n\n\ndef get_x_eq_y(axes):\n    minvals = []\n    maxvals = []\n    for ax in axes.flatten():\n        xlims, ylims = ax.get_xlim(), ax.get_ylim()\n        minval = min(min(xlims), min(ylims))\n        maxval = max(max(xlims), max(ylims))\n        minvals.append(minval)\n        maxvals.append(maxval)\n    return min(minvals), max(maxvals)\n\n\nminval, maxval = get_x_eq_y(axes)\nfor ax in axes.flatten():\n    ax.plot([minval, maxval], [minval, maxval], color=\"black\", ls=\"--\")\n    ax.set_xticks(ax.get_yticks())\n    sns.despine()\nplt.tight_layout()\n</code></pre> <p></p> <p>In terms of raw performance, the scikit-learn random forest model performs the best, while the scikit-learn Gaussian Process model performs the worst. This is all done without any hyperparameter tweaking though. Again, laziness is the primary reason why I didn't go deeper into optimizing these baseline models, though, you know, using something like pycaret to satisfy my laziness would be a totally valid way to go.</p> <p>Now, at this point, we might be tempted to say that the NNGP model doesn't compare well vs. the Random Forest model. However, we haven't done any kind of exploration of the parameter space for the neural network models, so such a conclusion would be premature.</p>"},{"location":"machine-learning/nngp/#optimizing-the-infinitely-wide-neural-network","title":"Optimizing the infinitely wide neural network","text":"<p>There's still some pieces of the infinitely wide neural network that I think we need to get a practical handle over. In particular, how does the neural network performance, and the uncertainties, change as a function of:</p> <ul> <li>The number of nodes in a hidden layer?</li> <li>The depth of the neural network?</li> <li>The activation function that we use?</li> </ul> <p>That's what we're going to explore in this section of the notebook.</p> <p>It's always good to go into an inquiry with some expected range of answers. Here's mine, stated up front:</p> <p>Number of nodes: The number of nodes shouldn't matter. We're talking about expanding each layer to infinite width, after all.</p> <p>Depth: Depth might affect the structure of the corresponding NNGP kernel. However, I am not well-versed enough in the math to know exactly how.</p> <p>Activation function: I suspect the activation function will also affect the structure of the NNGP kernel. A hunch tells me it may have the weakest effect compared to depth, but I can't back that up with any theory. It's just a hunch.</p> <p>Let me first set up some utility functions below to help out with coding later.</p> <pre><code># Some utility functions to help out\n\nimport jax.numpy as np\nimport neural_tangents as nt\nfrom neural_tangents import stax\nfrom jax import jit\n\n\nclass NNGP:\n\"\"\"sklearn-compatible NNGP class.\"\"\"\n    def __init__(self, n_layers:int = 1, nonlinearity=stax.Relu()):\n        # Storing for information only\n        self.n_layers = n_layers\n        self.nonlinearity = nonlinearity\n\n        init_fn, apply_fn, kernel_fn = make_inf_nn(n_layers, 300, nonlinearity=nonlinearity)\n        self.init_fn = init_fn\n        self.apply_fn = apply_fn\n        self.kernel_fn = kernel_fn\n        self.predict_fn = None\n\n    def __repr__(self):\n        return f\"NNGP()\"\n\n    def fit(self, X, y):\n        if len(y.shape) == 1:\n            y = y.reshape(-1, 1)\n        self.predict_fn = nt.predict.gradient_descent_mse_ensemble(\n            self.kernel_fn, X, y, diag_reg=1e-4\n        )\n\n    def predict(self, X):\n        mean = self.predict_fn(x_test=X, get=\"nngp\", compute_cov=False)\n        return mean\n\n\ndef compute_mean_std(kernel_fn, x_train, y_train, x_test):\n\"\"\"Function to compute mean and std function.\"\"\"\n    predict_fn = nt.predict.gradient_descent_mse_ensemble(\n        kernel_fn, x_train, y_train, diag_reg=1e-4\n    )\n\n    nngp_mean, nngp_covariance = predict_fn(x_test=x_test, get=\"nngp\", compute_cov=True)\n\n    nngp_std = np.sqrt(np.diag(nngp_covariance))\n    return nngp_mean, nngp_std\n\n\ndef plot_results(SCOREFUNC, y_test, mean, std, ax, n_layers, n_hidden, FIG_ALPHA):\n\"\"\"Helper plotting function.\"\"\"\n    ax.errorbar(y_test.squeeze(), mean.squeeze(), yerr=std * 3, marker=\"o\", ls=\"\", alpha=FIG_ALPHA)\n    score = SCOREFUNC(mean.squeeze(), y_test.squeeze())\n    ax.set_title(f\"Score: {score:.2f}, {n_layers} layers,\\n{n_hidden} hidden nodes\")\n\n\ndef make_inf_nn(n_layers: int, n_hidden: int, nonlinearity=stax.Erf()):\n\"\"\"Convenience function to make infinitely wide NN of different architectures.\"\"\"\n    layers = []\n    for _ in range(n_layers):\n        layers.append(stax.Dense(n_hidden))\n        if nonlinearity:\n            layers.append(nonlinearity)\n    init_fn, apply_fn, kernel_fn = stax.serial(\n        *layers,\n        stax.Dense(1),\n    )\n    apply_fn = jit(apply_fn)\n    kernel_fn = jit(kernel_fn, static_argnums=(2,))\n    return init_fn, apply_fn, kernel_fn\n</code></pre>"},{"location":"machine-learning/nngp/#experiment-setup","title":"Experiment setup","text":"<p>For number of layers, we're going to explore 1, 10 and 20 layers.</p> <p>For number of nodes per hidden layer, we'll explore what happens with 1, 512, and 4096 nodes. (I mean, it's gonna go to infinity, so why would I try thousands of hidden nodes, right?)</p> <p>For non-linearities, we'll do:</p> <ul> <li>No non-linearities (i.e. simple linear projections)</li> <li>Erf non-linearity</li> <li>Relu non-linearity</li> <li>Gelu non-linearity</li> </ul> <p>The three non-linearities are available in neural tangents, so we're using them for convenience. At 4x3x3, we're already at 36 combinations, which I think is more than enough to see any patterns.</p>"},{"location":"machine-learning/nngp/#linear-projections-aka-no-non-linearities","title":"Linear projections (a.k.a. no non-linearities)","text":"<p>Let's start with linear projections and see what happens.</p> <pre><code>from itertools import product\n\nNUM_LAYERS = [1, 10, 20]\nNUM_HIDDEN = [1, 512, 4096]\n\nfig, axes = plt.subplots(nrows=3, ncols=3, figsize=(12, 12), sharex=True, sharey=True)\n\nfor i, (n_layers, n_hidden) in enumerate(product(NUM_LAYERS, NUM_HIDDEN)):\n    init_fn, apply_fn, kernel_fn = make_inf_nn(n_layers, n_hidden, nonlinearity=None)\n    mean, std = compute_mean_std(kernel_fn, x_train, y_train, x_test)\n\n    plot_results(SCOREFUNC, y_test, mean, std, axes.flatten()[i], n_layers, n_hidden, FIG_ALPHA)\n\nminval, maxval = get_x_eq_y(axes)\nfor ax in axes.flatten():\n    ax.plot([minval, maxval], [minval, maxval], color=\"black\", ls=\"--\")\n    sns.despine()\n\nfor ax in axes[:, 0]:\n    ax.set_ylabel(\"Predicted\")\n\nfor ax in axes[-1, :]:\n    ax.set_xlabel(\"Actual\")\n\nplt.tight_layout()\n</code></pre> <p></p> <p>Apparently, with no pure linear projections, we:</p> <ol> <li>Do much better with the performance as measured by posterior expectation values.</li> <li>See that layer depth and number of hidden nodes have zero effect on performance and uncertainty widths.</li> </ol>"},{"location":"machine-learning/nngp/#erf-nonlinearity","title":"Erf nonlinearity","text":"<p>Now let's see what happens when we use the Erf nonlinearity, which was the default activation function that the neural tangents demo notebooks use.</p> <pre><code>fig, axes = plt.subplots(nrows=3, ncols=3, figsize=(12, 12), sharex=True, sharey=True)\n\nfor i, (n_layers, n_hidden) in enumerate(product(NUM_LAYERS, NUM_HIDDEN)):\n    init_fn, apply_fn, kernel_fn = make_inf_nn(n_layers, n_hidden)\n    mean, std = compute_mean_std(kernel_fn, x_train, y_train, x_test)\n\n    plot_results(SCOREFUNC, y_test, mean, std, axes.flatten()[i], n_layers, n_hidden, FIG_ALPHA)\n\nminval, maxval = get_x_eq_y(axes)\nfor ax in axes.flatten():\n    ax.plot([minval, maxval], [minval, maxval], color=\"black\", ls=\"--\")\n    sns.despine()\n\nfor ax in axes[:, 0]:\n    ax.set_ylabel(\"Predicted\")\n\nfor ax in axes[-1, :]:\n    ax.set_xlabel(\"Actual\")\n\nplt.tight_layout()\n</code></pre> <p></p> <p>We see here that the Erf non-linearity gives us poorer performance when compared to having no non-linearities as the activation function.</p> <p>Additionally, there is a trend w.r.t. network depth. The deeper the network, the poorer the performance of the corresponding NNGP kernel. I suspect there may be some kind of overfitting that happens. How the overfitting happens, though, remains something of inquiry for me to learn. (My current hypothesis hinges on the hunch that deeper networks correspond to stacked GPs, though I would need further confirmation from someone better at math than I am to see if that is correct.)</p> <p>An intriguing thing I see as well is that as we add depth to the network, the resulting NNGP kernel yields smaller uncertainties. Anthropomorphically, we would say that the neural network is more confident of its predictions even as it becomes generally more wrong. We should call these networks Dunning-Kruger networks.</p>"},{"location":"machine-learning/nngp/#try-with-relu-non-linearity","title":"Try with Relu non-linearity","text":"<p>Let's now try a popular non-linearity, the Relu non-linearity.</p> <pre><code>fig, axes = plt.subplots(nrows=3, ncols=3, figsize=(12, 12), sharex=True, sharey=True)\n\nfor i, (n_layers, n_hidden) in enumerate(product(NUM_LAYERS, NUM_HIDDEN)):\n    init_fn, apply_fn, kernel_fn = make_inf_nn(n_layers, n_hidden, nonlinearity=stax.Relu())\n    mean, std = compute_mean_std(kernel_fn, x_train, y_train, x_test)\n\n    plot_results(SCOREFUNC, y_test, mean, std, axes.flatten()[i], n_layers, n_hidden, FIG_ALPHA)\n\nminval, maxval = get_x_eq_y(axes)\nfor ax in axes.flatten():\n    ax.plot([minval, maxval], [minval, maxval], color=\"black\", ls=\"--\")\n    sns.despine()\n\nfor ax in axes[:, 0]:\n    ax.set_ylabel(\"Predicted\")\n\nfor ax in axes[-1, :]:\n    ax.set_xlabel(\"Actual\")\n\nplt.tight_layout()\n</code></pre> <p></p> <p>A really cool result shows up here: we have better performance using the Relu activation function! The same trend w.r.t. number of hidden layers also holds: as we increase the number of hidden layers, the performance decreases, and the uncertainty decreases. The network becomes a Dunning-Kruger network.</p>"},{"location":"machine-learning/nngp/#try-with-gelu-nonlinearity","title":"Try with Gelu nonlinearity","text":"<pre><code>fig, axes = plt.subplots(nrows=3, ncols=3, figsize=(12, 12), sharex=True, sharey=True)\n\nfor i, (n_layers, n_hidden) in enumerate(product(NUM_LAYERS, NUM_HIDDEN)):\n    init_fn, apply_fn, kernel_fn = make_inf_nn(n_layers, n_hidden, nonlinearity=stax.Gelu())\n    mean, std = compute_mean_std(kernel_fn, x_train, y_train, x_test)\n\n    plot_results(SCOREFUNC, y_test, mean, std, axes.flatten()[i], n_layers, n_hidden, FIG_ALPHA)\n\nminval, maxval = get_x_eq_y(axes)\nfor ax in axes.flatten():\n    ax.plot([minval, maxval], [minval, maxval], color=\"black\", ls=\"--\")\n    sns.despine()\n\nfor ax in axes[:, 0]:\n    ax.set_ylabel(\"Predicted\")\n\nfor ax in axes[-1, :]:\n    ax.set_xlabel(\"Actual\")\n\nplt.tight_layout()\n</code></pre> <p>The Gelu network is a highly interesting one! As the depth increases, the correctness of the network, i.e. performance, doesn't go down, but the uncertainties do become smaller. Anthropomorphically, the network remains just as wrong about predictions, and becomes more confident of them. As usual, the number of hidden nodes affect neither the model's expected performance nor the uncertainties.</p>"},{"location":"machine-learning/nngp/#summary-of-trends-observed","title":"Summary of Trends observed","text":""},{"location":"machine-learning/nngp/#depth","title":"Depth","text":"<p>The observable trend here is that going with deeper nets that have non-identity activation functions leads to overfitting on the training set; shallower nets have the best performance. This holds in general, with the Gelu network being the exception of the activation functions tested here.</p>"},{"location":"machine-learning/nngp/#activation-functions","title":"Activation functions","text":"<p>Of the three tested inside the nt.stax library of nonlinearities, the Gelu activation function provided the best performance based on the posterior expectation, with the Relu activation function a close second.</p> <p>The effect of activation functions on model performance was dramatic, and this was something out of expectation based on my stated prior beliefs.</p> <p>What the effect of activation functions are on the width of the posterior uncertainties is something we will investigate next.</p>"},{"location":"machine-learning/nngp/#number-of-hidden-nodes","title":"Number of hidden nodes","text":"<p>From the get-go, I expected that the number of hidden nodes declared in the network constructor should not play a role with NNGP performance, as after all, we're taking the number of hidden nodes to the infinite limit. As the results show, whether we use 1, 2, or 3 hidden nodes in the hidden layer, the infinite-width performance is identical.</p>"},{"location":"machine-learning/nngp/#comparison-between-non-linearities-effects-on-uncertainties","title":"Comparison between non-linearities' effects on uncertainties","text":"<p>Having seen that neural networks begin to exhibit the Dunning-Kruger effect as they get more powerful (at least in the limit of infinite width), we will limit further probing of our model to just the single layer depth networks with a single hidden node. I'm playing with fire here, just to prove the point. Take that, overparameterized networks, we're going infinite now.</p> <p>To investigate how the nonlinearities affect uncertainty, we will plot the distribution of standard deviations of the test set predictions. It will be an ECDF plot, so that we can cram everything into one plot and compare directly. (Histograms are so 1950s!)</p> <pre><code>from neural_tangents import stax\nnonlinearities = {\n    \"none\": None,\n    \"ReLu\": stax.Relu(),\n    \"GeLu\": stax.Gelu(),\n    \"Erf\": stax.Erf(),\n}\n\nfig, ax = plt.subplots(nrows=1, ncols=1, figsize=(4, 4))\n\n\ndef ecdf(data):\n    x, y = np.sort(data), np.arange(1, len(data)+1) / len(data)\n    return x, y\n\n\nfor i, (name, nonlinearity) in enumerate(nonlinearities.items()):\n    init_fn, apply_fn, kernel_fn = make_inf_nn(n_layers=1, n_hidden=1, nonlinearity=nonlinearity)\n    mean, std = compute_mean_std(kernel_fn, x_train, y_train, x_test)\n    x, y = ecdf(std)\n    ax.plot(x, y, label=name)\nax.set_xlabel(\"Std. Dev\")\nax.set_ylabel(\"Cumulative Fraction\")\nax.set_title(\"Distribution\\nof uncertainties\")\nax.legend()\nsns.despine()\n</code></pre> <p></p> <p>What do we see here? Well, to begin with, infinitely wide neural networks with no nonlinearities end up being the most uncertain about their predictions. The ReLu and GeLu network end up showing practically identical distributions of uncertainties in test set predictions. Meanwhile, the Erf network, which was the worst of all of the networks tested, showed a tighter distribution of uncertainties, though roughly at the same scale as the other networks.</p>"},{"location":"machine-learning/nngp/#how-do-we-measure-the-performance-of-the-uncertainties","title":"How do we measure the \"performance\" of the uncertainties?","text":"<p>Well, one way to do so is to ask what fraction of the uncertainties' range capture the correct value. We have the standard deviation of predictions from our GP; 3x the standard deviation gives us about 96% of the posterior probability. If the mean \u00b1 3x standard deviation encapsulates the true value, then we know our posterior standard deviation was highly likely to capture the true value, even if the expected value was off.</p> <pre><code>def evaluate_uncertainty(mean: float, std: float, y_test: float):\n    lt = y_test &lt; mean + std * 3\n    gt = y_test &gt; mean - std * 3\n    return np.logical_and(lt, gt)\n</code></pre> <pre><code>from jax import vmap\n\nresults = dict()\nfor i, (name, nonlinearity) in enumerate(nonlinearities.items()):\n    init_fn, apply_fn, kernel_fn = make_inf_nn(n_layers=1, n_hidden=1, nonlinearity=nonlinearity)\n    mean, std = compute_mean_std(kernel_fn, x_train, y_train, x_test)\n    result = vmap(evaluate_uncertainty)(mean, std, y_test)\n    results[name] = result.squeeze()\n</code></pre> <pre><code>import pandas as pd\n\npd.DataFrame(results).mean().plot(kind=\"bar\", rot=45)\nax = plt.gca()\nax.set_xlabel(\"Activation\")\nax.set_ylabel(\"P(correct)\")\nax.set_ylim(0, 1)\nsns.despine()\n</code></pre> <p></p> <p>As we can see above, the ReLu function has the best performing uncertainties, with 70% of the posteriors harbouring the correct value. One might say that the ReLu infinite networks are best calibrated for prediction purposes.</p> <p>You'd think that the linear projection networks would have well-calibrated uncertainties given their widths, and that's certainly something I thought would happen, but that turned out not to be the case.</p>"},{"location":"machine-learning/nngp/#using-the-uncertainties-for-decision-making","title":"Using the uncertainties for decision-making","text":"<p>In a chemistry setting, we're usually concerned with prioritization tasks. A chemist's time for testing molecules is usually finite, so we need some help rank-ordering items in a principled fashion. Well, turns out when you have Gaussian uncertainties, you can do this pretty easily.</p> <p>If you know anything about Bayesian optimization, we need to have an acquisition function that helps us rank-order our predictions. The acquisition function that gets chosen is going to depend on the task at hand. Because we're dealing with solubility as a task, and in medicinal chemistry high solubility is a desirable property, we can rank order our test samples according to the probability that they are above some threshold value. This is effectively evaluating the survival function, or <code>1 - CDF(threshold)</code>, of a Gaussian.</p> <pre><code>test_indices = indices[n_train:]\n</code></pre> <pre><code>from jax.scipy.stats import norm\ndef prob_above_threshold(mean: float, std: float, threshold: float):\n    return 1 - norm.cdf(x=threshold, loc=mean, scale=std)\n</code></pre> <pre><code>from functools import partial\nprob_thresh = dict()\nfor i, (name, nonlinearity) in enumerate(nonlinearities.items()):\n    init_fn, apply_fn, kernel_fn = make_inf_nn(n_layers=1, n_hidden=1, nonlinearity=nonlinearity)\n    mean, std = compute_mean_std(kernel_fn, x_train, y_train, x_test)\n    result = vmap(partial(prob_above_threshold, threshold=-0.1))(mean, std)\n    prob_thresh[name] = result.squeeze()\n</code></pre> <p>Let's now query for molecules in the test set that are predicted to have greater than 50% probability of having solubility greater than 0. This would be akin to helping a chemist prioritize the selection of molecules where we'd be happy to bet at 1:1 odds of having a desirable property, given the model's predictions.</p> <p>Remember that the uncertainties dataframe is indexed differently, so we'll need to do some fancy indexing to get back to the indexing that is used in the original dataframe.</p> <pre><code>prob_thresh_df = pd.DataFrame(prob_thresh).sort_values(\"ReLu\", ascending=False).query(\"ReLu &gt; 0.5\")\nprob_thresh_df\n</code></pre> none ReLu GeLu Erf 133 1.000000 1.000000 1.000000 1.000000 158 1.000000 0.991346 0.999885 0.999994 78 1.000000 0.980182 0.920877 1.000000 121 0.999974 0.538650 0.629214 1.000000 39 1.000000 0.536250 0.007561 0.206573 <pre><code>test_samples.loc[test_indices[prob_thresh_df.index.values]]\n</code></pre> Compound ID ESOL predicted log solubility in mols per litre Minimum Degree Molecular Weight Number of H-Bond Donors Number of Rings Number of Rotatable Bonds Polar Surface Area measured log solubility in mols per litre smiles mol 679 Glycerol 0.688 1 92.094 3 0 2 60.69 1.120 OCC(O)CO &lt;rdkit.Chem.rdchem.Mol object at 0x7febddea1bd0&gt; 785 Urea 0.832 1 60.056 2 0 0 69.11 0.960 NC(=O)N &lt;rdkit.Chem.rdchem.Mol object at 0x7febddea4ac0&gt; 298 Raffinose 0.496 1 504.438 11 3 8 268.68 -0.410 OCC1OC(CO)(OC2OC(COC3OC(CO)C(O)C(O)C3O)C(O)C(O... &lt;rdkit.Chem.rdchem.Mol object at 0x7febdde97370&gt; 145 2-hydroxypteridine -1.404 1 148.125 1 2 0 71.79 -1.947 Oc2ncc1nccnc1n2 &lt;rdkit.Chem.rdchem.Mol object at 0x7febdde92ff0&gt; 983 Ethanol 0.020 1 46.069 1 0 0 20.23 1.100 CCO &lt;rdkit.Chem.rdchem.Mol object at 0x7febddeaa1f0&gt; <p>Hey, not bad! From what I know about chemistry, glycerol is highly soluble in water, and is used by microbiologists to protect frozen bacteria cells from being cracked open upon freeze/thawing cycles. Urea is definitely a molecule that is soluble in water; we make it all the time in our bodies as a waste product, and is carried out of our body in our urine. Raffinose is a sugar, and so with all of the oxygens and hydrogens in there, it'll likely be dissolvable in water. 2-hydroxy-something always sounds like it'll be soluble in water, and Ethanol... well, we know how that one goes :).</p>"},{"location":"machine-learning/nngp/#conclusions","title":"Conclusions","text":"<p>Overall, this deep dive into infinite-width neural networks tells me that we deep learners have a superb tool at our disposal for modelling complex relationships that don't have an obvious input-output structure. We can write a single layer neural network, expand it out to its infinite width form easily to get a Neural Net Gaussian Process kernel that, when conditioned on training data, could give us good test performance. (No guarantees though, we'd have to benchmark.) For expensive data collection problems like those that we find in the life sciences, this is a superb modelling tool on many levels.</p> <p>Firstly, Gaussian processes are highly flexible and inherently Bayesian modelling tools, but proper kernel design is difficult; neural net design is an easier alternative language to work with to generate GPs for our modelling problems. Infinitely wide neural networks give us the best of both worlds.</p> <p>Secondly, because we get a highly flexible function learner, complex functions are easily modelled without needing to do much work with hyperparameter tuning. Down with the Sisyphean tasks of twiddling with neural networks! (OK, maybe I'm taking the rant a bit too far.) Single layer infinitely wide neural networks did a practically great job on solubility with standard molecular descriptors, were as easy to wield as the cheminformatics-favourite Random Forest model, and had performance metrics effectively on par with Random Forests. We may be able to squeeze out more performance by adopting alternative model architectures that have different inductive biases, such as graph neural networks, but I have to admit, it was pretty easy to get to the performance levels we observed here.</p> <p>Thirdly, because NNGPs are inherently Bayesian, the suite of tools from Bayesian decision science are at our disposal. Prioritizing, ranking, selecting... once you have the posterior likelihood density at hand, you have the principle by which these activities can be done. No more premature thresholds; delay all binarization until the end, when we have the posterior likelihood calculated. No fancy methods needed either; \"likelihood is all you need\".</p>"},{"location":"machine-learning/nngp/#lessons-learned","title":"Lessons Learned","text":"<p>From this exercise, it seems evident to me that the nonlinearity is the biggest thing that affects the kernel, which in turn affects the expected performance (as given by the posterior means) and uncertainty calibration (as measured by the posterior standard deviations). If you remember where my headspace was when we first started, I was completely wrong w.r.t. this point; the NNGP kernel is highly influenced by the choice of activation function. Where I was correct was that the number of hidden nodes per layer didn't matter one bit for the performance of the network, because we are, after all, going to infinity ~~and beyond~~.</p> <p>Knowing that we can get pretty decent expected predictive performance and somewhat nicely calibrated uncertainties without needing to fiddle with layer width, number of layers, number of training epochs, learning rate, and the myriad of other hyperparameters that we might be interested in... is actually kind of a nice result. With the NNGP kernel, we basically eliminate a ton of hyperparameters in training neural nets, and can focus primarily on the non-linear activation function to design the best NNGP kernel. Hyperparameter tuning is the worst Sisyphean task that will bore the crap out of any data scientist.</p> <p>Hence, from the perspective of workflow, using infinitely-wide neural networks is an attractive proposition for modelling problems that do not have an obvious input-output relationship. At the end of the day, by using infinitely wide neural networks, fitting a Gaussian Process with a particular kernel to the problem at hand, and this cuts down on the number of hyperparameters that we have to fiddle with. Pragmatically, I have a hunch that we can mostly stick with single layer NNGPs and leverage the uncertainty instead.</p> <p>Finally, I have to comment on the lack of compute intensity needed to wield infinitely wide neural networks. Everything I did in this notebook was first prototyped on an M1 MacBook Air, done in emulation mode. No GPUs were used. None! The entire notebook runs from top to bottom in under two minutes. Take that, Transformers.</p> <pre><code>%watermark\n</code></pre> <pre><code>Last updated: 2022-12-23T14:36:34.109063-05:00\n\nPython implementation: CPython\nPython version       : 3.10.8\nIPython version      : 8.7.0\n\nCompiler    : GCC 10.4.0\nOS          : Linux\nRelease     : 5.15.0-53-generic\nMachine     : x86_64\nProcessor   : x86_64\nCPU cores   : 8\nArchitecture: 64bit\n</code></pre> <pre><code>%watermark --iversions\n</code></pre> <pre><code>pandas         : 1.5.2\nneural_tangents: 0.6.1\nmatplotlib     : 3.6.2\njax            : 0.4.1\njanitor        : 0.23.1\nseaborn        : 0.12.1\n</code></pre>"},{"location":"machine-learning/reimplementing-models/","title":"Reimplementing and Testing Deep Learning Models","text":"<p>At work, most deep learners I have encountered have a tendency to take deep learning models and treat them as black boxes that we should be able to wrangle. While I see this as a pragmatic first step to testing and proving out the value of a newly-developed deep learning model, I think that stopping there and not investing the time into understanding the nitty-gritty of the model leaves us in a poor position to know that model's (1) applicability domain (i.e. where the model should be used), (2) computational and statistical performance limitations, and (3) possible engineering barriers to getting the model performant in a \"production\" setting.</p> <p>As such, with deep learning models, I'm actually a fan of investing the time to re-implement the model in a tensor framework that we all know and love, NumPy (and by extension, JAX).</p>"},{"location":"machine-learning/reimplementing-models/#benefits-of-re-implementing-deep-learning-models","title":"Benefits of re-implementing deep learning models","text":"<p>Doing a model re-implementation from a deep learning framework into NumPy code actually has some benefits for the time being invested.</p>"},{"location":"machine-learning/reimplementing-models/#developing-familiarity-with-deep-learning-frameworks","title":"Developing familiarity with deep learning frameworks","text":"<p>Firstly, doing so forces us to know the translation/mapping from deep learning tensor libraries into NumPy. One of the issues I have had with deep learning libraries (PyTorch and Tensorflow being the main culprits here) is that their API copies something like 90% of NumPy API without making easily accessible the design considerations discussed when deciding to deviate. (By contrast, CuPy has an explicit API policy that is well-documented and front-and-center on the docs, while JAX strives to replicate the NumPy API.)</p> <p>My gripes with tensor library APIs aside, though, translating a model by hand from one API to another forces growth in familiarity with both APIs, much as translating between two languages forces growth in familiarity with both languages.</p>"},{"location":"machine-learning/reimplementing-models/#developing-a-mechanistic-understanding-of-the-model","title":"Developing a mechanistic understanding of the model","text":"<p>It is one thing to describe a deep neural network as being \"like the brain cell connections\". It is another thing to know that the math operations underneath the hood are nothing more than dot products (or tensor operations, more generally). Re-implementing a deep learning model requires combing over every line of code, which forces us to identify each math operation used. No longer can we hide behind an unhelpfully vague abstraction.</p>"},{"location":"machine-learning/reimplementing-models/#developing-an-ability-to-test-and-sanity-check-the-model","title":"Developing an ability to test and sanity-check the model","text":"<p>If we follow the workflow (that I will describe below) for reimplementing the model, (or as the reader should now see, translating the model between APIs) we will develop confidence in the correctness of the model. This is because the workflow I am going to propose involves proper basic software engineering workflow: writing documentation for the model, testing it, and modularizing it into its logical components. Doing each of these requires a mechanistic understanding of how the model works, and hence forms a useful way of building intuition behind the model as well as correctness of the model.</p>"},{"location":"machine-learning/reimplementing-models/#reimplementing-models-is-not-a-waste-of-time","title":"Reimplementing models is not a waste of time","text":"<p>By contrast, it is a highly beneficial practice for gaining a deeper understanding into the inner workings of a deep neural network. The only price we pay is in person-hours, yet under the assumption that the model is of strong commercial interest, that price can only be considered an investment, and not a waste.</p>"},{"location":"machine-learning/reimplementing-models/#a-proposed-workflow-for-reimplementing-deep-learning-models","title":"A proposed workflow for reimplementing deep learning models","text":"<p>I will now propose a workflow for re-implementing deep learning models.</p>"},{"location":"machine-learning/reimplementing-models/#identify-a-coding-partner","title":"Identify a coding partner","text":"<p>Pair programming is a productive way of teaching and learning. Hence, I would start by identifying a coding partner who has the requisite skillset and shared incentive to go deep on the model.</p> <p>Doing so helps a few ways.</p> <p>Firstly, we have real-time peer review on our code, making it easier for us to catch mistakes that show up.</p> <p>Secondly, working together at the same time means that both myself and my colleague will learn something about the neural network that we are re-implementing.</p>"},{"location":"machine-learning/reimplementing-models/#pick-out-the-forward-step-of-the-model","title":"Pick out the \"forward\" step of the model","text":"<p>The \"forward\" pass of the model is where the structure of the model is defined: basically the mathematical operations that transform the input data into the output observations.</p> <p>A few keywords to look out for are the <code>forward()</code> and  <code>__call__()</code> class methods.</p> <pre><code>class MyModel(nn.Model):\n    # ...\n    def forward(self, X):\n        # Implementation of model happens here.\n        something = ...\n        return something\n</code></pre> <p>For models that involve an autoencoder, somewhat more seasoned programmers might create a class method called <code>encoder()</code> and <code>decoder()</code>, which themselves reference another model that would have a <code>forward()</code> or <code>__call__()</code> defined.</p> <pre><code>class AutoEncoder(nn.Model):\n    # ...\n    def forward(self, X):\n        something = self.encoder(X)\n        output = self.decoder(something)\n        return output\n</code></pre> <p>Re-implementing the <code>forward()</code> part of the model is usually a good way of building a map of the equations that are being used to transform the input data into the output data.</p>"},{"location":"machine-learning/reimplementing-models/#inspect-the-shapes-of-the-weights","title":"Inspect the shapes of the weights","text":"<p>While the equations give the model structure, the weights and biases, or the parameters, are the part of the model that are optimized. (In Bayesian statistics, we would usually presume a model structure, i.e. the set of equations used alongside the priors, and fit the model parameters.)</p> <p>Because much of deep learning hinges on linear algebra, and because most of the transformations that happen involve transforming the input space into the output space, getting the shapes of the parameters is very important.</p> <p>In a re-implementation exercise with my intern, where we re-implemented a specially designed recurrent neural network layer in JAX, we did a manual sanity check through our implementation to identify what the shapes would need to be for the inputs and outputs. After that, we encoded that manual test into an automatic test. Later on, after we built another test that integrated which paradoxically failed on shapes, we eventually uncovered that we were indexing into the wrong dimensions in our implementation. This led to us (1) fixing the bug, (2) writing a more comprehensive documentation and test suite, and (3) writing better documentations for the semantic meaning of each tensor axis.</p>"},{"location":"machine-learning/reimplementing-models/#write-tests-for-the-neural-network-components","title":"Write tests for the neural network components","text":"<p>Once we have the neural network model and its components implemented, writing tests for those components is a wonderful way of making sure that (1) the implementation is correct, to the best of our knowledge, and that (2) we can catch when the implementation might have been broken inadvertently.</p> <p>The shape test (as described above) is one way of doing this.</p> <pre><code>def test_layer_shapes():\n    weights = np.random.normal(size=(input_dims, output_dims))\n    data = np.random.normal(size=(batch_size, input_dims))\n    output = nn_layer(weights, data)\n    assert output.shape[1] == output_dims\n</code></pre> <p>If there are special elementwise transforms performed on the data, such as a ReLU or exponential transform, we can test that the numerical properties of the output are correct:</p> <pre><code>def test_layer_shapes():\n    weights = np.random.normal(size=(input_dims, output_dims))\n    data = np.random.normal(size=(batch_size, input_dims))\n\n    output = nn_layer(weights, data, nonlinearity=\"relu\")\n    assert np.all(output &gt;= 0)\n</code></pre>"},{"location":"machine-learning/reimplementing-models/#write-tests-for-the-entire-training-loop","title":"Write tests for the entire training loop","text":"<p>Once the model has been re-implemented in its entirety, prepare a small set of training data, and pass it through the model, and attempt to train it for a few epochs.</p> <p>If the model, as implemented, is doing what we think it should be, then after a dozen epochs or so, the training loss should go down. We can then test that the training loss at the end is less than the training loss at the beginning. If the loss does go down, it's necessary but not sufficient for knowing that the model is implemented correctly. However, if the loss does not go down, then we will definitely know that a problem exists somewhere in the code, and can begin to debug.</p> <p>An example with pseudocode below might look like the following:</p> <pre><code>from data import dummy_graph_data\nfrom model import gnn_model\nfrom params import make_gnn_params\nfrom losses import mse_loss\nfrom jax import grad\nfrom jax.example_libraries.optimizers import adam\n\ndef test_gnn_training():\n    # Prepare training data\n    x, y = dummy_graph_data(*args, **kwargs)\n    params = make_gnn_params(*args, **kwargs)\n\n    dloss = grad(mse_loss)\n    init, update, get_params = adam(step_size=0.005)\n    start_loss  = mse_loss(params, model, x, y)\n\n    state = init(params)\n    for i in range(10):\n        g = dloss(params, model, x, y)\n\n        state = update(i, g, state)\n        params = get_params(state)\n\n    end_loss = mse_loss(params, model, x, y)\n\n    assert end_loss &lt; start_loss\n</code></pre> <p>A side benefit of this is that if you commit to only judiciously changing the tests, you will end up with a stable and copy/paste-able training loop that you know you can trust on new learning tasks, and hence only need to worry about swapping out the data.</p>"},{"location":"machine-learning/reimplementing-models/#build-little-tools-for-yourself-that-automate-repetitive-boring-things","title":"Build little tools for yourself that automate repetitive (boring) things","text":"<p>You may notice in the above integration test, we wrote a lot of other functions that make testing much easier, such as dummy data generators, and parameter initializers.</p> <p>These are tools that make composing parts of the entire training process modular and easy to compose. I strongly recommend writing these things, and also backing them with more tests (since we will end up relying on them anyways).</p>"},{"location":"machine-learning/reimplementing-models/#now-run-your-deep-learning-experiments","title":"Now run your deep learning experiments","text":"<p>Once we have the model re-implemented and tested, the groundwork is present for us to conduct extensive experiments with the confidence that we know how to catch bugs in the model in a fairly automated fashion.</p>"},{"location":"machine-learning/reimplementing-models/#concluding-words","title":"Concluding words","text":"<p>Re-implementing deep learning models can be a very fun and rewarding exercise, because it serves as an excellent tool to check our understanding of the models that we work with.</p> <p>Without the right safeguards in place, though, it can also very quickly metamorphose into a nightmare rabbithole of debugging. Placing basic safeguards in place when re-implementing models helps us avoid as many of these rabbitholes as possible.</p>"},{"location":"miscellaneous/code-style-tools/","title":"Tools to help you write consistent Python code","text":"<p>As a data scientist, you might be feeling the need to level-up your code-writing skills. After all, if your hacked together script becomes the basis of production systems, imagine the headache :). Moreover, as time goes by, it seems to me that for data scientists to uncover the high risk, but nonetheless potentially high value and high impact outlets, nothing beats having software skills to flexibly express the custom logic that's needed. Hadley Wickham's sentiment that one can't do data science in a GUI seems to ring true, though I'd probably refine it by saying that one can't do highest value data science without programming skills!</p> <p>Mostly because of the community of <code>pyjanitor</code> developers, who encounter a smattering of tooling that I might not have, I have had the privilege of learning from them a collection of tooling that I think can really help with code style. Especially if you transition your code from a Jupyter notebook to a <code>.py</code> source file! Equipped with the right tools, you can write consistent code without needing to memorize all of the rules. And that idea of externalizing the rules to a program is what these tools are all about!</p>"},{"location":"miscellaneous/code-style-tools/#tldr-what-tools-are-there","title":"tl;dr: What tools are there?","text":"<p>For the impatient, here's the summary:</p>"},{"location":"miscellaneous/code-style-tools/#code-formatters","title":"Code formatters","text":"<p>i.e. those that explicitly change your code:</p> <ul> <li>Black</li> <li>isort</li> </ul>"},{"location":"miscellaneous/code-style-tools/#code-linters","title":"Code linters","text":"<p>i.e. those that don\u2019t change your code, but flag errors:</p> <ul> <li>Flake8</li> <li>Interrogate</li> <li>Darglint</li> <li>pylint</li> <li>mypy</li> </ul> <p>Some of these come with <code>pre-commit hooks</code>, which I strongly suggest you use!</p>"},{"location":"miscellaneous/code-style-tools/#vscode-tooling","title":"VSCode tooling","text":"<p>VSCode tooling for those who are used to VSCode as your development environment:</p> <ul> <li>Python extension</li> <li>pylance: this is superbly fast. hats off, Microsoft.</li> </ul>"},{"location":"miscellaneous/code-style-tools/#external-ci-services","title":"External CI services","text":"<p>Tools that run as an external service for your continuous integration pipeline:</p> <ul> <li>deepsource</li> </ul>"},{"location":"miscellaneous/code-style-tools/#thats-a-lot-of-tools-how-do-i-interact-them","title":"That\u2019s a lot of tools; how do I interact them?","text":"<p>A full-fledged tutorial is probably not the realm of this tiny blog post, but I will begin by giving you the overview of how you interact with them in general. That should give you enough patterns to work with when you look at their docs.</p>"},{"location":"miscellaneous/code-style-tools/#at-the-cli","title":"At the CLI","text":"<p>The code formatters and linters are runnable at the CLI. You can consider this to be the lowest-level/primitive way of using them. For example, with <code>black</code>, you would call on it at the command line at the root of your project repository:</p> <pre><code>black .\n</code></pre> <p>What pops out is Black telling you what files it formatted, or error messages telling you which files it couldn\u2019t format.</p> <p>Likewise for <code>flake8</code>:</p> <pre><code>flake8 .\n</code></pre> <p>What pops out is an entire collection of \u201cerror codes\u201d, which specify one and only one error that it caught. The error messages will tell you what\u2019s wrong, so correcting the error is a matter of doing the opposite. For example, <code>flake8</code> might tell me that a line is too long at 89 characters, which is greater than the 79 that it is configured for, and so I would have to split up that line, or if it\u2019s a string then remove unnecessary words, to correct the error.</p>"},{"location":"miscellaneous/code-style-tools/#configuration","title":"Configuration","text":"<p>Sometimes, two tools might come with defaults that conflict with one another. <code>flake8</code> and <code>black</code> are two; <code>black</code>, being opinionated, will format lines to fit in 88 characters, but <code>flake8</code> will by default look for 79 characters. (I\u2019m in favour of 79, because it allows me to look at my code in its entirety side-by-side on a 15\u201d MacBook Pro.)</p> <p>Each tool comes with its own configuration documentation page. The Python community has started gravitating towards a config file called <code>pyproject.toml</code>, which is intended for Python language tools. If <code>pyproject.toml</code> is supported by a tool, then you should be able to find a page in the documentation. A well-written docs page will give you a copy/paste-able config file with extensive comments that you can read to know what\u2019s going on. (Black itself leads the charge: its page on the <code>pyproject.toml</code> file is very well-documented!)</p> <p>I think that the community-wide standardization is a great thing! The way this has happened is a bit slow, but at least the tool authors don\u2019t make it onerous to migrate from existing config files. Fits all of the right patterns: make doing the right thing easy!</p>"},{"location":"miscellaneous/code-style-tools/#inside-vscode","title":"Inside VSCode","text":"<p>If you use VSCode as your day-to-day integrated development environment (or IDE), there are two extensions that are indispensable for Python developers: the official Python extension and Pylance. The official Python extension adds support for:</p> <p>IntelliSense, linting, debugging, code navigation, code formatting, Jupyter notebook support, refactoring, variable explorer, test explorer, snippets, and more!</p> <p>Pylance, on the other hand, is:</p> <p>powered by Pyright, Microsoft's static type checking tool. Using Pyright, Pylance has the ability to supercharge your Python IntelliSense experience with rich type information, helping you write better code faster.</p> <p>With code linting enabled in VSCode, you get the benefits of errors showing up as you type. An example from the <code>pyjanitor</code> library, which contains some holdover code style issues from before the time I was proficient in these tools, can be seen in the screenshot below.</p> <p></p> <p>Underneath the hood, for code style checking, both extensions call out to <code>flake8</code>, <code>black</code>, <code>mypy</code> and other tools to perform code checking, and this is configurable in VSCode itself.</p>"},{"location":"miscellaneous/code-style-tools/#inside-your-ci-system","title":"Inside your CI system","text":"<p>If you build a project and have a continuous integration system for it, you can continuously check your code using the CLI tools. Concpetually, the steps you'd follow are:</p> <ol> <li>Install the tools inside the CI environment at build time.</li> <li>Place in the right configuration files in your project root.</li> <li>Execute the tools in the testing section of your CI build.</li> </ol> <p>The <code>pyjanitor</code> Azure pipelines CI has a great example of this in action. I'd recommend putting these early on, as waiting for test to pass before catching these slightly more trivial problems can make for a slightly irritated developer \ud83d\ude05.</p> <p>Because we're talking about CI systems, we might as well also talk about the external CI service highlighted here, DeepSource. It's a commercial service that is free for open source projects, and in there I've gotten a few reminders on certain anti-patterns. I like what they're doing; the team does have to make a living, so I'm not opposed to the commercialization of their tool, and in fact I'm quite appreciative of the fact that they've made it freely accessible for OSS projects.</p>"},{"location":"miscellaneous/code-style-tools/#what-about-inside-jupyter-notebooks","title":"What about inside Jupyter notebooks?","text":"<p>Most of us who are data scientists spend a majority of our time inside Jupyter notebooks. Are there tools available for us to help us write good code inside notebooks?</p> <p>Turns out, there's quite a few projects that try to do this! Given the breadth of people who know how to use Python to do things, it never seems far-fetched to think that someone else who has a need to scratch has already figured this out.</p> <p>Through digging around on the internet, I found at least the following tools that could be promising:</p> <ul> <li><code>black-nb</code></li> <li><code>nb_black</code></li> <li><code>jupyter-black</code></li> </ul> <p>That said, I usually adhere to the following principle with notebooks: that they are intended to be both read and executed, meaning if you have long walls of code inside your notebook, they should be refactored out into a custom package to keep the storytelling logic of your notebook flowing nicely. That is when using an IDE can help you. If your intent with <code>black</code>-ifying notebooks is to avoid refactoring your code, then you'll be disappointed - <code>black</code> has a tendency to add lines to code, such as hard-coded lists that are unavoidably long. <code>black</code>-ifying your notebook code may thus have unintended consequences for readability!</p> <p>Instead, do the right thing: Refactor your code!! To learn how to break out custom code, check out this very well-written blog post!</p>"},{"location":"miscellaneous/code-style-tools/#help-me-take-action-what-should-i-do-next","title":"Help me take action! What should I do next?","text":"<p>Here's the next thing you can do: add these tools into your <code>environment.yml</code> (conda users) or <code>requirements.txt</code> (venv/pip-only users) files! Then, update your virtual environment. Voil\u00e0! You now have these tools in your environment, and can start checking your code style!</p> <p>(Not using environments? I won't judge you \ud83d\ude05, but I would strongly encourage that you do so! Here's some tips to help.)</p>"},{"location":"miscellaneous/code-style-tools/#conclusions","title":"Conclusions","text":"<p>Hopefully this short essay has helped you get familiar with some of the tooling to help you write consistently-styled code that is also readable by a large swathe of the Python-writing universe of programmers. Writing code in a consistent style is important. That style usually involves rules; having tools to externalize that style, such you don't have to worry about remembering it, helps us conform to that style more easily. Have fun writing clean code!</p>"},{"location":"miscellaneous/dashboarding-landscape/","title":"A Review of the Python Data Science Dashboarding Landscape in 2019","text":"Note <p>There is an updated essay by Sophia Yang that I would highly recommend checking out! Please find it here.</p>"},{"location":"miscellaneous/dashboarding-landscape/#introduction","title":"Introduction","text":"<p>As Pythonista data scientists, we are spoiled for choice when it comes to developing front-ends for our data apps. We used to have to fiddle with HTML in Flask (or Plotly's Dash), but now, there are tools in which \"someone wrote the HTML/JS so I didn't have to\".</p> <p>Let me give a quick tour of the landscape of tools as I've experienced it in 2019.</p>"},{"location":"miscellaneous/dashboarding-landscape/#beginnings-voila","title":"Beginnings: Voila","text":"<p>Previously, I had test-driven Voila. The key advantage I saw back then was that in my workflow, once I had the makings of a UI present in the Jupyter notebook, and just needed a way to serve it up independent of having my end-users run a Jupyter server, then Voila helped solve that use case. By taking advantage of existing the <code>ipywidgets</code> ecosystem and adding on a way to run and serve the HTML output of a notebook, Voila solved that part of the dashboarding story quite nicely. In many respects, I regard Voila as the first proper dashboarding tool for Pythonistas.</p> <p>That said, development in a Jupyter notebook didn't necessarily foster best practices (such as refactoring and testing code). When my first project at work ended, and I didn't have a need for further dashboarding, I didn't touch Voila for a long time.</p>"},{"location":"miscellaneous/dashboarding-landscape/#another-player-panel","title":"Another player: Panel","text":"<p>Later, Panel showed up. Panel's development model allowed a more modular app setup, including importing of plotting functions defined inside <code>.py</code> files that returned individual plots. Panel also allowed me to prototype in a notebook and see the output live before moving the dashboard code into a source <code>.py</code> file.</p> <p>At work, we based a one-stop shop dashboard for a project on Panel, and in my personal life, I also built a minimal panel app that I also deployed to Heroku. Panel was definitely developed targeting notebook and source file use cases in mind, and this shows through in its source development model.</p> <p>That said, panel apps could be slow to load, and without having a \"spinner\" solution in place (i.e. something to show the user that the app is \"doing something\" in the background), it sometimes made apps feel slow even though the slowness was not Panel's fault really. (My colleagues and I pulled out all the tricks in our bag to speed things up.)</p> <p>Additionally, any errors that show up don't get surfaced to the app's UI, where developer eyeballs are on - instead, they get buried in the browser's JavaScript console or in the Python terminal where the app is being served. When deployed, this makes it difficult to see where errors show up and debug errors.</p>"},{"location":"miscellaneous/dashboarding-landscape/#enter-streamlit","title":"Enter Streamlit","text":"<p>Now, Streamlit comes along, and some of its initial demos are pretty rad. In order to test-drive it, I put together this little tutorial on the Beta probability distribution for my colleagues.</p> <p>Streamlit definitely solves some of the pain points that I've observed with Panel and Voila.</p> <p>The most important one that I see is that errors are captured by Streamlit and bubbled up to the UI, where our eyeballs are going to be when developing the app. For me, this is a very sensible decision to make, for two reasons:</p> <p>Firstly, it makes debugging interactions that much easier. Instead of needing to have two interfaces open, the error message shows up right where the interaction fails, in the same browser window as the UI elements.</p> <p>Secondly, it makes it possible for us to use the error messages as a UI \"hack\" to inform users where their inputs (e.g. free text) might be invalid, thereby giving them informative error messages. (Try it out in the Beta distribution app: it'll give you an error message right below if you try to type something that cant be converted into a float!)</p> <p>The other key thing that Streamlit provides as a UI nice-ity is the ability to signal to end-users that a computation is happening. Streamlit does this in three ways, two of which always come for free. Firstly, if something is \"running\", then in the top-right hand corner of the page, the \"Running\" spinner will animate. Secondly, anything that is re-rendering will automatically be greyed out. Finally, we can use a special context manager to provide a custom message on the front-end:</p> <pre><code>import streamlit as st\nwith st.spinner(\"Message goes here...\"):\n    # stuff happens\n</code></pre> <p>So all-in-all, Streamlit seems to have a solution of some kind for the friction points that I have observed with Panel and Voila.</p> <p>Besides that, Streamlit, I think, uses a procedural paradigm, rather than a callback paradigm, for app construction. We just have to think of the app as a linear sequence of actions that happen from top to bottom. State is never really an issue, because every code change and interaction re-runs the source file from top to bottom, from scratch. When building quick apps, this paradigm really simplifies things compared to a callback-based paradigm.</p> <p>Finally, Streamlit also provides a convenient way to add text to the UI by automatically parsing as Markdown any raw strings unassigned to a variable in a <code>.py</code> file and rendering them as HTML. This opens the door to treating a <code>.py</code> file as a literate programming document, hosted by a Python-based server in the backend. It'd be useful especially in teaching scenarios. (With <code>pyiodide</code> bringing the PyData stack to the browser, I can't wait to see standalone <code>.py</code> files rendered to the DOM!)</p> <p>Now, this isn't to say that Streamlit is problem-free. There are still rough edges, the most glaring (as of today) in the current release is the inability to upload a file and operate on it. This has been fixed in a recent pull request, so I'm expecting this should show up in a new release any time soon.</p> <p>The other not-so-big-problem that I see with Streamlit at the moment is the procedural paradigm - by always re-running code from top-to-bottom afresh on every single change, apps that rely on long compute may need a bit more thought to construct, including the use of Streamlit's caching mechanism. Being procedural does make things easier for development though, and on balance, I would not discount Streamlit's simplicity here.</p>"},{"location":"miscellaneous/dashboarding-landscape/#where-does-streamlit-fit","title":"Where does Streamlit fit?","text":"<p>As I see it, Streamlit's devs are laser-focused on enabling devs to very quickly get to a somewhat good-looking app prototype. In my experience, the development time for the Beta distribution app took about 3 hours, 2.5 of which were spent on composing prose. So effectively, I only used half an hour doing code writing, with a live and auto-reloading preview greatly simplifying the development process. (I conservatively estimate that this is about 1.5 times as fast as I would be using Panel.)</p> <p>Given Streamlit, I would use it to develop two classes of apps: (1) very tightly-focused utility apps that do one lightweight thing well, and (2) bespoke, single-document literate programming education material.</p> <p>I would be quite hesitant to build more complex things; then again, for me, that statement would be true more generally anyways with whatever tool. In any case, I think bringing UNIX-like thinking to the web is probably a good idea: we make little utilities/functional tools that can pipe standard data formats from to another.</p>"},{"location":"miscellaneous/dashboarding-landscape/#common-pain-points-across-all-three-dashboarding-tools","title":"Common pain points across all three dashboarding tools","text":"<p>A design pattern I have desired is to be able to serve up a fleet of small, individual utilities served up from the same codebase, served up by individual server processes, but all packaged within the same container. The only way I can think of at the moment is to build a custom Flask-based gateway to redirect properly to each utility's process. That said, I think this is probably out of scope for the individual dashboarding projects.</p>"},{"location":"miscellaneous/dashboarding-landscape/#how-do-we-go-forward","title":"How do we go forward?","text":"<p>The ecosystem is ever-evolving, and, rather than being left confused by the multitude of options available to us, I find myself actually being very encouraged at the development that has been happening. There's competing ideas with friendly competition between the developers, but they are also simultaneously listening to each other and their users and converging on similar things in the end.</p> <p>That said, I think it would be premature to go \"all-in\" on a single solution at this moment. For the individual data scientist, I would advise to be able to build something using each of the dashboarding frameworks. My personal recommendations are to know how to use:</p> <ul> <li>Voila + <code>ipywidgets</code> in a Jupyter notebook</li> <li>Panel in Jupyter notebooks and standalone <code>.py</code> files</li> <li>Streamlit in <code>.py</code> files.</li> </ul> <p>These recommendations stem mainly from the ability to style and layout content without needing much knowledge of HTML. In terms of roughly when to use what, my prior experience has been that Voila and Streamlit are pretty good for quicker prototypes, while Panel has been good for more complex ones, though in all cases, we have to worry about speed impacting user experience.</p> <p>From my experience at work, being able to quickly hash out key visual elements in a front-end prototype gives us the ability to better communicate with UI/UX designers and developers on what we're trying to accomplish. Knowing how to build front-ends ourselves lowers the communication and engineering barrier when taking a project to production. It's a worthwhile skill to have; be sure to have it in your toolbox!</p>"},{"location":"miscellaneous/learning-to-learn/","title":"How I Learned to Learn","text":"<p>In this essay, I'd like to reflect back on how I learned to learn new things. For a data scientist, it's impossible to know everything, but I do think that having a broad knowledge base can be very handy. Especially when confronted with a new problem class, having a broad toolkit of methods to solve it can give us a leg-up in terms of efficiency. This set of reflections hopefully lights up some lighbulbs for your own learning journey.</p>"},{"location":"miscellaneous/learning-to-learn/#learning-by-doingbuildingmaking","title":"Learning by doing/building/making","text":"<p>\"Carve out time to reinvent the wheel, to learn about the wheel.\"</p> <p>One way that I think is very effective in learning new topics is to learn by making things from scratch. This trick, I believe, is particularly effective when learning about the foundational topics that underlie the API abstractions that we interact with as data scientists.</p> <p>For example, I learned quite a ton about architecting a deep learning library by trying to make one myself. The end result is fundl, a deep learning framework that I wrote that supports my own learning about the so-called fancy math that belies deep learning models. <code>fundl</code> fits in the \"model\", \"loss\", \"optimizer\" thought framework that I rely on for reasoning about deep learning models, and helps me focus on the \"model\" portion.</p> <p>In there, I have used it at work to re-implement models that I have seen implemented in other frameworks (e.g. PyTorch and TensorFlow), and translate them into the NumPy API. In doing so, I not only build familiarity with the models, but also gain familiarity with the other tensor library APIs, helping me to keep pace with framework development while also leveraging existing knowledge that I have (in the NumPy API).</p> <p>Through the process of implementing deep learning models, I have also found my mental model of linear algebra and data transformations has also grown. For example, I no longer am satisfied to think of a deep learning model in terms of an amorphous black box. Rather, thanks to reimplemtation, I am much more inclined to think about the model as doing some form of rotation and projection in n-dimensional space, which is exactly what dot products are all about. Thinking this way, I think, prevents a predisposition towards anthropomorphization of machine learning models, which is just a fancy term for ascribing human properties to models.</p>"},{"location":"miscellaneous/learning-to-learn/#learning-by-teaching","title":"Learning by teaching","text":"<p>\"Having made the wheel, share how it was made.\"</p> <p>Teaching something is also an incredibly effective method to learn a new topic. I was able to learn graph theory during graduate school not only because I used it as a tool in my research, but also because I made teaching material in Python and brought it to conferences to share the knowledge.</p> <p>I think one of the key reasons why teaching is so unreasonably effective in learning is that it forces us to demonstrate our mastery over our knowledge in two ways.</p> <p>Firstly, in preparing the teaching material, we anticipate the questions that may arise from others. To address those questions, in turn, we must be prepared with knowledge deeper than what we have chosen to present.</p> <p>Secondly, any presentation of the material involves a linearization of a messy knowledge graph. In my conception, when I present material, I am tracing a path through the knowledge graph, while sprinkling in edges that branch off a main knowledge trunk.</p> graph LR;     A((A)) ==&gt; B((B));     A((A)) --&gt; C((C));     B((B)) ==&gt; D((D));     C((C)) ==&gt; E((E));     D((D)) ==&gt; C((C));     B((B)) --&gt; E((E));     D((D)) --&gt; E((E)); <p>The third point pertains to learning by teaching in quantitative topics. By forcing myself to \"teach\" the ultimate dumb student - a Python interpreter - to do math-y things, I not only make concrete an abstract topic, I also have to verify that the abstract topic is implemented correctly, because a Python interpreter will definitely get it wrong if I implemented it wrong.</p> <p>I've been incredibly fortunate to have a few platforms to do teaching, the primary one being the Python and data science conferences that I attend. That said, there are many avenues for teaching that you could take advantage of, including at work (1-on-1 pair coding or workshops), regional or international conferences, e-learning platforms, and more, and I would encourage you to leverage the platform that suits your situation best.</p>"},{"location":"miscellaneous/learning-to-learn/#leveraging-existing-knowledge","title":"Leveraging existing knowledge","text":"<p>\"Pick projects that are adjacenct to what I know how to do.\"</p> <p>Continuing the \"knowledge graph\" analogy referenced above, I have made an effort in my learning journey to leverage as much existing knowledge that I can. It seems to me that knowledge is best picked up and made to stick when I can use one topic to anchor another, and vice versa.</p> <p>A few lightweight examples that have showed up in my learning journey:</p> <ul> <li>Connecting graph message passing with linear algebra</li> <li>Implementing Bayesian models from scratch but leveraging Python</li> <li>Digging into deep learning starting from linear regression</li> </ul> <p>In the process of leveraging my existing knowledge to learn new things, I find that tying the learning process to the creation of \"minimally complex examples\" greatly accelerates my own learning process.</p> Minimally Complex Examples <p>These are examples that are simple to grok, but not trivial. For example, it's trivial to illustrate sampling from a (multivariate) Gaussian distribution, which is how sampyl illustrates MCMC sampling on its docs page. However, it is non-trivial, and in fact quite illuminating, to illustrate sampling from a joint distribution of data, likelihood, and priors involving a Gaussian and its parameters.</p>"},{"location":"miscellaneous/learning-to-learn/#seeking-learning-partners-and-teachers","title":"Seeking learning partners and teachers","text":"<p>Learn and teach with others.</p> <p>I also have to state that I have benefited much from learning from others. For example, my primary deep learning teacher was David Duvenaud, back when he was a post-doc at Harvard. (He is now a professor at the University of Toronto.) It was from him through which I gained the framework of deep learning as \"model + loss + optimizer\", and if I remember correctly, he was the one that taught me how to think about linear regression in that exact framework.</p> <p>Additionally, a friend from amongst the PyMC developers, Colin Carroll, has been particularly helpful and inspiring. I read his blog in which he writes about his own learnings and insights. In particular, I very much appreciate how he uses \"minimal complex examples\" to illustrate how things work. He was also the one who kept reminding me that gradient descent doesn't happen in MCMC, which thus inspired the essay on MCMC.</p> <p>More generally, I find that identifying learning partners and teachers against whom I can check understanding is a great \"social\" strategy for picking up ideas. I generally try to find win-win scenarios, where I can offer something in exchange, as this helps balance out the learning partnership and makes it win-win for my fellow learner too.</p>"},{"location":"miscellaneous/learning-to-learn/#asking-the-dumb-questions","title":"Asking the \"dumb\" questions","text":"<p>One thing I do know I'm infamous for is asking dumb questions. By \"dumb\" questions, I mostly mean questions that clarify basic ideas that I might have missed, or still have a gap on.</p> <p>In my mind, there are very, very few dumb questions. (I would probably classify repetitively asking the same basic questions over and over as not being particularly smart - use a notebook for heaven's sake!) In a more intimate learning situation, say, a 1-on-1 session, clarifying basic questions as soon as they come up is a wonderful way of ensuring that our foundational knowledge is strengthened. In larger settings, I am almost always certain that someone else will share the same basic questions that I do.</p>"},{"location":"miscellaneous/learning-to-learn/#concluding-words","title":"Concluding Words","text":"<p>This was neither a comprehensive reflection on how exactly I learn nor a comprehensive overview of how everybody learns. Nonetheless, it is my hope that you find it useful to reflect on, and that it gives you ideas for learning new technical topics.</p>"},{"location":"miscellaneous/pydata-landscape/","title":"An Opinionated and Unofficial Guide to the PyData Ecosystem","text":"<p>Last Updated: 30 May 2020</p> <p>This essay is the culmination of many years of learning to navigate the PyData ecosystem. Other data science ecosystems appear to have a major supportive entity, for example the R world has RStudio putting brain power and personnel into software and community development. Julia also has one, Julia Computing. The Python world, by contrast, seems a bit more fragmented, with a myriad of large and small groups; highly famed individuals and other individuals toiling quietly in the background; commercial and not-for-profit entities developing tools together, and more. What is considered \u201cstandard\u201d and perhaps the \u201cone and preferably only one way of doing something\u201d sometimes isn\u2019t entirely clear.</p> <p>In addition, given the age of the language and its growth during the Homo interconnectus era, ways of \u201cdoing things\u201d have evolved over time, and yet vestiges of the past lurk all over the place on the internet. For example, the <code>matplotlib</code> <code>pylab</code> API has been long deprecated, but a newcomer who stumbles upon an old StackOverflow Q&amp;A may not know better if they see the now-deprecated line, <code>from pylab import *</code>.</p> <p>The combination of longevity and diversity means navigating the Python data science ecosystem can sometimes be intimidating for someone who is used to a more unified ecosystem. A colleague of mine who moved from Matlab to R found RStudio\u2019s open source approach refreshing (and yes, I really dig what they\u2019re doing as a Public Benefit Corporation too!), but when he dipped his toes into the Python world, he found the Python world a bit like the \u201cWild Wild West\u201d - confusing, to say the least.</p>"},{"location":"miscellaneous/pydata-landscape/#the-keys-to-navigating-the-pydata-world","title":"The keys to navigating the PyData world","text":"<p>Personally, I think that one of the keys to navigating the PyData ecosystem is to discard the \u201cone vendor and tool to rule them all\u201d mentality, and to embrace the fact that you can glue (almost) anything you want together. In other words, to embrace the latent hacker inside of you. Every person is a hacker at heart; we MacGyver/improvise all the time, and it\u2019s no different when we do modern development of software.</p> <p>Another key to navigating the PyData world is to recognize that it is comprised of a collection of wonderfully talented individuals who are using the language to solve a myriad of problems, and that open source tools are often a byproduct of this creative energy. The corollary is that every tool is geared with a bias towards the particular problems that the author was originally solving. If you find your use cases are similar, that\u2019s great! And if you find that you use cases are not similar, the right way to approach the community is to either (1) work with package authors to improve their tool, (2) develop patches/workarounds around that tool, (3) or develop and release your own. Disparaging another tool just because it doesn't work the way you think it does should never be on your list of appropriate actions. I would call you an \u201cinsecure bro\u201d if you do that.</p>"},{"location":"miscellaneous/pydata-landscape/#how-to-use-this-essay","title":"How to use this essay?","text":"<p>Alright, with those preface words in place, you probably came to this essay because you want a \u201cmap\u201d of where to go when you want to do stuff. That will be the structure of this essay; I will intentionally keep descriptions of tools and packages high-level. My goal here is to illuminate the variety of packages and their associated practical contexts in which they get used.</p> <p>I will structure the essay in terms of \"what should I use to do X\", or \"where should I go to do Y\".</p> <p>Use the right navigation bar to find the question that you're interested in having answered. If you can't find the exact thing, use the search bar above. And if you still can't find the thing you're interested in learning about, send me a question on the Issue Tracker for this repository!</p>"},{"location":"miscellaneous/pydata-landscape/#what-do-i-do-when-i-want-to","title":"What do I do when I want to...","text":""},{"location":"miscellaneous/pydata-landscape/#what-should-i-use-to-handle-tabularstructured-data","title":"What should I use to handle tabular/structured data?","text":"<p>If your data are small enough to fit in RAM, <code>pandas</code>](https://pandas.pydata.org/) should be your \u201cgo-to\u201d package. Its API has become a community idiom, and lots of derivative packages target the <code>pandas</code> API for drop-in compatibility. Also, it\u2019s just hit version 1.0! (Happy birthday, Pandas!)</p> <p>Amongst the derivative packages include:</p> <ul> <li><code>dask</code></li> <li><code>vaex</code> both provide a scalable DataFrame API.</li> <li><code>modin</code> also scales the <code>pandas</code> API.</li> </ul> <p>Building on top of the <code>pandas</code> API, I ported over the <code>janitor</code> R package functions to <code>pyjanitor</code>, to provide convenient data cleaning functions that one can \u201cchain\u201d. It\u2019s now grown to the point where multiple individuals have made their first open source contributions through the project!</p>"},{"location":"miscellaneous/pydata-landscape/#which-plotting-packages-should-i-use","title":"Which plotting packages should I use?","text":"<p><code>matplotlib</code> is the workhorse package that has been steadily evolving for a very long time! It was also the first one that I picked up for general purpose plotting. Because it provides a lot of low-level plotting components, it is extremely powerful and flexible. I also made my first open source contribution through the package, in which I helped rework the examples to use the <code>pyplot</code> API rather than the deprecated <code>pylab</code> API. Even today, I still use it for highly bespoke and customized plotting; its SVG output also makes for <code>git</code> diff-able figures too!</p> <p>For a convenient statistical visualization API, <code>seaborn</code> is where you would want to go. It provides the ability to do faceting, pair plots, and more, using a declarative API that interoperates nicely with <code>pandas</code>.</p> <p>To go to web plotting, Bokeh, HoloViews, Altair and Plotly are where you probably would want to head to. hvPlot is the sibling package to HoloViews, providing a high-level plotting API.</p> <p>Everything is moving quickly in the \u201cPython web plotting\u201d space, so the best way to invest in yourself is to know how to read code from each of the libraries. HoloViews and hvPlot are part of the <code>pyviz</code> ecosystem of packages supported and developed by Anaconda developers. (I \u2764\ufe0f what Anaconda has been doing all this while!)</p>"},{"location":"miscellaneous/pydata-landscape/#what-tools-should-i-use-for-dashboarding","title":"What tools should I use for dashboarding?","text":"<p>The original thing I would have done to build a dashboard is to program a Flask app, write HTML templates and style it with custom CSS, and inject plot figures as base64-encoded strings or Bokeh plots. Thankfully, those days are over now!</p> <p>The key thing you\u2019ll want to look for in a dashboarding package is a set of widgets that you can program interactions on, as well as tools to display visualizations on the interface. Here\u2019s an overview of the tools that I\u2019ve worked with.</p> <p>Firstly, Voila builds upon the Jupyter <code>ipywidgets</code> ecosystem, and allows a data scientist to compose prose and widgets together while staying within the Jupyter notebook environment. Voila, in my opinion, is \u201cl\u2019original\u201d of dashboard tooling.</p> <p>Panel, which comes from the PyViz ecosystem of packages, uses the Bokeh ecosystem of widgets. This piece of history makes sense only in light of the packages being developed originally at Anaconda. The Panel devs, however, are now also building in <code>ipywidgets</code> interoperability. Panel allows you to move outside of the Jupyter notebook to build apps.</p> <p>Streamlit is another dashboard creation package that embraces the \u201cscripting\u201d mindset. Streamlit apps are essentially one long Python script that is continually executed from top to bottom (not unlike what we would expect from a Jupyter notebook). Streamlit apps give you an extremely fast path to realize your idea as a prototype; because of its idioms, it is my current \u201cgo-to\u201d tool.</p> <p>From personal experience, Streamlit has helped me go fast from idea to prototype. However, its programming idioms encourage a \u201cdump everything into the script\u201d programming style, which may make apps harder to maintain and modify long-term. Panel and Voila encourage the more modular style of programming, but that slows me down to getting a prototype stood up. It seems the tradeoff lies not in the package per se, but in the programming model for building UIs.</p> <p>The other package that I have not yet used is Plotly\u2019s Dash. From my colleagues\u2019 description, it plays very nicely with HTML standards, so for production systems that require branding because they are external, client-facing, this might be a great way to go. (I, on the other hand, happen to not work on data products that require a branding.)</p>"},{"location":"miscellaneous/pydata-landscape/#how-do-i-deploy-my-dashboard-app","title":"How do I deploy my dashboard app?","text":"<p>Deploying a dashboard app usually means you need a place to host it separate from the machine on which you do your exploratory project work.</p> <p>The key idea here is to use a platform as a service, or to use a Docker container.</p> <p>If your company provides a PaaS, such as a Heroku-like service (they are the originals!), then you should be using it! With a PaaS, you declare the resources that you need for your app using configuration files, and let the PaaS system reproducibly build exactly what you've declared on every push to your <code>master</code> branch! Doing things this way seriously simplifies the deployment of your dashboard app.</p> <p>If your company doesn't provide a PaaS, then I would recommend figuring out how to use Docker. Docker itself won't allow you to deploy the app on a separate computer (you need other tooling and possibly will have to work with your DevOps team to get containers deployed to the web), but it will at least get you a build of your project that is isolated from your development environment.</p>"},{"location":"miscellaneous/pydata-landscape/#what-packages-should-i-use-for-machine-learning","title":"What packages should I use for machine learning?","text":"<p>Thankfully, the community has really coalesced around <code>scikit-learn</code>, with many packages going as far as adopting its API idioms. (Adopting and conforming to API idioms is a good thing! Don't let Oracle's lawsuit against Google fool you otherwise.) For the vast majority of ML problems (which don\u2019t really need deep learning methods), <code>scikit-learn</code> should be your go-to package.</p> <p>The ecosystem of packages that all adopt <code>scikit-learn</code>'s APIs include:</p> <ul> <li>XGBoost</li> <li>CatBoost</li> <li>TPOT: AutoML on top of scikit-learn</li> <li>PyCaret: low-code machine learning on top of <code>scikit-learn</code>-compatible packages.</li> </ul> <p>At this point, I usually default to PyCaret to get baseline models quickly set up. It is basically painless and automates many routine tasks that I would otherwise end up writing many lines of code for.</p>"},{"location":"miscellaneous/pydata-landscape/#what-libraries-do-i-use-for-tensor-computing-and-deep-learning","title":"What libraries do I use for tensor computing and deep learning?","text":"<p>Those who know me well enough will know that I'm a fan of community standard APIs. So when it comes to Tensor computing, I naturally will first recommend the packages that play nicely with one another. Here, there are obvious ones:</p> <ul> <li>NumPy: the workhorse! Also basically evolving into a community-standard API.</li> <li>CuPy: NumPy API on GPUs</li> <li>Dask Arrays: Distributed CPU and GPU arrays</li> <li>JAX: NumPy on CPU, GPU and TPU, plus automatic differentiation as a first-class citizen!</li> </ul> <p>In particular, I want to highlight JAX. It's non-trivial to build a robust and general purpose automatic differentiation system that also works within the boundaries of known community idioms, but that is exactly what the JAX (and original <code>autograd</code>) devs did. In fact, the guys who taught me deep learning are the JAX developers, for which I remain eternally thankful: anthropomorphisms of neural networks no longer capture my attention.</p> <p>Built on top of JAX include a number of neural network libraries whose semantics play very nicely with the NumPy API. This includes:</p> <ul> <li><code>stax</code>: Built into jax, under <code>jax.experimental.stax</code></li> <li><code>flax</code>: Designed for flexibility</li> <li><code>trax</code>: Built for speed. I believe DeepMind is the Google subgroup that built this.</li> </ul> <p>Regardless, though, there's still a smaller (but sizeable) group of people who use the deep learning frameworks quite productively, even though they do not have perfect interoperability with the NumPy API:</p> <ul> <li>PyTorch</li> <li>TensorFlow</li> <li>Gluon</li> </ul> <p>PyTorch and Chainer share a lineage, and if I understand history correctly, Chainer was the original. That said, PyTorch was backed by a behemoth tech firm in SV (Facebook), while Chainer was supported by a Japanese consulting firm (Preferred Networks), and we know that the monopolistic tendencies of Silicon Valley won out. The Chainer team at Preferred Networks have discontinued further development, instead providing CuPy and Optuna.</p>"},{"location":"miscellaneous/pydata-landscape/#what-do-i-do-if-i-need-to-do-statistical-inference","title":"What do I do if I need to do statistical inference?","text":"<p>It\u2019s taken me many years of studying to finally reach this conclusion: it seriously pays to know the core concepts of \u201ceffect size\u201d and probability distributions. Those, more than significance tests, matter. Additionally, being able to describe data generating processes through the lens of probability distributions is extremely fundamental.</p> <p>All of this is a long-winded way to say, don't rely on canned statistics! Go learn it well. And if you are lucky enough to do so, learn it Bayesian first, or rather, from the lens of simulating data generating processes. Everything in statistics makes sense only in light of data generating distributions.</p> <p>Now, with that said, how do we do statistical inference, especially Bayesian statistical inference?</p> <p>PyMC3 is one of the two major probabilistic programming languages for Pythonistas, the other being Stan (through the PyStan interface). I personally have developed and contributed to PyMC3, and am extremely fond of being able to do Bayesian statistics without needing to learn another domain-specific language, so I have a biased interest in the package. It pairs with ArviZ, a package that provides visual diagnostics for Bayesian statistical analysis workflows.</p> <p>Theano, the tensor library that PyMC3 is built on top of, seems to have a few years more of life in its belt, though the compatibilities with modern operating systems are starting to show through the cracks.</p> <p>PyMC4 is the next generation of PyMC, and is built on top of Tensorflow Probability, and already has an alpha release made.</p> <p>Other probabilistic programming languages are also under constant development, not least one of which I'm paying close attention to being <code>mcx</code>, which is built on top of JAX!</p> <p>If you are well-versed in statistics, and you know the assumptions of the models you\u2019re using, and you\u2019re able to correctly interpret them, then <code>statsmodels</code> provides a unified interface to \u201cstandard\u201d statistical inference algorithms, including linear models, time series models, and more. As with all models, know the tools that you\u2019re wielding well enough to know when they can fail!</p>"},{"location":"miscellaneous/pydata-landscape/#what-do-i-use-if-my-data-dont-fit-in-ram","title":"What do I use if my data don't fit in RAM?","text":"<p>In the PyData ecosystem, there are two major players: Dask and Spark.</p> <p>In my personal experience, Dask has been the more productive tool to use. It\u2019s easy to install, provides a near-perfect drop-in replacement for the NumPy and Pandas APIs, and is easy to get going without additional software on existing cluster systems (e.g. GridEngine). Coiled Computing, led by Dask creator Matthew Rocklin, is also apparently working on tooling to enable individual data scientists to scale to the cloud easily from a single laptop. I can\u2019t wait to see what happens there.</p> <p>That said, if you\u2019re stuck with Spark because of legacy systems, there\u2019s thankfully a not-so-bad ecosystem of packages in the Spark world. (Its API policy, though, plays less nicely with the rest of the FOSS PyData world.)</p> <p>NVIDIA has also been investing heavily in the PyData stack with a suite of GPU-accelerated tools, RAPIDS. In there are a suite of packages that are actively developed for GPU-accelerated DataFrames (cuDF), graph analytics (cuGraph), SQL query engines (BlazingSQL), machine learning (cuML), and more. Definitely worth keeping an eye out on.</p>"},{"location":"miscellaneous/pydata-landscape/#how-should-i-obtain-a-python","title":"How should I obtain a Python?","text":"<p>I\u2019ve deliberately worded it as \u201ca Python\u201d.</p> <p>If you\u2019re doing data science work, in my opinion, the best way to install Python is to grab <code>miniconda</code>, and ensure that each of your environments have their own <code>conda</code> environment.</p>"},{"location":"miscellaneous/pydata-landscape/#what-are-environments-and-how-should-i-structuremaintain-them","title":"What are environments, and how should I structure/maintain them?","text":"<p>There are basically two idiomatic ways. For data science users, <code>conda</code> probably should be your default, especially if you need access to non-Python libraries. For non-data science users, use of <code>venv</code> is probably a good way to go, though it only handles Python packages.</p> <p>Each project you work on really should have its own conda environment. The way to do this is to make sure each project lives in its own isolated directory on your filesystem, and contains an <code>environment.yml</code> file that fully specifies every single package you need to work on the project.</p> <p>If you want to take this one step further, use Docker containers! VSCode has the ability [to open any repository inside a \"development container\"][devcontainer]. I've test-driven this on some projects that I work on, including <code>pyjanitor</code>, notes on causal inference, Network Analysis Made Simple, and more.</p>"},{"location":"miscellaneous/pydata-landscape/#which-integrated-development-environment-ide-andor-text-editor-should-i-use","title":"Which Integrated Development Environment (IDE) and/or text editor should I use?","text":"<p>Which IDE/text editor you use is personal, but the general idea here is to \u201clearn one really well, and be dangerous enough with 2-3 more.\u201d Here\u2019s a partial list you can think about:</p> <p>IDEs:</p> <ul> <li>Visual Studio Code for software development. Their notebook support is also not bad. The remote extension lets you work really well. Also LiveShare works wonders. Finally, opening repositories on GitHub inside pre-defined \"development containers\" seriously simplifies the whole process of getting setup in a new computer.</li> <li>Jupyter Lab for notebook + software development. Notebooks are first-class citizens here.</li> <li>PyCharm. They have free licenses for open source developers.</li> <li>Sublime Text. I go to this one for times I need a fast text editor up and running.</li> </ul> <p>Personally, I stick with Jupyter and VSCode, though I\u2019ve picked up enough <code>nano</code> hacks to work with it in the terminal.</p> <p>More than anything, though, don\u2019t disparage anybody\u2019s use of their favorite text editor. At the same time, be ready to use the ones that allow collaborative text editing.</p>"},{"location":"miscellaneous/pydata-landscape/#should-i-worry-about-git-what-tools-around-it-should-i-use","title":"Should I worry about <code>git</code>? What tools around it should I use?","text":"<p>The short answer is a resounding yes!</p> <p>The longer answer is as follows: Knowing how to use a version control system will make you more versatile as a data scientist. The biggest advantage of knowing a modern version control system is the ability to introduce a workflow that lets you experiment on branches isolated from a gold standard source of verified, tested, and \u201cknown-to-work\u201d collection code and notebook. Everything around that builds on this tooling.</p> <p>This includes the most important downstream feature of using <code>git</code> on the web: being able to automatically trigger builds of anything, including static sites, Docker development containers, documentation, and more. Once you embrace this workflow plus all of the automation surrounding it, you level up your skill level!</p> <p>Through whatever accidents of history, <code>git</code> has emerged as the de facto version control system for modern software and data science workflows. Knowing it will let you, a data scientist, work more effectively with your engineering counterparts.</p> <p>In using <code>git</code>, there is only one other tool I would recommend knowing, that is <code>pre-commit</code>. It gives you the ability to apply automatic code and notebook checks before you commit anything into the repository, thus preventing \u201cbad practices\u201d from seeping in, such as having large figures in your notebooks checked into your version control repository. Pre-commit will stop the committing from happening if code and/or notebook checks fail. Some tools like black and nbstripout will also automagically modify the offending files for you, so that when you do the commit once more after that, the checks will pass.</p>"},{"location":"miscellaneous/pydata-landscape/#how-do-i-build-documentation-for-my-projects","title":"How do I build documentation for my projects?","text":"<p>Documentation is usually the last thing on everybody\u2019s minds with a project, but it is also the only thing that will scale you and expand the reach of your project. Investing in documentation, thus, increases your impact. Don\u2019t neglect it.</p> <p>With documentation, static site generators that produce standalone HTML documents are what you probably want to go for. This allows you to host the documentation on the internet (or your organization\u2019s intranet). (Where to host the HTML pages is a different question, this section only discusses how to generate them.)</p> <p>So what tools are available for documentation?</p> <p>The workhorse of the Python documentation world is Sphinx. It comes with a lot of features and extensions, and many developers swear by it. Many others, however, also swear a ton because of it, because it is complicated to learn, and the syntax is not easy to remember \u2014 especially if one doesn\u2019t write documentation every day like code! If you\u2019re willing to invest the time to learn, you\u2019ll be amazed at the power that Sphinx provides for building static HTML documentation. Think of Sphinx as the matplotlib of documentation: old, low-level, and extremely powerful.</p> <p>If you prefer Markdown documentation, then consider using MkDocs, which is an up-and-coming documentation system. Markdown is quite idiomatic to write, has a nice one-to-one correspondence to HTML, lets you interleave HTML, and as such makes writing documentation much simpler. An ecosystem of packages that you can mix-and-match are available to help with automatic parsing of docstrings (for API docs), and the <code>mkdocs-material</code> package provides very fancy ways to write device screen-responsive documentation with built-in static site search (no server required). Personally, I\u2019ve switched to writing all new docs using MkDocs and <code>mkdocs material</code>. (This site itself is built using mkdocs material!) There are some compatibility rough spots that I have seen, but that should not discourage you from trying it out.</p>"},{"location":"miscellaneous/pydata-landscape/#what-do-i-do-to-solve-my-code-copypasting-problem","title":"What do I do to solve my code copy/pasting problem?","text":"<p>You\u2019ve found yourself copying and pasting code across notebooks, and now it\u2019s unsustainable to keep modifying them in 13 places. Writing a software library will help you consolidate your code in a single source of truth, which will let you update once to be applied everywhere the code is called.</p> <p>Python comes with all of the tools you need to make your own software library and import it into your notebooks and apps. There is an official Python Packaging Tutorial that you should refer to when building your own Python packages.</p>"},{"location":"miscellaneous/pydata-landscape/#how-do-i-test-my-code","title":"How do I test my code?","text":"<p>Running and writing tests may sound like something only Quality Assurance engineers should be doing, but because data scientists are writing code, having good software skills helps. You don\u2019t need to go far, you only need to have the basics on hand.</p> <p><code>unittest</code> is built into the Python standard library, and you can get started pretty easily.</p> <p>On the other hand, <code>pytest</code> has become a very common testing package. The developers put a lot of thought into how the API is developed. As such, it is easy to get started with, extremely capable for complex situations, and runs everything in between very productively.</p> <p>To take your testing to the next level, add in Hypothesis to your list of packages to experiment with. Hypothesis has helped me find edge cases in my code that I never would have thought to find. The docs are also amazing, and it's easy to get started with on select parts of your code base (especially if you're not ready to commit all-in)!</p>"},{"location":"miscellaneous/pydata-landscape/#how-do-i-guarantee-the-reproducibility-of-my-notebooks","title":"How do I guarantee the reproducibility of my notebooks?","text":"<p>You\u2019ll want to know how to do this if you are writing notebooks that contain crucial analyses that you need to reproduce.</p> <p>Thankfully, there is one idiomatic way to handle this, and that is by using <code>nbconvert</code>. The <code>nbconvert</code> docs provide documentation on how to programmatically execute your notebooks from Python and the command line.</p> <p>Then, you'll want to subject your notebooks to \"continuous execution\". The general idea is to master a \u201ccontinuous pipeline runner\u201d. A few for open source projects include:</p> <ul> <li>Azure Pipelines</li> <li>Travis CI</li> <li>CircleCI</li> <li>Jenkins (your company may host it internally).</li> </ul> <p>If you develop the ability to work with <code>git</code> properly, then the entire world of automatic pipeline runners is at your fingertips! Almost every pipeline runner can be triggered on new commits to any pre-specified branches, and also come with the ability to execute checks automatically on any pull request branches, thereby guaranteeing that you have fully reproducible builds!</p>"},{"location":"miscellaneous/pydata-landscape/#what-package-should-i-use-for-network-analysis","title":"What package should I use for network analysis?","text":"<p>I would strongly recommend the <code>NetworkX</code> package! It provides a very consistent, easy-to-learn API, which lowers the barrier of entry to learning graph theory concepts.</p> <p>Together with Mridul Seth, I teach Network Analysis Made Simple at PyCon, SciPy and ODSC conferences. It's the culmination of our respective learning journeys, shared freely with everyone! We also have courses on DataCamp where you can learn how to use the NetworkX package in an interactive learning environment.</p> <p>NetworkX provides the data structures and algorithms for working with graphs in memory, but in case you want additional speed, the <code>igraph</code> package is written in C++ with Python and R bindings.</p>"},{"location":"miscellaneous/pydata-landscape/#what-can-i-do-to-improve-my-code-quality","title":"What can I do to improve my code quality?","text":"<p>There's a few things you can do!</p> <p>Firstly, use type hints in your function signatures. They help a code reader reason about the types of the objects that you pass into a function.</p> <p>Secondly, learn how to design software from the outside in. As my friend Jesse Johnson puts it, it is tempting to write software focusing on the most interesting piece first, but you end up making it difficult to write composable code.</p> <p>Thirdly, learn how to test your code! That will give you confidence that you've written correct code. It'll also train you to write software in a composable fashion.</p>"},{"location":"miscellaneous/pydata-landscape/#was-there-something-missing-that-you-still-have-questions-about","title":"Was there something missing that you still have questions about?","text":"<p>If so, head over to the issue tracker and post it there; don't forget to reference the essay!</p>"},{"location":"miscellaneous/pydata-landscape/#with-thanks","title":"With thanks","text":"<p>Special shout-out to my Patreon supporters, Eddie Janowicz, and Carol Willing, and Hector Munoz! Thank you for keeping me caffeinated and supporting my work that gets shared with the rest of the PyData community!</p>"},{"location":"miscellaneous/static-sites-on-dokku/","title":"Static Sites and Apps On Your Own Dokku Server","text":"<p>Summary: In this essay, I'm going to share with you how you can deploy your own static sites and apps on a Dokku server.</p>"},{"location":"miscellaneous/static-sites-on-dokku/#introduction","title":"Introduction","text":"<p>You've worked on this awesome Streamlit app, or a Panel dashboard, or a Plotly Dash web frontend for your data science work, and now you've decided to share the work. Or you've built documentation for the project, and you need to serve it up. Except, if your company doesn't have a dedicated platform for apps, you're stuck! That's because you've now got to share it from your laptop/workstation and point your colleagues to there (right... go ahead and email them a link to <code>http://10.16.213.24:8501</code> right now...) and keep your computer running perpetually to serve the app in order for them to interact with it.</p> <p>Or, you copy/paste the docs into a separate hosted solution, and now the docs are divorced from your code, leading to documentation rot in the long-run because it's too troublesome to maintain two things simultaneously. That's much too fragile. What you need is another hosting machine that isolated from your development machine, so that you can develop in peace while the hosting machine reliably serves up the working, stable version of your work.</p> <p>The specific thing that you really need is a \"Platform as a Service\". There's a lot of commercial offerings, but if you're \"cheap\" and don't mind learning some new concepts to help you get around the web, then this essay is for you. Here, I'm going to show you how to configure and deploy Dokku as a personal PaaS solution that you can use at work and for hobby projects. I'm then going to show you how to deploy a static site (which can be useful for hosting blogs and documentation), and finally I'll show you how to deploy a Streamlit app, which you can use to show off a front-end to your fancy modelling work. Along the way, I hope to also point out the \"generalizable\" ideas behind the steps listed here, and give you a framework (at least, one useful one) for building things on the web.</p>"},{"location":"miscellaneous/static-sites-on-dokku/#but-but-why","title":"But, but why?","text":""},{"location":"miscellaneous/static-sites-on-dokku/#if-youre-part-of-a-company","title":"If you're part of a company...","text":"<p>Your organization might not be equipped with modern PaaS tools that will enable you, a data scientist, to move quickly from local prototype to share-able prototype. If, however, you have access to bare metal cloud resources (i.e. just gimme a Linux box!), then as a hacker-type, you might be able to stand up your own PaaS and help demonstrate to your infrastructure team the value of having one.</p>"},{"location":"miscellaneous/static-sites-on-dokku/#if-youre-doing-this-for-your-hobby-projects","title":"If you're doing this for your hobby projects...","text":"<p>You might be as cheap as I am, but need a bit more beefy power than the restrictions given to you on Heroku (512MB RAM, 30 minute timeouts, and limited number of records in databases), and you don't want to pay $7/month for each project.</p> <p>Additionally, you might want a bit more control over your hosting options, but you don't feel completely ready to go fiddling with containers and networking without a software stack to help out just a bit.</p>"},{"location":"miscellaneous/static-sites-on-dokku/#more-generally","title":"More generally...","text":"<p>You might like the idea of containers, but find it kind of troublesome to learn yet another thing that's not trivial to configure, execute and debug (i.e. Docker). Dokku can be a bridge to get you there, as it automates much of the workflow surrounding Docker containers. It also comes with an API that both matches closely to Heroku (which is famous for being very developer-friendly) and also helps you handle proxy port mapping and domains easily.</p> <p>Are you ready? Let's go!</p>"},{"location":"miscellaneous/static-sites-on-dokku/#pre-requisites","title":"Pre-requisites","text":"<p>I'm assuming you know how to generate and use SSH keys to remotely access another machine. This is an incredibly useful thing to know how to do, and so I'd recommend that you pick this skill up. (As usual, DigitalOcean's community pages have a great tutorial.)</p> <p>I am also assuming that you have access to a Linux box of some kind with an IP address that you can expose to the \"internet\". The \"internet\" in this case can mean the world wide web if you're working on a personal project, or your organization's \"intranet\" if you're planning on only letting those in your organization access the sites and apps that you will build.</p> <p>I'm also assuming familiarity with <code>git</code>, which I consider to be an indispensable tool in a data scientist's toolkit.</p> <p>This last point is not an assumption, but an exhortation: you should be building your data app prototype in accordance to 12-factor app principles. It'll make (most of) your Engineering colleagues delight in working with you. (Yes, there are some esoteric types that don't subscribe to 12 factor app principles...) If you've never heard of it, go check it out here. It's a wonderful read that will change how you build your data app prototypes for the better. It will also make handing over the app to Engineering much easier in the long-run, improving your relationship with the Engineering team that will take care of your app!</p>"},{"location":"miscellaneous/static-sites-on-dokku/#set-up-a-box-in-the-cloud-optional-if-you-have-one","title":"Set up a Box in the Cloud (optional if you have one)","text":"<p>If you don't already have another computer that you have access to, or if you're curious on how to get set up in the cloud, then follow along these instructions.</p> <p>To make things a bit more concrete, I'm going to rent a server on DigitalOcean (DO). If you don't have a DigitalOcean account, feel free to use my referral link to sign up (and get free $100 credits that you can spend over 60 days) if you haven't got a DigitalOcean account already. (Disclaimer: I benefit too, but your support helps me make more of this kind of content!) Once you've signed up and logged into the DigitalOcean cloud dashboard, you can now set up a new Droplet.</p> Droplets? <p>\"Droplet\" is nothing more than DO's fancy name for a cloud server, or basically a computer that they're running.</p> <p>To do so, click on the big green \"Create\" button, set up a Droplet with the following settings:</p> <ol> <li>Ubuntu operating system</li> <li>\"Standard\" plan at $5/mo or $10/mo <sup>1</sup></li> <li>Additional options: \"monitoring\"</li> <li>Authentication: you should use SSH keys (this is a pre-requisite for this essay).</li> <li>Hostname: Give it something memorable.</li> <li>Backups: Highly recommended. Give yourself peace of mind that you can rollback anytime.</li> </ol> <p>Once you're done with that, hit the big green \"Create Droplet\" button right at the bottom of the screen!</p> <p>Once your droplet is set up, you can go ahead and click on the \"Manage &gt; Droplets\" left menu item, and that will bring you to a place where you can see all of your rented computers.</p> The Cloud <p>FYI, that's all the cloud is: renting someone else's computers. It turns out to be a pretty lucrative business! And because it's just someone else's computers, don't think of it as something magical.</p>"},{"location":"miscellaneous/static-sites-on-dokku/#setup-dokku-on-your-shiny-rented-machine","title":"Setup Dokku on your Shiny Rented Machine","text":"<p>Let's now go ahead and set up Dokku on your shiny new droplet.</p>"},{"location":"miscellaneous/static-sites-on-dokku/#run-the-dokku-installation-commands","title":"Run the Dokku installation commands","text":"<p>Dokku installation on Ubuntu is quite easy; the following instructions are taken from the Dokku docs. SSH into your machine, then type the following:</p> <pre><code>wget https://raw.githubusercontent.com/dokku/dokku/v0.20.4/bootstrap.sh;\n# CHECK FOR LATEST DOKKU VERSION!\n# http://dokku.viewdocs.io/dokku/getting-started/installation/\nsudo DOKKU_TAG=v0.20.4 bash bootstrap.sh\n</code></pre>"},{"location":"miscellaneous/static-sites-on-dokku/#wrap-up-dokku-installation","title":"Wrap up Dokku installation","text":"<p>If you're on an Ubuntu installation, you'll want to navigate to your Linux box's IP address, and finish up the installation instructions, which involves letting your Dokku installation know about your SSH public keys. These keys are important, as the are literally the keys to letting you push your app to your Dokku box!</p> SSH Keys and Dokku <p>More pedantically, these SSH keys identify your machine as being \"authorized\" to push to the <code>git</code> server that is running on your Dokku machine. This is what allows you to <code>git push dokku master</code> later on!</p> <p>For those of you who have CentOS or other flavours of Linux, you will need to follow analogous instructions on the Dokku website. I have had experience following the CentOS instructions at work, and had to modify the installation commands a little bit to work with our internal proxies.</p>"},{"location":"miscellaneous/static-sites-on-dokku/#test-that-dokku-is-working","title":"Test that Dokku is working","text":"<p>To test that your Dokku installation is working, type the following command:</p> <pre><code>dokku help\n</code></pre> <p>That should show the Dokku help menu, and you'll know the installation has completed successfully!</p>"},{"location":"miscellaneous/static-sites-on-dokku/#optionally-set-proxies-for-docker","title":"Optionally set proxies for Docker","text":"<p>Now, because Dokku builds upon Docker, if you're behind a corporate proxy, you might need to configure your Docker daemon proxies as well. You'll then want to follow instructions on the Docker daemon documentation.</p> <p>Those steps will generally be the same as what's in the docs, though the specifics will change (e.g. your proxy server address).</p>"},{"location":"miscellaneous/static-sites-on-dokku/#configure-domain-names","title":"Configure domain names","text":"<p>The allure of Heroku is that it gives your app a name: <code>myapp.herokuapp.com</code>, or <code>myshinyapp.herokuapp.com</code> quite automagically. With Dokku and a bit of extra legwork, we can replicate this facility.</p> <p>We're going to set up your Dokku box uch that its main domain will be <code>mydomain.com</code>, and apps will get a subdomain <code>myapp.mydomain.com</code>.</p>"},{"location":"miscellaneous/static-sites-on-dokku/#register-a-domain-name","title":"Register a domain name","text":"<p>Firstly, you'll need a domain name from a Domain Name Service (DNS) registrar. Cloudflare seems to be doing all of the right things at the moment, so their domain name registration service is something I'm not hesitant to recommend (at least as of 2020). For historical reasons, I'm currently using Google Domains. At work, we have an internal service that lets us register an internal domain name. What matters most is that you have the ability to assign a domain name that points to your Dokku machine's IP address.</p> <p>Go ahead and register a domain name that reflects who \"you\" are on the web. For myself, I have a personal domain name that I use. At work, I registered a name that reflects the research group that I work in. Make sure that the name \"points\"/\"forwards\" to the IP address of your Dokku box.</p>"},{"location":"miscellaneous/static-sites-on-dokku/#enable-subdomains","title":"Enable subdomains!","text":"<p>To enable the ability to use subdomains like <code>myapp.mydomain.com</code> for each app, you'll want to also configure the DNS settings. On your domain registrar, look for the place where you can customize \"resource records\". On Google Domains, it's under \"DNS &gt; Custom resource records\".</p> <p>There, you'll want to add an \"A\" record (as opposed to other options that you might see, like \"CNAME\", \"AAAA\", and others). The \"name\" should be <code>*</code>, literally just an asterisk. The IPv4 address should point to your Dokku machine. This is all that is needed.</p> What is an 'A' record? <p>The most comprehensive explainer I've seen is at dnsimple, but the long story short is that it is an \"Address\" record. Yes, \"A\" stands for \"Address\", and it is nothing more than a pointer that maps \"string\" address to IP address.</p> What then about the name <code>*</code>? <p>What we just did up there was to say, point everything <code>*.mydomain.com</code> to to the Dokku box IP address. How then do we get subdomains if we don't configure them with our DNS? Well, the secret here is that Dokku can help us with subdomains. Read on for how configuration of your Dokku box!</p> <p>To test whether your domain name is setup correctly, head over to the domain in your favourite browser. At this point, you should see the default NGINX landing page, as you have no apps deployed and no domains configured.</p> How do you pronounce 'NGINX'? <p>The official way is \"engine-X\". The wrong way is \"en-jinx\". Don't get it wrong!</p> And what is NGINX? <p>From Wikipedia:</p> <p>Nginx is a web server which can also be used as a reverse proxy, load balancer, mail proxy and HTTP cache.</p> <p>For our purposes, we treat it as a thing that routes URLs to containers.</p>"},{"location":"miscellaneous/static-sites-on-dokku/#deploy-a-test-app","title":"Deploy a test app","text":"<p>Heroku provides a \"Python getting started\" repository that we will use to check that the installation is working correctly. This one deploys reliably with all of the vanilla commands entered. Leveraging this, I will also show you how to leverage your <code>*</code> A record to put in nice subdomains!</p>"},{"location":"miscellaneous/static-sites-on-dokku/#clone-the-test-app","title":"Clone the test app","text":"<p>Firstly, <code>git clone</code> Heroku's <code>python-getting-started</code> repository to your laptop/local machine (i.e. not your Dokku box).</p> <p>Next, <code>cd</code> into the repository:</p> <pre><code>cd python-getting-started\n</code></pre> <p>After that, add your Dokku box as a <code>git</code> remote to the repository:</p> <pre><code>git remote add dokku dokku@your-domain-name:python-getting-started\n</code></pre> <p>Be sure to replace <code>your-domain-name</code> with your newfangled domain that you registered.</p> App name <p>The <code>python-getting-started</code> after the colon is the \"app name\" that you will see at the command line when interacting with Dokku later.</p> <p>Now, push the app to your Dokku box!</p> <pre><code>git push dokku master\n</code></pre> <p>Unlike your usual pushes to GitHub, GitLab or Bitbucket, you'll see a series of remote outputs being beamed back to your terminal. What's happening here is the build of the app! In particular, a Docker build is happening behind-the-scenes, so your app is completely self-contained and containerized on the Dokku box!</p> <p>If everything went well, the last output beamed back to you should look like:</p> <pre><code>=====&gt; Application deployed:\n       http://mydomain.com:10161\n</code></pre> <p>Wonderful! Now let's go back to Dokku and configure your app.</p>"},{"location":"miscellaneous/static-sites-on-dokku/#configure-the-app-domain-and-ports","title":"Configure the app domain and ports","text":"<p>We're now going to configure Dokku to recognize which subdomains should point to which apps.</p> <p>Firstly, get familiar with the Dokku domains command:</p> <pre><code># On your Dokku box\ndokku domains:help\n</code></pre> <p>That should list out all of the Dokku <code>domains</code> sub-commands.</p> <pre><code>Usage: dokku domains[:COMMAND]\n\nManage domains used by the proxy\n\nAdditional commands:\n    domains:add &lt;app&gt; &lt;domain&gt; [&lt;domain&gt; ...]       Add domains to app\n    domains:add-global &lt;domain&gt; [&lt;domain&gt; ...]      Add global domain names\n    domains:clear &lt;app&gt;                             Clear all domains for app\n    domains:clear-global                            Clear global domain names\n    domains:disable &lt;app&gt;                           Disable VHOST support\n    domains:enable &lt;app&gt;                            Enable VHOST support\n    domains:remove &lt;app&gt; &lt;domain&gt; [&lt;domain&gt; ...]    Remove domains from app\n    domains:remove-global &lt;domain&gt; [&lt;domain&gt; ...]   Remove global domain names\n    domains:report [&lt;app&gt;|--global] [&lt;flag&gt;]        Displays a domains report for one or more apps\n    domains:set &lt;app&gt; &lt;domain&gt; [&lt;domain&gt; ...]       Set domains for app\n    domains:set-global &lt;domain&gt; [&lt;domain&gt; ...]      Set global domain names\n</code></pre> <p>You can report domains used for the app, <code>python-getting-started</code>:</p> <pre><code># On your Dokku box\ndokku domains:report python-getting-started\n</code></pre> <p>The output should look something like this:</p> <pre><code>$ dokku domains:report python-getting-started\n=====&gt; python-getting-started domains information\n       Domains app enabled:           false\nDomains app vhosts:\n       Domains global enabled:        false\nDomains global vhosts:\n</code></pre> <p>This tells us that <code>python-getting-started</code> has no domains configured for it. We can now set it:</p> <pre><code># On your Dokku box\ndokku domains:set python-getting-started python-getting-started.mydomain.com\n</code></pre> <p>The output will look like this:</p> <pre><code>-----&gt; Added python-getting-started.mydomain.com to python-getting-started\n-----&gt; Configuring python-getting-started.mydomain.com...(using built-in template)\n-----&gt; Creating http nginx.conf\n       Reloading nginx\n</code></pre> <p>Now, you should be able to go to <code>http://python-getting-started.mydomain.com</code>, and the page that gets loaded should be the \"Getting Started with Python on Heroku\" landing page!</p> So, what magic happened here? <p>What's happening here is that NGINX resolving subdomains to particular containers, and mapping them to the appropriate container port that is being exposed.</p> <p>If everything deployed correctly up till this point, you're good to go with deploying a data app on your Dokku machine!</p>"},{"location":"miscellaneous/static-sites-on-dokku/#deploy-your-data-app","title":"Deploy your data app","text":"<p>Deploying the <code>python-getting-started</code> app should have given you:</p> <ol> <li>the confidence that your Dokku installation is working correctly,</li> <li>firsthand experience configuring Dokku,</li> <li>a taste of the workflow for deploying an app.</li> </ol> <p>Now, we're going to apply that to a Streamlit app. I've chosen Streamlit because it's got the easiest programming model amongst all of the Dashboard/app development frameworks that I've seen; in fact, I was able to stand up an explainer on the Beta distribution in under 3 hours, the bulk of which was spent on writing prose, not figuring out how to program with Streamlit.</p>"},{"location":"miscellaneous/static-sites-on-dokku/#build-a-streamlit-app-skip-if-you-already-have-an-app","title":"Build a streamlit app (skip if you already have an app)","text":"<p>If you don't have a Streamlit app, here's one that you can use as a starter, which simply displays some text and a button:</p> <pre><code># app.py\n\nimport streamlit as st\n\n\"\"\"\n# First Streamlit App!\n\nThis is a dummy streamlit app.\n\"\"\"\n\nfinished = st.button(\"Click me!\")\nif finished:\n    st.balloons()\n</code></pre> <p>Save it as <code>app.py</code> in your project directory.</p> <p>Now, you can run the app with Streamlit:</p> <pre><code># On your local machine\nstreamlit run app.py\n</code></pre> <p>You should see the following show up in your terminal:</p> <pre><code>  You can now view your Streamlit app in your browser.\n  Local URL: http://localhost:8501\n  Network URL: http://&lt;your_ip_address&gt;:8501\n</code></pre> <p>You can go to the local URL and confirm that the app is running correctly, and that it does exactly what's expected.</p>"},{"location":"miscellaneous/static-sites-on-dokku/#add-project-specific-configuration-files-for-dokku","title":"Add project-specific configuration files for Dokku","text":"<p>Now, we need to add a few configuration files that Dokku will recognize.</p>"},{"location":"miscellaneous/static-sites-on-dokku/#requirementstxt","title":"<code>requirements.txt</code>","text":"<p>Firstly, make sure you have a <code>requirements.txt</code> file in the project root directory, in which you specify all of the requirements for your app to run.</p> <pre><code># requirements.txt\nstreamlit==0.57.3  # pinning version numbers is good for apps.\n# put more below as necessary, e.g.:\nnumpy==0.16\n</code></pre> <p>With a <code>requirements.txt</code> file, Dokku (and Heroku) will automagically recognize that you have a Python app. Dokku will then create a Docker container equipped with Python, and install all of the dependencies in there. Declarative configuration FTW!</p>"},{"location":"miscellaneous/static-sites-on-dokku/#procfile","title":"<code>Procfile</code>","text":"<p>Next, you need a Procfile in the project root directory:</p> <pre><code># Procfile\nweb: streamlit run app.py\n</code></pre> Procfile <p>A quick note: It is <code>Procfile</code>, with no file extensions. Don't save it as <code>Procfile.txt</code>, because that will not get recognized by Dokku/Heroku. To learn more, read about it on Heroku.</p> <p>The general syntax in the Procfile is:</p> <pre><code>&lt;process_type&gt;: &lt;command&gt;\n</code></pre> <p>The <code>command</code> is always a single line, and tells Dokku/Heroku what commands to execute in order to run the app. In our case, we simply execute the same command that we used to run the app locally for development purposes.</p> More complex Procfile commands <p>You can have multiple bash commands in a single line, though if it gets complicated, you may want to extract the commands out into a separate bash script that you execute instead, e.g.:</p> <pre><code># dokku.sh\npip install -e .\nexport CONFIG_DIR=\"/path/to/config/files\"\nstreamlit run app.py\n</code></pre> <p>For the <code>process_type</code>, in the case of Dokku, is always \"web\". Heroku, on the other hand, can handle other process types. Since we're dealing with Dokku and not Heroku, don't bother changing <code>process_type</code>.</p>"},{"location":"miscellaneous/static-sites-on-dokku/#configure-git-remote-with-dokku","title":"Configure <code>git</code> Remote with Dokku","text":"<p>Now, let's configure your Dokku remote.</p> <pre><code># On your local machine\ngit remote add dokku dokku@mydomain.com:streamlit-app\n</code></pre> <p>Remember two points!</p> <ol> <li>Firstly, change <code>mydomain.com</code> to your domain.</li> <li>Secondly, you can use\u00a0any\u00a0app name you want, it doesn't have to be <code>streamlit-app</code>.</li> </ol> <p>A convention that has helped me is to have a 1:1 mapping between app and project repository folder name. It means one less thing to be confused about.</p> <p>Once you're done configuring the remote, now push it up!</p> <pre><code># On your local machine\ngit push dokku master\n</code></pre> <p>The same build commands will take place. While they are taking place, go ahead and open a new Terminal, and SSH into the Dokku box. We're going to configure the new app on Dokku!</p>"},{"location":"miscellaneous/static-sites-on-dokku/#configure-dokku-subdomain","title":"Configure Dokku Subdomain","text":"<p>Let's start with the subdomain name first. For this tutorial, I'm going to use the domain name <code>streamlit-app.mydomain.com</code>. Let's configure the app <code>streamlit-app</code> with that domain name:</p> <pre><code># On Dokku box\ndokku domains:set streamlit-app streamlit-app.mydomain.com\n</code></pre>"},{"location":"miscellaneous/static-sites-on-dokku/#configure-dokku-port-mapping","title":"Configure Dokku port mapping","text":"<p>Next, we have to configure the port mapping that Dokku's proxy server will recognize. By default, every container has the \"hosting box\" (i.e. the machine Dokku is running on) port 80 mapped to \"container box\" (i.e. the container the app is running on) port 5000. You can see this with:</p> <pre><code># On Dokku box\ndokku proxy:report streamlit-app\n</code></pre> <p>That will give you something like:</p> <pre><code>=====&gt; python-getting-started proxy information\n       Proxy enabled:                 true\nProxy port map:                http:80:5000\n       Proxy type:                    nginx\n</code></pre> <p>Now, because <code>streamlit</code> is going to be run on port 8501 (in the container) by default, we need to change the port mapping from <code>http:80:5000</code> to <code>http:80:8501</code>. To do so:</p> <pre><code># On Dokku box\ndokku proxy:ports-set streamlit-app http:80:8501\n</code></pre> <p>Putting these two configurations together, i.e. setting the subdomain and port mapping, we have now told Dokku, \"Each time you get a request from <code>http://streamlit-app.mydomain.com</code>, forward it to port 8501 on the <code>streamlit-app</code> container.\"</p>"},{"location":"miscellaneous/static-sites-on-dokku/#test-it-out","title":"Test it out!","text":"<p>Well, we now can test it out. Go ahead and head over to your app URL, and see if the app works for you!</p>"},{"location":"miscellaneous/static-sites-on-dokku/#debugging","title":"Debugging","text":"<p>If things look like they're crashing, how do you debug? Well, you always should know how to look at the logs:</p> <pre><code>dokku logs my_app_name -t\n</code></pre> <p>That will keep the logs updating in the terminal as you refresh the page. Use the information in the logs to help you debug. Also, see if you can reproduce the error in the logs locally.</p> <p>Additionally, if you get <code>nginx</code> errors, you can look at the <code>nginx</code> logs to help you debug proxy errors as they show up:</p> <pre><code>dokku nginx:access-logs my_app_name -t\n</code></pre> <p>Look at the logs and dig through for anything that might help you with your Google searches. Follow this pattern, and soon enough, you'll become an expert at debugging your web apps!</p>"},{"location":"miscellaneous/static-sites-on-dokku/#deploy-a-static-site","title":"Deploy a static site","text":"<p>Now that you've seen how to deploy an app that's powered by a container behind-the-scenes, let's now figure out how to deploy a static site that is built upon every deploy. It's essentially the same. We have configuration files (in this case, slightly different ones) that declare what kind of environment we need. We basically treat the static site generator as an \"app\" that generates the HTML pages that we serve up freshly on each build.</p> <p>For this example, I'm going to use <code>mkdocs</code>, as it is also easy to use to build sites, and can be extended with some pretty awesome templates (like <code>mkdocs-material</code>) for responsive docs generated from Markdown files. If you've got another static site builder (I have used Lektor, sphinx, and Nikola before), the places where we use <code>mkdocs</code> commands can be easily replaced by the relevant ones for your situation.</p>"},{"location":"miscellaneous/static-sites-on-dokku/#build-a-static-site-skip-if-you-already-have-one","title":"Build a static site (skip if you already have one)","text":"<p>If you don't already have a static site, then feel free to use the following example.</p> <p>In your project root directory, create a <code>docs/</code> directory, and then place a dummy <code>index.md</code> in there:</p> <pre><code>&lt;!-- index.md --&gt;\n# Index Page\n\nHello world!\n</code></pre> <p>Now, in the project root directory, create a <code>mkdocs.yml</code> file, in which you configure <code>mkdocs</code> to build the static site:</p> <pre><code># mkdocs.yml\nsite_name: Marshmallow Generator\n</code></pre> <p>This is a minimal <code>mkdocs</code> configuration.</p> <p>Now, make sure you have mkdocs installed in the Python environment that you're using. It's available on PyPI:</p> <pre><code># On your local machine\npip install -U mkdocs\n</code></pre> <p>Once installation has finished, you can now command <code>mkdocs</code> to build the static site to view locally:</p> <pre><code># On your local machine\nmkdocs serve\n</code></pre> <p>If you can successfully view the static site on your local machine, i.e. you see the contents of <code>index.md</code> show up as a beautifully rendered HTML page, you're good to move on!</p>"},{"location":"miscellaneous/static-sites-on-dokku/#add-project-specific-configuration-files-for-dokku_1","title":"Add project-specific configuration files for Dokku","text":"<p>We're now going to add the necessary configuration files to work with Dokku.</p> <p>Firstly, we have to add in a <code>.static</code> file in the project root directory. This file tells Dokku that the site that is going to be built is a static site. To do so in the terminal, you only have to <code>touch</code> the file at the command line:</p> <pre><code># On your local machine\ntouch .static\n</code></pre> <p>Secondly, we have to add a <code>.buildpacks</code> file, where we specify that we are using two \"buildpacks\": one to provide the environment to run the commands that build the site, and another to build the site and serve up the static site files. In the case of our dummy <code>mkdocs</code> static sites, we need in <code>.buildpacks</code>:</p> <pre><code>https://github.com/heroku/heroku-buildpack-python.git\nhttps://github.com/dokku/buildpack-nginx.git\n</code></pre> <p>They have to go in that order, so that the first one is used for building, and the second one is used for serving the site.</p> What are 'buildpacks? <p>Once again, Heroku's docs have the most comprehensive explanation, as they're the originators of the idea. The short answer is that they are pre-built and configured \"base\" environments that you can build off. It's like having an opinionated Dockerfile that you can extend, except we extend it using declared configuration files in the repository instead.</p> <p>Thirdly, instead of a <code>Procfile</code>, we add an <code>app.json</code> file that contains the command for building the static site.</p> <pre><code>{\n\"scripts\": {\n\"dokku\": {\n\"predeploy\": \"cd /app/www &amp;&amp; mkdocs build\"\n}\n}\n}\n</code></pre> Deployment tasks <p>If you want to read more about this file, as well as the custom \"deployment tasks\" bit of Dokku, then check out the docs pages here.)</p> <p>OK, we just created a bunch of files, but I haven't explained how they're interacting with Dokku. There's definitely some opinionated things that we'll have to unpack.</p> <p>Firstly, the Dokku <code>buildpack-nginx</code> buildpack makes the opinionated assumption that your repository will be copied over into the Docker container's <code>/app/www</code> directory. That is why we have the <code>cd /app/www</code> command. Then, we follow it up with a <code>mkdocs build</code>, which you can change depending on what static site generator you're using.</p> <p>Secondly, the <code>predeploy</code> key declares to Dokku to execute the commands in the value (i.e. <code>cd /app/www &amp;&amp; mkdocs build</code>) before starting up the <code>nginx</code> server that points to the static site files.</p> <p>As you probably can grok by now, basically, the static sites are being built upon every deploy. This saves you from having to build the site locally and then pushing it up, which is both in-line with how <code>git</code> is supposed to be used (you only <code>git push</code> files that are generated by hand), and is in-line with the continuous deployment philosophy.</p> <p>Finally, we still need our <code>requirements.txt</code> file to be populated with whatever is needed to build the docs locally:</p> <pre><code># requirements.txt\nmkdocs==1.1\n# put other dependencies below!\n</code></pre> <p>Now that we're done, let's configure our remotes once again.</p>"},{"location":"miscellaneous/static-sites-on-dokku/#configure-git-remote-with-dokku_1","title":"Configure <code>git</code> Remote with Dokku","text":"<p>As with the Streamlit app, go ahead and configure the <code>git</code> remote with Dokku:</p> <pre><code># On your local machine\ngit remote add dokku dokku@mydomain.com:my-static-site\n</code></pre> <p>Now, push up to Dokku!</p> <pre><code># On your local machine\ngit push dokku master\n</code></pre>"},{"location":"miscellaneous/static-sites-on-dokku/#configure-dokku","title":"Configure Dokku","text":"<p>As with the Streamlit app, let's now configure the domains:</p> <pre><code># On your Dokku box\ndokku domains:set my-static-site my-static-site.mydomain.com\n</code></pre> <p>Unlike the app, we don't have to configure ports, because they will be mapped correctly by default.</p> <p>Finally, we need to configure <code>nginx</code> to point to the directory in which the <code>index.html</code> page is generated. In the case of <code>mkdocs</code>, the directory is in the <code>site/</code> directory in the project root directory. We'll now configure it:</p> <pre><code># On your Dokku box\ndokku config:set my-static-site NGINX_ROOT='site'\n</code></pre> Warning <p>You'll want to change <code>site</code> to whatever the output directory is for the static site generator you use!</p> <p>Alrighty, go ahead and visit your static site to confirm that it's running!</p>"},{"location":"miscellaneous/static-sites-on-dokku/#debugging_1","title":"Debugging","text":"<p>As with the Streamlit app above, debugging is done in exactly the same way, using the two commands:</p> <pre><code># Inspect application logs\ndokku logs my_app_name -t\n# Inspect nginx logs\ndokku nginx:access-logs my_app_name -t\n</code></pre>"},{"location":"miscellaneous/static-sites-on-dokku/#the-framework","title":"The Framework","text":"<p>I have a habit of categorizing things into a \"framework\" to help me anchor how I debug things, and I hope to share my framework for domains, apps, and Dokku with you.</p> <p>Firstly, we organized our Dokku box + domain name such that the Dokku box was referenced by the domain name, while individual apps got subdomains. We got subdomains for free by configuring a <code>*</code> on the DNS provider's A records, which forwarded all sub-domains to the Dokku box.</p> <p>Secondly, we configured each app on the Dokku box to resolve which subdomain points to it. In this way, subdomains need not be set on our DNS provider.</p> <p>Thirdly, we configured both static sites and dynamic data apps, using a collection of configuration files. For our data apps, it was primarily a <code>Procfile</code> and <code>requirements.txt</code>. For our static sites, it was a <code>.buildpacks</code> file, <code>app.json</code> file, and <code>requirements.txt</code>. Each have their purpose, but together they tell Dokku how to configure the environment in which apps are built.</p>"},{"location":"miscellaneous/static-sites-on-dokku/#cheatsheet-of-commands","title":"Cheatsheet of Commands","text":"<p>Here's a cheatsheet of commands we used in this essay, to help you with getting set up.</p>"},{"location":"miscellaneous/static-sites-on-dokku/#domain-name-registration","title":"Domain Name Registration","text":"<ul> <li>Register your domain.</li> <li>Add a custom resource record <code>*</code> pointing to your Dokku box's IP address</li> </ul>"},{"location":"miscellaneous/static-sites-on-dokku/#streamlit-commands","title":"Streamlit Commands","text":"<pre><code># Run streamlit app\nstreamlit run app.py\n</code></pre>"},{"location":"miscellaneous/static-sites-on-dokku/#git-commands","title":"Git commands","text":"<pre><code># Add dokku remote\ngit remote add dokku dokku@mydomain.com:streamlit-app\n\n# Push master branch to Dokku box\ngit push dokku master\n</code></pre>"},{"location":"miscellaneous/static-sites-on-dokku/#interacting-with-proxies","title":"Interacting with proxies","text":"<pre><code># View port forwarding for app\ndokku proxy:report streamlit-app\n\n# Set port forarding for app\ndokku proxy:ports-set streamlit-app http:80:8501\n</code></pre> <p>The syntax for the ports is:</p> <pre><code>&lt;protocol&gt;:&lt;host port&gt;:&lt;container port&gt;\n</code></pre> <p>For port forwarding, if you follow the general framework we're using here, you should only have to configure the container port.</p>"},{"location":"miscellaneous/static-sites-on-dokku/#interacting-with-domains","title":"Interacting with domains","text":"<pre><code># View domains for an app\ndokku domains:report streamlit-app\n\n# Set domains for an app\ndokku domains:set streamlit-app streamlit-app.mydomain.com\n</code></pre> <p>Again, if you follow the framework we have used here, then you should only need to configure <code>&lt;app-url&gt;.mydomain.com</code></p>"},{"location":"miscellaneous/static-sites-on-dokku/#config-files","title":"Config Files","text":"<p><code>Procfile</code> for apps</p> <pre><code>web: streamlit run app.py\n</code></pre> <p><code>mkdocs.yml</code> for MkDocs config</p> <pre><code># mkdocs.yml\nsite_name: Marshmallow Generator\n</code></pre> <p>Create <code>.static</code> for static sites:</p> <pre><code>touch .static\n</code></pre> <p><code>.buildpacks</code> for static sites and multi-buildpack apps</p> <pre><code>https://github.com/heroku/heroku-buildpack-python.git\nhttps://github.com/dokku/buildpack-nginx.git\n</code></pre> <p><code>app.json</code> for static sites:</p> <pre><code>{\n\"scripts\": {\n\"dokku\": {\n\"predeploy\": \"cd /app/www &amp;&amp; mkdocs build\"\n}\n}\n}\n</code></pre> <ol> <li> <p>I am paying for the $10/mo plan for the extra RAM seems to help.\u00a0\u21a9</p> </li> </ol>"},{"location":"newsletter/2020/05-may/","title":"Data Science Programming May 2020 Newsletter","text":"<p>Hello fellow datanistas!</p> <p>Here\u2019s the May 2020 edition of my newsletter. I\u2019m trying out a slightly different formatting; as always, though, I hope you find it useful. If you have feedback, do send it my way!</p>"},{"location":"newsletter/2020/05-may/#multi-task-learning-in-the-wild","title":"Multi-Task Learning in the Wild","text":"<p>I recently watched Andrej Karpathy\u2019s talk on multi-task learning, and I learned a ton. When you\u2019re faced with hardware constraints, how do you tweak your ML model to get better at more tasks? Check out his talk to learn more.</p>"},{"location":"newsletter/2020/05-may/#gaussian-processes-explained-quite-simply","title":"Gaussian processes explained quite simply","text":"<p>I\u2019ve been a fan of Gaussian Processes for non-linear predictive modeling tasks, especially when writing a neural network feels too much for the small-ish data that I have. Learning about them wasn\u2019t easy though. That said, there\u2019s a wonderful blog post from Yuge Shi, a PhD student at Oxford, explaining GPs in a pretty intuitive fashion. She put in enough pictures to accompany the math that you should find it an enjoyable read!</p>"},{"location":"newsletter/2020/05-may/#preview-of-an-exciting-development-using-dask","title":"Preview of an exciting development using Dask","text":"<p>If you\u2019re a Dask user, this next video preview is going to be music to your ears. Matthew Rocklin, lead developer of Dask and founder of Coiled Computing (which is providing support for Dask and more) just showed us spinning Dask clusters in the cloud from a laptop, getting us to interactive-scale compute. This is the dream: burstable, interactive-time, portable, large-scale computing from my laptop to the cloud with minimal config! Check out the screencast he made for a preview!</p>"},{"location":"newsletter/2020/05-may/#scipy-2020","title":"SciPy 2020","text":"<p>Virtual SciPy 2020\u2019s schedule has been released! The conference has been a wonderful place to learn about the latest in data science and scientific computing tooling. Come check out the schedule here[scipy]. I will be there presenting a tutorial on Bayesian statistics; hope to see you there!</p>"},{"location":"newsletter/2020/05-may/#faster-pandas-applies-with-swifter","title":"Faster pandas applies with swifter","text":"<p>While seeking out faster ways to do <code>pandas</code> applies, I learned about a new tool, called <code>swifter</code>! It automatically finds the fastest way to apply a pandas function. Fits very nicely into the paradigm of \u201cdo one thing and only one thing well\u201d. Check out the GitHub repository and let me know what you think of it! I will be experimenting with it on <code>pyjanitor</code> to see whether it does a better job with speeding up some of the functions in there.</p>"},{"location":"newsletter/2020/05-may/#from-my-collection","title":"From my collection","text":""},{"location":"newsletter/2020/05-may/#sane-path-management-in-your-project-directory","title":"Sane path management in your project directory","text":"<p>I recently wrote a little post about how we can use Python\u2019s pathlib to make file paths a little more sane in our projects. <code>pyprojroot</code>, the tool I feature in the post, was developed by one of my Python conference dopplegangers, Daniel Chen, who has this wonderfully ironic habit of doing book giveaways and signings of his Pandas book at R conferences :).</p>"},{"location":"newsletter/2020/05-may/#updates-to-our-network-analysis-tutorial","title":"Updates to our network analysis tutorial!","text":"<p>Finally, with my collaborator Mridul Seth (who runs the GSoC program with NumFOCUS), we\u2019ve been updating our Network Analysis Made Simple repository! My Patreon supporters will get early access to the tutorial repository before its public launch later in the year, so if you like it, please consider sending a cup of coffee each month! Your support would go a long way to supporting the creation and maintenance of this teaching material! (Up next will be Bayesian content - on probabilistic programming - just in time for SciPy 2020!)</p> <p>Stay safe and enjoy the sunshine! Eric</p>"},{"location":"newsletter/2020/06-june/","title":"Data Science Programming June 2020 Newsletter","text":"<p>Hello datanistas!</p> <p>We're back with another edition of the programmer-oriented data science newsletter. This month, I have so much I've learned and to share, so I'm thoroughly excited to be writing this newsletter edition!</p>"},{"location":"newsletter/2020/06-june/#python-39-beta","title":"Python 3.9 Beta!","text":"<p>First things first, Python 3.9's latest beta has been released! There are new language features in there, including:</p> <ol> <li>New dictionary operators</li> <li>A topological sorter class in functools</li> <li>A \"least common multiple\" (<code>lcm</code>) function in the math library,</li> <li>And the best of them all: <code>string.removeprefix('prefix_goes_here')</code> and <code>string.removesuffix('suffix_goes_here')</code>! This is a serious convenience piece for those of us who work with files!</li> </ol> <p>Check out Martin Heinz' blog post on Medium to learn more!</p>"},{"location":"newsletter/2020/06-june/#learn-through-the-data-science-design-manual","title":"Learn Through The Data Science Design Manual","text":"<p>During this extraordinary COVID-19 time, Springer did an extraordinary thing that I never expected: They released a whole bucketload of books for free online! One of them caught my eye: \"The Data Science Design Manual\". Having browsed through the book PDF, I'm impressed by its coverage of the foundational topics that I think every data scientist should be equipped with: statistical inference, data wrangling, linear algebra, and machine learning. The author, Steven Skiena, also covers more in there.</p> <p>Go grab the PDF while it's still free!</p>"},{"location":"newsletter/2020/06-june/#easy-matplotlib-animations","title":"Easy <code>matplotlib</code> animations","text":"<p>Recently, <code>celluloid</code>caught my eye: it's a package that lets you create <code>matplotlib</code> animations easily!</p> <p>If you need a dead-simple example to convince you to check it out, here's one lifted straight from the repository:</p> <pre><code>from matplotlib import pyplot as plt\nfrom celluloid import Camera\n\nfig = plt.figure()\ncamera = Camera(fig)\nfor i in range(10):\n    plt.plot([i] * 10)\n    camera.snap()\nanimation = camera.animate()\n</code></pre> <p>But seriously though, if you use the workhorse Python drawing package <code>matplotlib</code> for anything, this package can be considered to be one of those \"great tricks to have\" in your bag!</p>"},{"location":"newsletter/2020/06-june/#better-design-skills-points-of-view","title":"Better Design Skills: Points of View","text":"<p>Continuing the theme of visualization, I wanted to share with you a resource from Nature Methods that has influenced the entirety of how I approach data visualization and figure design. This is the Points of View series, written by Bang Wong and Martin Krzywinski and many other co-authors. The entire series is available online, and is a valuable resource to read.</p> <p>Two fun tidbits: I devoured the entire series while doing my doctoral training, eagerly awaiting each new release like a Netflix addict. And I was thoroughly thrilled when Bang decided to join the department I work in at NIBR! Imagine getting to work with your grad school hero :).</p>"},{"location":"newsletter/2020/06-june/#better-software-skills-document-your-tests","title":"Better Software Skills: Document your tests!","text":"<p>For those of you who know me, I am a strong proponent of data scientists being equipped with good, basic software skills. When we write code in a \"basically good\" way (refactored, documented and tested), we accelerate our productivity many-fold. One of my interns reminded me of this when we realized that something that would have otherwise taken days to get right in SQL ended up being 10 minutes of work because we documented and tested our pandas DataFrame caches. (If you wish to read more about testing, I write about it on my Essays on data science.)</p> <p>Documenting code is important. Turns out, your test suite is also code! So in his blog post, Hyne Schlawack makes the argument that we ought to document our tests, something that has become painfully obvious in some of the projects I have worked on. His blog post, then, gets an absolute strong recommendation from me!</p>"},{"location":"newsletter/2020/06-june/#work-anywhere-with-development-containers","title":"Work Anywhere with Development Containers","text":"<p>For those of you who, like myself, moonlight as a software engineer because you develop tools, this next piece might come as music to your ears: Visual Studio Code has superb support for developing a project inside a Docker container. If you try it out, I guarantee you the convenience of never having to get someone else set up with development instructions will be liberating.</p> <p>Since finding out about it on Thursday (28 May), I've enabled dev containers on my personal website, my Essays collection, and the pyjanitor project. In each case, Dockerhub automatically builds containers on every commit to the default branch, and those containers are referenced in the dev container configuration file, which means your local machine never has to build the container, you only have to pull it down! I also got everything working remotely, so my puny little 12\" MacBook now uses a remote GPU-enabled development server. Speaking of which, if you're interested in making an open source contribution, or wish to just test-drive dev containers on an actual project, check out the docs I wrote for the pyjanitor project!</p>"},{"location":"newsletter/2020/06-june/#automate-workflow-with-continuous-x","title":"Automate Workflow with Continuous X","text":"<p>I first saw what \"Continuous X\" meant when I made my first pull requests to the <code>matplotlib</code> project, and was hooked ever since. Having a continuous pipeline runner like Travis or Jenkins or Azure Pipelines automatically run code and style checks on every commit takes a ton of drudgery out of guaranteeing that our software works properly. It's like having a Roomba go through your kitchen every time it knows you've finished a meal. How does \"continuous X\" apply for data science projects though?</p> <p>Turns out, individuals way more experienced than myself and much smarter than me have been thinking about this problem too. In particular, I want to highlight two articles, one by Danilo Sato, Arif Wider and Christoph Windheuser and one on Booklet.ai. In both cases, they raise possible ways to integrate pipeline-based automation into data projects, making them robust and reproducible. Be sure to check the articles out!</p>"},{"location":"newsletter/2020/06-june/#from-my-collection","title":"From My Collection","text":"<p>I have two articles from my own collection to share.</p> <p>The first one is about how to set up a personal platform as a service (PaaS) called Dokku. It's written for those who are especially cheap (like yours truly) and don't want to pay $7/month to Heroku for each project that gets hosted there. For those of you who do want to learn the basics of Heroku-based deployment, I have a class on Skillshare that you can use too, which is being used by the Insight Data Science Fellows in Boston!</p> <p>The second one is about a hack to speed up data loading, using a package called <code>cachier</code>. It's a neat hack - especially if you wrap specific data queries from a database into a Python function!</p>"},{"location":"newsletter/2020/06-june/#take-a-break-have-a-bit-of-humour","title":"Take a break, have a bit of humour","text":"<p>Let's close with some humorous stuff, if not at least to lighten the mood in these tumultuous times.</p> <p>Firstly, Hossein Siamaknejad actually did it: automate a game using Python. And the hack was absolutely brilliant: \"RGB detection and programmatically controlled mouse and keyboard\". Props to you, Hossein!</p> <p>Secondly, the prolifically-hilarious Kareem Carr writes about \"practicing safe.... modelling\".</p>"},{"location":"newsletter/2020/06-june/#happy-ahem-modelling","title":"Happy, ahem, modelling :)","text":"<p>Hope you enjoyed this edition of the programmer-oriented data science newsletter! As always, let me know on Twitter if you've enjoyed the newsletter, and I'm always open to hearing about the new things you've learned from it.</p> <p>If you'd like to get early access to new written tutorials, essays, 1-on-1 consulting (I just did one session with one of my supporters!) and complimentary access to the Skillshare workshops that I make, I'd appreciate your support on Patreon!</p> <p>Stay safe, stay indoors, and keep hacking!</p> <p>Cheers, Eric</p>"},{"location":"newsletter/2020/07-july/","title":"Data Science Programming July 2020 Newsletter","text":"<p>Hello datanistas!</p> <p>Welcome to the July edition of the programming-oriented data science newsletter.</p> <p>I usually try to send the newsletter on the first Monday of the month, but this edition is a little bit later than usual, and that\u2019s because I was attending SciPy 2020\u2019s virtual conference this month! Be sure to catch the videos on Enthought\u2019s YouTube channel next week, when they are edited and uploaded! (The talks are already up, check them out!)</p> <p>Back to regular scheduled programming (*cough cough the SciPy puns cough*), this month\u2019s newsletter focuses on production ML systems and everything around it!</p>"},{"location":"newsletter/2020/07-july/#on-getting-ml-models-into-production","title":"On getting ML models into production","text":"<p>Vicki Boykis has this very well-written article titled \"Getting machine learning to production\". In there, she details a lot of the struggle in getting an ML model into a production system. I found it very instructive to read. As it turns out, your ML model is kind of the least of your worries. I won\u2019t spoil it for you - take a good 10 minutes out of your day to read it!</p>"},{"location":"newsletter/2020/07-july/#mlops","title":"MLOps","text":"<p>Related to ML in production is the term that is quickly becoming \"a thing\": MLOps. In the same vein as DevOps, DevSecOps etc., it\u2019s all about continuously running things to check for reproducibility of your analyses, and at least ensuring that the code continuously runs. (Checking that everything is semantically correct is still a human job that can\u2019t be eliminated.)</p> <p>GitHub has put together a resource to help you learn about some of the tooling to help you facilitate the automation, collaboration, and reproducibility in your ML workflows.</p> <p>If anything, I have found at work that continuously executed pipelines are the basic unit of engineering reliability into both my software and my models, and I\u2019d encourage you to do the same!</p>"},{"location":"newsletter/2020/07-july/#approach-your-data-with-a-product-mindset","title":"Approach Your Data with a Product Mindset","text":"<p>This one comes from the Harvard Business Review. Usually the HBR is a tad too suit-oriented for my tastes, but having been involved in some data products at work, this article resonated with me. Production systems usually imply something that directly impact decision-making, and \"data products\" are what help facilitate/accelerate that process. Especially if there\u2019s a focus on \"unmet needs\", that\u2019s when a data + model project can turn into something impactful. Let me not spoil the read for you, and instead come check out the article here. I hope it gives you inspiration for your work!</p>"},{"location":"newsletter/2020/07-july/#on-technical-debt","title":"On Technical Debt...","text":"<p>If you\u2019ve read the paper titled \"Hidden Technical Debt in Machine Learning Systems\", then come read an article by Matthew McAteer, in which he dissects the paper and teases out which points have been made obsolete as time progressed. It\u2019s an eye-opening read!</p>"},{"location":"newsletter/2020/07-july/#assortments-of-goodies","title":"Assortments of Goodies","text":"<p>Some other things I have found to be important and informative include:</p> <ul> <li>The proposal of a DataFrame protocol for the PyData ecosystem</li> <li>A succinct introduction to metamorphic testing</li> <li><code>pbcopy</code> and <code>pbpaste</code>, a macOS utility for copying things to the clipboard from the terminal</li> <li>and what I would consider to be Coiled Computing\u2019s manifesto! (To be clear, they did not pay me to put this link in here, I\u2019m genuinely excited about what they\u2019re building!)</li> </ul>"},{"location":"newsletter/2020/07-july/#from-my-collection","title":"From my collection","text":"<p>Now for some things from my own collection that I\u2019m excited to share!</p>"},{"location":"newsletter/2020/07-july/#network-analysis-made-simple","title":"Network Analysis Made Simple","text":"<p>Each year, I submit Network Analysis Made Simple to PyCon, SciPy and PyData conferences, where they get recorded and are shared with the world for free. This year, I\u2019m super happy to announce that my co-instructor and I have revamped the website! We spent some time restructuring the material, adding a theme that provides search, and adding a pipeline that reproducibly builds the notebook collection. For those of you who like eBook artifacts to keep, we also compiled a book! If you\u2019re interested in it, come tell us what you think the book is worth. We\u2019ll be officially launching next week, after the final chapter is added to the collection!</p> <p>(Bayesian Data Science by Simulation and Probabilistic Programming is also undergoing a similar rebuild, stay tuned!)</p> <p>A few colleagues have also given me feedback that the Python data science ecosystem is kind of like \"the Wild Wild West\". Reflecting on my prior experience thus far, I can appreciate the sentiment, and so I sat down and wrote a long essay that tries to linearize/untangle the ecosystem for newcomers. I hope it\u2019s useful for you too :). My Patreon supporters have had early access to the article for a while, so if you appreciate the work, I\u2019d love to hear from you on Patreon!</p>"},{"location":"newsletter/2020/07-july/#moar-twitter","title":"Moar Twitter","text":"<p>Have you tried to unsubscribe from a email list and got the response that it can \"take a few days\"? Well... follow this thread to learn why! (I\u2019d love it if you\u2019d stay with this newsletter though!)</p>"},{"location":"newsletter/2020/08-august/","title":"Data Science Programming August 2020 Newsletter","text":"<p>Hello fellow datanistas!</p> <p>Welcome to the August edition of the programming-oriented data science newsletter!</p> <p>This edition of the newsletter has a large dose of SciPy 2020 inside it. I participated in the conference as a tutorial instructor (and as one of the Financial Aid chairs), though I did also miss Austin TX food! (Seriously, Texan BBQ is one of the best!) If you're interested in seeing the whole playlist, check it out on YouTube! If not, come check out some of those that I've watched and liked, my subset curated for you!</p>"},{"location":"newsletter/2020/08-august/#frictionless-data-for-reproducible-biology-by-lilly-winfree","title":"Frictionless Data for Reproducible Biology by Lilly Winfree","text":"<p>The reason I like this talk is primarily because of the idea of \"Data Packages\", where raw data and its metadata are packaged in a machine-readable format. In my mind, I'm contrasting this idea against the large-scale data collection efforts; in biosciences, many datasets are small and designed for one question, but may be useful for other problems by providing, for example, useful priors on parameters. Here, a data package helps users ship and distribute a self-contained unit of data that others can build on top of. I'm imagining many cool use cases, both in public-facing research and in internal-facing workflows!</p>"},{"location":"newsletter/2020/08-august/#continuous-integration-for-scientific-python-projects-by-stanley-seibert","title":"Continuous Integration for Scientific Python Projects by Stanley Seibert","text":"<p>In this talk, Stan Seibert (one of the Numba core developers) speaks about the advantages of standing up a continuous integration pipeline for your code, as well as challenges that you'll encounter along the way. I find this to be a useful video for data scientists, because in it Stan gives a good overview of what to look out for.</p>"},{"location":"newsletter/2020/08-august/#awkward-array-manipulating-json-like-data-with-numpy-like-idioms-by-jim-pivarski","title":"Awkward Array: Manipulating JSON like Data with NumPy like Idioms by Jim Pivarski","text":"<p>This one has to be one of my favourite talks, because the package featured in there has an awesome name, brings over NumPy idioms and semantics into world of nested and \"awkwardly\"-structured data.</p>"},{"location":"newsletter/2020/08-august/#jax-accelerated-machine-learning-research-by-jake-vanderplas","title":"JAX: Accelerated Machine Learning Research by Jake Vanderplas","text":"<p>I'm a fan of the NumPy API because it's the array-computing lingua franca of the Python world, and I strongly believe that targeting a common API (and evolving it in a community-oriented fashion) is the right way to build the PyData ecosystem. JAX does this by making array-oriented automatic differentiation, GPU/TPU acceleration, just-in-time compilation, and vectorized mapping all first-class citizens alongside the idiomatic NumPy API. I love it and totally dig it! And I use it for research and production at work. I'd encourage you to try it out too!</p>"},{"location":"newsletter/2020/08-august/#matplotlib-data-model-by-hannah-aizenman","title":"<code>matplotlib</code> Data Model by Hannah Aizenman","text":"<p>If you use <code>matplotlib</code>, then this Maintainer's track talk by Hannah Aizenman is going to make your eyes light up! In here, she talks about CZI-funded work to refactor the data model underneath <code>matplotlib</code>, which will enable a ton of really cool things downstream. I'm not going to spoil it for you; check it out! (And also check out the other cool talks by the other maintainers!)</p>"},{"location":"newsletter/2020/08-august/#interactive-supercomputing-with-jupyter-at-nersc-by-rollin-thomas","title":"Interactive Supercomputing with Jupyter at NERSC by Rollin Thomas","text":"<p>I think this is a great case study talk that shows how JupyterHub is used at a research institution to help facilitate computational research. If your organization is thinking about setting something up, I think this talk will give you valuable insights and lessons!</p>"},{"location":"newsletter/2020/08-august/#tutorials","title":"Tutorials","text":"<p>If I really wanted to, I would have listed all 10 tutorials down here amongst my recommendations, but I know you came for a curation. Here's the two that I think are most generally useful for data scientists:</p> <ul> <li>Introduction to Conda for (Data) Scientists: This being such a foundational tool for distributing data science packages, I think it's work getting our mental models straightened out!</li> <li>Jupyter Interactive Widget Ecosystem: With Jupyter notebooks being so idiomatic, and with widgets being so useful for dashboarding, pedagogy and more, this one is an easy recommendation!</li> </ul> <p>But seriously, check out all 10 of them!</p>"},{"location":"newsletter/2020/08-august/#from-my-collection","title":"From my collection","text":"<p>Here's a few snippets of my participation this year at SciPy!</p> <ul> <li>Call prediction, prediction, not inference! (My ~~rant~~ lightning talk at SciPy.)</li> <li>Bayesian Data Science by Simulation (tutorial I led, based on material I co-developed with Hugo Bowne-Anderson!)</li> </ul> <p>In some other news, the Network Analysis Made Simple eBook has launched! In line with my personal philosophy of democratizing access to learning material, everything is freely available online, but if you'd like to support us (mostly by keeping us caffeinated) or would like an offline copy to keep that will be kept up-to-date for life, please consider purchasing a copy!</p>"},{"location":"newsletter/2020/09-september/","title":"Data Science Programming September 2020 Newsletter","text":"<p>Hello fellow datanistas!</p> <p>Welcome to the September edition of the programming-oriented data science newsletter. I hope you've all been staying safe amid the COVID-19 outbreak.</p> <p>There's no special theme this month, just a smattering of cool tools and articles that I think will improve your productivity!</p>"},{"location":"newsletter/2020/09-september/#setting-up-vscode-for-python-development-like-rstudio","title":"Setting up VSCode for Python Development like RStudio","text":"<p>Firstly, a blog post by Steven Mortimer on how to set up VSCode, which is a really awesome IDE, in such a way that it behaves like RStudio. For R users who have to transition over to Python (e.g. for work, or for personal interest), this should help bridge the gap a bit!</p>"},{"location":"newsletter/2020/09-september/#pylance-in-vscode","title":"Pylance in VSCode","text":"<p>Speaking of VSCode, I have been test-driving Pylance in my workflow at work, and it's blazing fast and performant for code checking! As I was writing my code, the Pylance VSCode extension continually checked my code, helping me to catch execution errors before I even executed the code. Amazing stuff, Microsoft, I like what you've become now :).</p>"},{"location":"newsletter/2020/09-september/#ecdfs-are-in-seaborn","title":"ECDFs are in Seaborn!","text":"<p>Since learning about ECDFs a few years ago, I have advocated for visualizing distributions of data using ECDFs rather than histograms. Well, nothing beats having best practices available conveniently, so I'm super happy to see ECDFs conveniently available in seaborn!</p>"},{"location":"newsletter/2020/09-september/#stupid-simple-kubernetes","title":"Stupid Simple Kubernetes","text":"<p>From experience at work, I can vouch for the idea that it's completely worthwhile for a data scientist to learn the ideas around containers, Kubernetes included. To help get up to speed, my colleague Zach Barry found an awesome article to help, titled \"Stupid Simple Kubernetes\". Lots of terms in the K8 world get clarified in that article. I hope you enjoy it!</p>"},{"location":"newsletter/2020/09-september/#learn-in-public","title":"Learn in Public","text":"<p>This is an article that resonated deeply with me. Learning in public has been, for me, the biggest career hack that I have experienced. Now, Shawn Wang has articulated clearly the benefits of doing so! The biggest is being able to build a public-facing portfolio that you can point to that demonstrates your skill set.</p>"},{"location":"newsletter/2020/09-september/#from-my-collection","title":"From my collection","text":"<p>Some things I recently wrote about:</p> <ul> <li>Software skills are important, for it helps us data scientists think clearly.</li> <li>Some early thoughts test-driving <code>pandera</code> for data validation.</li> <li><code>.also()</code>, which comes from the Kotlin programming language, proposed in <code>pyjanitor</code> as a new feature - I'm excited to see where this one goes!</li> <li>I'll be speaking at JupyterCon 2020 this year!   Super excited to release a talk on how we compiled Network Analysis Made Simple   into our eBook and website!</li> </ul>"},{"location":"newsletter/2020/09-september/#a-plug-for-an-awesome-open-source-contributor","title":"A plug for an awesome open source contributor","text":"<p>The final thing I'd like to include in this newsletter is a completely unsolicited but heartfelt advertisement for Samuel Oranyeli. He's been a consistent contributor to the <code>pyjanitor</code> project, and I have witnessed his skills growth over the past few months of contribution. The most important quality he possesses is consistent learning! If you're hiring for a Python developer in the Sydney, Australia area or remotely, do consider him on your list!</p>"},{"location":"newsletter/2020/10-october/","title":"Data Science Programming October 2020 Newsletter","text":"<p>Hello fellow datanistas!</p> <p>Welcome to the October edition of the programming-oriented data science newsletter. As the weather chills down, hope you all are staying warm indoors, and safe both indoors and outdoors!</p> <p>This edition of the Data Science Programming Newsletter has a particular focus on machine learning engineering, which is a discipline that is evolving out of the old \"data science\" umbrella into its own.</p>"},{"location":"newsletter/2020/10-october/#effective-testing-for-machine-learning-systems","title":"Effective testing for machine learning systems","text":"<p>Recently at work, I've been building some bespoke machine learning models (autoregressive hidden Markov models and graph neural networks) for scientific problems that we encounter. In building those bespoke models, because we aren't using standard reference libraries, we have to build the model code from scratch. Since it's software, it needs tests, and Jeremy Jordan has a great blog post on how to effectively test ML systems. Definitely worth a read!</p>"},{"location":"newsletter/2020/10-october/#software-engineering-fundamentals-for-data-scientists","title":"Software engineering fundamentals for Data Scientists","text":"<p>In his Medium article, Gonzalo Ferreiro Volpi shares some fundamentals software skills for data scientists. For those of you who want to invest in levelling up your code-writing skills to reap multiplicative dividends in time saved, frustrations avoided, and happiness multiplied, come check it out!</p>"},{"location":"newsletter/2020/10-october/#reflecting-on-a-year-of-making-machine-learning-actually-useful","title":"Reflecting on a year of making machine learning actually useful","text":"<p>In her blog post, Shreya Shankar has some extremely valuable insights into the practice of making ML useful in the real world, which I absolutely agree with. One, in particular, being the quote:</p> <p>Outside of ML classes and research, I learned that often the most reliable way to get performance improvements is to find another piece of data which gives insight into a completely new aspect of the problem, rather than to add a tweak to the loss. Whenever model performance is bad, we (scientists and practitioners) shouldn\u2019t only resort to investigating model architecture and parameters. We should also be thinking about \u201cculprits\u201d of bad performance in the data.</p> <p>Reminds me of the power of finding \"the invariants\" of a problem. With that little teaser, I hope this gives you enough impetus to read it!</p>"},{"location":"newsletter/2020/10-october/#the-multiplicative-power-of-masks","title":"The Multiplicative Power of Masks","text":"<p>This article is one that is topical and relevant. I also appreciated the illustrations put in there! Also, it's a blog post that highlights a really powerful model -- where powerful doesn't mean millions of parameters, but rather conceptually simple, easy to communicate, broadly applicable, and intensely relevant for the times. Aatish Bhatia has done a tremendously wonderful job here with this explanation. It's a technical masterpiece.</p>"},{"location":"newsletter/2020/10-october/#from-my-collection","title":"From my collection","text":"<ul> <li>Some colleagues had questions about environment variables, so I decided to surface up an old post on the topic and spruce it up with more information on my essays collection.</li> <li>I moved data across work sites securely and as fast a commercial tools using nothing but free and open source tooling. Come read how!</li> <li>I also recently figured out how to directly open a Jupyter notebook in a Binder session. The hack is really cool!</li> <li>Finally, some more Twitter humour from the ever on fire Kareem Carr :).</li> </ul>"},{"location":"newsletter/2020/11-november/","title":"11 november","text":"<p>Hello fellow datanistas,</p> <p>Welcome to the November 2020 edition of the Data Science Programming newsletter. Each newsletetter edition, I hope to bring you something new that is of value for your learning as a data scientist.</p> <p>This past month, I stumbled across Andy Matuschack's notes, and found one of them to be super inspiring: Working with the garage door up. As such, I wanted to highlight some \"open garage door learnings\" from data practitioners in my circle, which I hope sparks more \"open garage door learnings\" from you too.</p>"},{"location":"newsletter/2020/11-november/#learning-on-functional-programming-with-r","title":"Learning on functional programming with R","text":"<p>This one is by Apoorva Srinivasan, a recent graduate of Columbia University whom I met at a hackathon organized by the company I work for. In her blog post, she touches on some really important pointers on why, as data scientists, we might want to level-up our programming skills. One of my favourite lines from her post</p> <p>However, as you take on increasingly complex projects, you may find yourself thinking about more and more about structuring your project well and writing code that is easy to understand, debug, reuse, and maintain.</p> <p>Be sure to check out the rest of her post here!</p>"},{"location":"newsletter/2020/11-november/#why-have-a-data-science-portfolio-and-what-it-shows","title":"Why Have a Data Science Portfolio and What It Shows","text":"<p>Eugene Yan writes extensively on the topic of data science careers, and I particularly enjoyed the essay he wrote titled Why Have a Data Science Portfolio and What It Shows. A lot of what he has written mirrors what I experienced working with my own garage door open, and for those looking to break into a career (not just data science), having a curated and constantly updated public-facing portfolio is a wonderful way to document your learnings, share them with the world, and demonstrate the non-technical trait of lifelong learning.</p>"},{"location":"newsletter/2020/11-november/#samuel-oranyelis-data-wrangling-blog","title":"Samuel Oranyeli's Data Wrangling Blog","text":"<p>Right up the alley of learning in the open is Sam's blog on data wrangling. Even with my 6 years of <code>pandas</code> experience, I found myself learning something new from Sam's writings. In there, you'll find lots of practical advice for those who are interested in practical wrangling of messy spreadsheet data that come to us from collaborators, and his writings are directly in the spirit of learning in the open.</p>"},{"location":"newsletter/2020/11-november/#git-scraping","title":"Git Scraping","text":"<p>Simon Willison, one of my heroes for giving us Datasette, writes about git scraping on his blog. The idea is to be able to track public sources of data over time, which sometimes is more interesting than each snapshot individually.</p> <p>An example of git scraping in action is a tracker for the battleground states in this year's US Presidential elections. Made by GitHub user alex, the website is found on GitHub pages, while the repo can be found on GitHub. Apart from being an incredibly useful real-time tracker of the election counts this year, in my opinion, it also served as a great example of building (and learning) in the open.</p>"},{"location":"newsletter/2020/11-november/#the-myths-and-traps-of-managing-up","title":"The myths and traps of managing up","text":"<p>This comes from a developer blog, but having read the post, I think the content is relevant for data scientists too, both being technical professions. One particular quote that I find important:</p> <p>The lead who is not great at managing up is also less able to sponsor others, a less useful ally to their own team, and their team has to contend with a harsher broader environment.</p> <p>After all, we as data scientists are not in the business of politics for politics sake. If we do engage in managing expectations, it's for the benefit of the whole team.</p>"},{"location":"newsletter/2020/11-november/#thank-you-for-reading","title":"Thank you for reading","text":"<p>Next edition, I am planning a newsletter centered on JAX, as the more I've used it, the more I've become excited about what we could build with it!</p> <p>I'd also like to celebrate with you all the milestone of hitting 300 newsletter subscribers - thank you to everyone who has subscribed :). Please do share the link to the subscribe page with those whom you think might benefit from it.</p> <p>As always, let me know on Twitter if you've enjoyed the newsletter, and I'm always open to hearing about the new things you've learned from it. In particular, I'm curious to hear from you too: what are you building in your open-door garage?</p> <p>Meanwhile, if you'd like to get early access to new written tutorials, essays, 1-on-1 consulting and complimentary access to the Skillshare workshops that I make, I'd appreciate your support on Patreon!</p> <p>Stay safe, stay indoors, and keep hacking!</p> <p>Cheers, Eric</p>"},{"location":"newsletter/2020/12-december/","title":"12 december","text":"<p>Hello, datanistas!</p> <p>This month is a special edition dedicated to JAX! It's a Python package built by some friends I made while they were at Harvard's Intelligent and Probabilistic Systems lab, and I was still in grad school.</p> <p>I've been a fan of JAX ever since I started seriously developing array programs that required the use of automatic differentiation. What's up with JAX, you might ask? It's a library that brings automatic differentiation and many other composable program transformations to the NumPy API.</p> <p>Why is automatic differentiation significant? The reason is that the ability to calculate the derivative of a function, w.r.t. one or more of its arguments, is essential to many computation realms. For example, we can use gradient-based optimization to train small and large models to do maximum likelihood or maximum a posteriori estimation of model parameters. Gradients are necessary for modern MCMC samplers, which leverage gradients to guide where to draw a new posterior sample next. Input design problems can also use gradient-based optimization, in which we either optimize or sample new inputs to achieve some output.</p> <p>What JAX does is it takes a function that returns a scalar value and returns the derivative of that function's output w.r.t. the inputs. JAX accomplishes this by using the <code>grad</code> function, which takes the function passed into it, and transforms it into another function that evaluates the gradient. Gradient transformations are one example of a broader class of program transformations, which take a program (e.g. a function implemented in NumPy code) and transforms it into another program (its derivative function). JAX houses other program transformations, including just-in-time compilation for speed-ups, loop-replacement functions, and more.</p> <p>Here, I'm going to highlight a sampling of the JAX projects that have come up on my radar to showcase the diversity of numerical computation projects that you can build with it. Hopefully, it'll give you some encouragement to give JAX a try if you haven't already done so!</p>"},{"location":"newsletter/2020/12-december/#neural-network-projects","title":"Neural network projects","text":"<p>Because differential programming is a broader thing than just neural networks, you can write neural networks and more using JAX. If you're not used to writing neural network models from scratch, not an issue: there are a few neural network API frontends that build on top of JAX's NumPy API, which implements PyTorch-like APIs.</p> <ul> <li><code>flax</code>: A neural network library focused on flexibility.</li> <li><code>haiku</code>: One developed by the fine folks at DeepMind, alongside their other JAX projects.</li> <li><code>stax</code>: JAX's internal experimental module for writing neural network models, which pairs well with its <code>optimizers</code> module!</li> <li><code>neural-tangents</code>: Research that I have been following, one that provides \"infinitely wide\" versions of classical neural networks. It extends the <code>stax</code> API.</li> </ul> <p>The best part of these projects? You never have to leave the idiomatic NumPy API :).</p>"},{"location":"newsletter/2020/12-december/#probabilistic-programming-projects","title":"Probabilistic programming projects","text":"<p>As someone who has dabbled in Bayesian statistical modelling, probabilistic programming is high on my watch list.</p> <p>The first one I want to highlight is PyMC3. More specifically, Theano. One of our PyMC devs, Brandon Willard, had the foresight to see that we could rewrite Theano to compile to JAX, providing a modernized array computation backend to Theano's symbolic graph manipulation capabilities. It's in the works right now! Read more about it on a blog post written by the PyMC devs.</p> <p>The second one I want to highlight is NumPyro, a JAX-backed version of the Pyro probabilistic programming language. A collection of Pyro enthusiasts built NumPyro; one of its most significant selling points is implementing the No-U-Turn Sampler (NUTS) in a performant fashion.</p> <p>The third one I want to highlight is <code>mcx</code>, a learning project built by Remi Louf, a software engineer in Paris. He has single-handedly implemented a probabilistic programming language leveraging JAX's idioms. I had the privilege of chatting with him about it and test-driving early versions of it.</p>"},{"location":"newsletter/2020/12-december/#tutorials-on-jax","title":"Tutorials on JAX","text":"<p>Here are two tutorials on JAX that I have encountered, which helped me along the way.</p> <p>Colin Raffel has a blog post on JAX that very much helped me understand how to use it. I highly recommend it!</p> <p>Eric Jang has a blog post on meta-learning, with accompanying notebooks linked in the post, that show how to do meta-learning using JAX.</p> <p>Beyond that, the JAX docs have a great tutorial to help get you up to speed.</p>"},{"location":"newsletter/2020/12-december/#from-my-collection","title":"From my collection","text":"<p>As I've experimented with JAX and used it in projects at work, here are things I've had a ton of fun building on top of JAX.</p> <p>The first is <code>jax-unirep</code>, done together with one of my interns Arkadij Kummer, in which we took a recurrent neural network developed by the Church Lab at Harvard Medical School and accelerated it over 100X using JAX, while also extending its API for ease of use. You can check out the pre-print we wrote as well.</p> <p>The second is a tutorial on differential programming. This one is one I'm continually building out as I learn more about differential programming. There are a few rough edges in there post-rewrite, but I'm sharing this early in the spirit of working with an open garage door. In particular, I had a ton of fun walking through the math behind Dirichlet process Gaussian mixture model clustering.</p>"},{"location":"newsletter/2020/12-december/#thank-you-for-reading","title":"Thank you for reading","text":"<p>I hope you enjoyed this JAX edition of the Data Science Programming Newsletter! Next month, we resume regular scheduled, ahem, programming :). If you've enjoyed this newsletter, please do share the link to the newsletter subscribe page with those whom you think might benefit from it.</p> <p>As always, let me know on Twitter if you've enjoyed the newsletter, and I'm always open to hearing about the new things you've learned from it. Meanwhile, if you'd like to get early access to new content I make, I'd appreciate your support on Patreon!</p> <p>Stay safe, stay indoors, and keep hacking!</p> <p>Cheers, Eric</p>"},{"location":"people-skills/hiring/","title":"Hiring and Interviewing Data Scientists","text":"<p>Back in 2021, I hired two new teammates for my home team, the Data Science and Artificial Intelligence (Research) team at Moderna. As indicated by the \"(Research)\" label, our main collaborators are other research scientists within the company. In 2023, I will be potentially hiring for four more positions (as long as the business situation doesn't change). While I had helped hire data scientists in the past, 2021 was the first time I was primarily responsible for the role. So when I first joined, I thought I would have some time, say, half a year or so, to ease into the team lead part of the role. But by day 2, the 21st of July, my manager Andrew sent me an MS Teams message saying, \"Hey, two positions for you are approved. Merry Christmas!\" I was both surprised and excited at the time.</p>"},{"location":"people-skills/hiring/#criteria-for-hiring","title":"Criteria for Hiring","text":"<p>During this time, I had to develop the criteria for hiring. Thankfully, our Talent Acquisition (TA) team had a process in place, so I didn't have to design that half. Our department head, Dave Johnson, emphasized keeping the technical bar high because we want to hire people who can hit the ground running straight away. I also knew from previous experience that I would enjoy working with bright individuals who were:</p> <ol> <li>quick to learn,</li> <li>thorough in thinking through problems,</li> <li>capable of going deep into technical details,</li> <li>sufficiently humble to accept feedback, and</li> <li>courageous enough to challenge my views.</li> </ol> <p>I also had to specify the exact skill requirements for the new roles. I came up with a list of skills that I believed were needed for the job. One thing our Talent Acquisition team suggested surprised me. They suggested that we shouldn't hire someone who hits all of the required technical skills for that level. Instead, we should hire someone who has a majority of the skills necessary and has shown the potential to learn the rest.</p> <p>Why? It's related to our psychology. Most of us are motivated when we see a positive delta in our abilities, receive affirmation that we are improving, and can see the fruits of our labor in a finished product. Hence, this advice ensures that our new hires can hit the ground running while staying motivated longer.</p> <p>Hiring takes time and is expensive; the longer we can retain high-quality individuals within the team, the better it is for continuity, morale, productivity, and the bottom line. This news should actually encourage those who think they're not a perfect fit for the role!</p> <p>What about skills and knowledge, though? What are we looking for in our candidates? Well, here are the broad categories that I assess:</p> <ol> <li>People skills</li> <li>Communication skills</li> <li>Scientific knowledge</li> <li>Coding skills</li> <li>Modeling skills</li> </ol> <p>Later on, I will go through them in more detail. Before we discuss those, though, let's talk about the hiring process.</p>"},{"location":"people-skills/hiring/#team-interviews","title":"Team Interviews","text":"<p>We interview candidates in a \"team-based\" interview; this aligns with my experiences interviewing at Verily and the Novartis Institutes for BioMedical Research (NIBR) when I interviewed for my first data science role. As the hiring manager, I get to assemble the committee and decide what aspect each hiring committee member will be interviewing for. Usually, I would pick potential close collaborators as interviewers, which makes sense given the nature of our team's work being collaborative.</p> <p>Interviewing as a team helps ensure that we cover a wide range of perspectives and a rich set of questions covering both the technical and people dimensions.</p> <p>I was most comfortable interviewing for technical skills, so I assigned the people dimension to my colleagues. Because we are a team that works with scientists, I also asked my colleagues to assess the candidates' scientific knowledge within the interviewers' domain. The hiring committee makes a go/no-go recommendation to the hiring manager, who makes the final call on whether to hire a candidate or to continue searching. (Other companies may have a different practice.)</p> <p>With the hiring process described, let's explore those five (broad) criteria in more detail below.</p>"},{"location":"people-skills/hiring/#people-skills","title":"People Skills","text":"<p>In assessing the people dimension, we were looking for stories highlighting a candidate's ability to handle difficult interpersonal situations and, more importantly, how they turned these testing situations around for the better. Taken together, I'm always curious to hear what lessons candidates have learned for the future. When doing the interview, I would ask the candidate to tell me about a time when they were in a difficult situation and how they handled it.</p> <p>Here, then, is a set of questions that I would use to evaluate the candidate.</p>"},{"location":"people-skills/hiring/#people-skills-rubric","title":"People Skills Rubric","text":"Was the situation sufficiently non-trivial to handle? <p>Trivial situations are neither interesting nor revealing. If the candidate can offer up a non-trivial situation they handled and explain how they handled it, that is a positive sign. On the other hand, if they can only offer up trivial situations, then there is a chance they may not have sufficient battle scars to act maturely when faced with a non-trivial situation.</p> How much verifiable detail did the candidate provide in the story? <p>Details indicate ownership over the situation, and verifiability means we can check the details for truthfulness. Therefore, it's a positive sign if the candidate can offer up a story with sufficient details, such as the nature of the situation and how the people involved would be affected by their actions and decisions. Additionally, if the details are self-consistent, that's a plus sign too!</p> If applicable, how did the candidate turn around the situation for the better? <p>We are looking for tangible \"how\"s here. I would pay attention to how the candidate re-established rapport, worked to restore trust, and guide the team towards a productive outcome. With these details, we can get a sense of a candidate's toolbox for handling tough situations. Equipped with this people skills toolbox, I have a strong prior that they would also handle day-to-day situations productively.</p> Did the candidate reveal generally desirable qualities, such as humility, courage, resilience, and empathy? Or did they reveal other positive traits? Which specific details revealed these qualities? <p>We can gain an accurate picture of the candidate's character as long as the candidate provides details. Hence, details matter! Additionally, the qualities mentioned above are what we want to see in our teammates.</p> Did they reveal negative qualities instead that may turn out to be orange or red flags? What specific details revealed these qualities? <p>We need to be on the lookout for potentially negative qualities. If the candidate reveals negative attributes, we need to assess whether they are red or orange flags or whether cultural differences led to the negative perception. This is another reason why I believe a hiring committee is worth the time tradeoff; we can reduce the bias from our own cultural upbringing when interpreting a candidate's story.</p>"},{"location":"people-skills/hiring/#communication-skills","title":"Communication Skills","text":"<p>Communication skills matter. Our work is irrelevant if our collaborators do not understand it. Hence, we need to ensure that our candidates can communicate effectively to a wide range of audiences - from the technical to the non-technical, from junior members to senior leadership, and from the scientific to the business. Additionally, communication skills are essential in a senior-level candidate's ability to mentor junior team members, which is important for developing the team's capabilities.</p> <p>To assess communication skills, we often use a seminar format. Here, we ask the candidate to present a data science project they have worked on. Sometimes it is their thesis work; at other times, it is a project they have worked on in their current role. Usually, there is a 30-40 minute presentation followed by additional time, interleaved or at the end, for Q&amp;A. When assessing a candidate's communication skills, this is the rubric that I usually go for.</p>"},{"location":"people-skills/hiring/#communication-skills-rubric","title":"Communication Skills Rubric","text":"Based on the presented material, am I able to summarize the candidate's problem statement, methodology, and key findings in 3 bullet points? <p>This question places the onus on me to put in a best-faith effort to understand the candidate's presentation. If I can summarize the candidate's presentation in 3 bullet points, that's an immediate positive sign. That said, even if I can't summarize the presentation in 3 bullet points, I can use follow-up questions to attempt to understand the work better. If, after a good-faith follow-up, the candidate still cannot explain their work in a way that I can understand, only then do we have a red flag.</p> How engaged was the audience during the candidate's presentation? <p>High engagement is a positive aggregate sign; while it doesn't necessarily reveal specific traits, it usually means the candidate is able to hold the audience's attention through a combination of:</p> <ol> <li>A clear and compelling problem statement that resonates,</li> <li>A methodology that is logical and, even better, exciting to the audience,</li> <li>A conclusion that is interesting and potentially relevant to the audience, and</li> <li>Effective visual storytelling employed by the candidate.</li> </ol> Did I learn something new that was technical? Alternatively, did I see a new way of explaining an existing thing I already understood? <p>This is especially important for senior candidates and speaks to the candidate's potential to mentor junior team members. If the candidate can teach me something new or present it in a novel way that is also effective, they likely can teach others too.</p> What were the aesthetically pleasing aspects of the presentation, if any? <p>This question is subjective but important to me. I want to work with individuals who have a good sense of design and, more importantly, can execute and bring that design to life. Good aesthetics is a marker of excellence! A pleasant presentation aesthetic, such as pixel-perfectly aligned shapes, harmonized colors across all figures, and a consistent font and font size, are second-order indicators of attention to detail, which is a general quality we want in our teammates.</p>"},{"location":"people-skills/hiring/#scientific-knowledge","title":"Scientific Knowledge","text":"<p>Because we are a team that works with scientists, we also need to ensure that our candidates can communicate with scientists, which heavily implies that they need a solid scientific knowledge base.</p> <p>For junior candidates, such as those fresh from a Master's or Bachelor's program, this means they need to have sufficient knowledge over at least one domain of science, such as biochemistry, analytical chemistry, or immunology, depending on the expectations of the role.</p> <p>Senior candidates, such as those who have finished a Ph.D. or post-doc training, should know more than one domain of science. Both levels should also be able to follow along in scientific discussions and be courageous enough to raise questions when they don't understand something.</p> <p>Because our work involves working with experimental scientists, familiarity with the experiment design process is also essential. Hence, we place a premium on knowing how to design an experiment and institute appropriate controls, all while articulating the limitations of that experiment. A data scientist in our domain who has those qualities will be able to win the trust of our wet lab scientist colleagues.</p> <p>Alternatively, if the candidate has not worked on experiments before, they should be able to articulate how they would work within the \"design-make-test-analyze\" cycle of machine-learning-guided experimentation. Prior experience using machine learning methods to impact any of these steps would significantly enhance their candidacy.</p> <p>To assess scientific knowledge, if the candidate presents a seminar with a strong science flavor, we will typically use the seminar topic to evaluate their scientific knowledge. Usually, we do this by asking detailed follow-up questions. If not, I would ask them to teach me a scientific topic of their choosing. To assess experimental design knowledge, I would ask the candidate to describe a scientific experiment they helped design and see how well they could articulate the details of the experiment. Then, I would use the following questions to evaluate their response.</p> Scientific Domain Knowledge <p>While I write extensively about the need for scientific training here, that is borne out of our role as a data science team working with scientists and engineers. For other data science roles at Moderna that don't work closely with scientists and engineers, this is not as important. The same may hold true at other companies. As a hiring manager, I believe it's vital for us to ask ourselves critically how much domain knowledge we expect a candidate to come in with and how much we are willing to coach. Some domains are easier to pick up through shallow osmosis; others need deep immersion in the field.</p>"},{"location":"people-skills/hiring/#scientific-knowledge-rubric","title":"Scientific Knowledge Rubric","text":"How much scientific detail can the candidate delve into? <p>The candidate should be able to provide a high-level overview of their scientific domain and be ready to field questions on any relevant details.</p> <p>For example, if the candidate is speaking on finetuning AlphaFold for protein design, they should be able to answer questions about how proteins fold, the different levels of structure, and common patterns of how mutations affect a protein's structure, and so on. If the candidate does not have a biochemistry background, that's even better! Being able to field such questions indicates their ability to learn quickly.</p> <p>As another example, if the candidate is speaking on the use of machine learning models to prioritize molecule candidates based on an assay and can talk fluently about the experiment design, caveats, and even go as far as to identify where the assay is particularly laborious, then that is an immensely positive sign of a candidate's ability to learn and to empathize with experimental scientists.</p> How much detail did the candidate provide about an experiment they designed? Did they highlight any limitations of the experiment? <p>This question checks a candidate's proficiency in experiment design. The presence of detail helps us build confidence that the candidate actually participated in the experiment design. They should be able to describe the experimental controls and what they control for. They should be familiar with the various axes of variation in the experiment (such as time or temperature) and how we expect these axes to affect the outcome of the experiment.</p> <p>No experiment will give us the answers we need for scientific inquiry. As such, the candidate should offer up the limitations of the experiment. For example, did they highlight limitations of the controls, where physical limitations may occur in the experiment, and how they would address those limitations in a future experiment?</p>"},{"location":"people-skills/hiring/#coding-skills","title":"Coding Skills","text":"<p>Coding skill is another axis along which we assess candidates. Because \"data science can't be done in a GUI\", we need to ensure that our candidates can write both readable and maintainable code. After all, even though our primary value-add is our modeling skills, we still need to write code that surrounds the model so that we can make the model useful. Hence, it is crucial to assess a candidate's coding skills.</p> <p>How do we assess coding skills? If a candidate interests me, I will proactively look out for their profile on GitHub to see what kind of code they've put out there. One well-maintained codebase is a positive sign; not having one is not a negative, as I understand not everybody has the privilege of time to maintain a codebase.</p> <p>I have found one way to deeply evaluate a candidate's coding skills, which I would like to share here. In this interview question, I would ask the candidate to bring a piece of code they are particularly proud of to the interview. We would then go through the code together, code-review style.</p> <p>We intentionally ask candidates to pick the code they are proud of, which gives them a home-court advantage. They should be able to explain their code better than anyone else. They also won't need to live up to an imagined standard of excellence. Additionally, because this is their best work, we gain a glimpse into what standards of excellence they hold themselves to.</p> <p>During the code review, I would ask the candidate questions about the code. These are some of the questions that I would cover:</p> <ol> <li>What's the purpose of the code?</li> <li>How is it organized? Are there other plausible ways of organizing the code?</li> <li>What tradeoffs did the candidate make in choosing that particular code organization?</li> <li>Are there parts of the code that remain dissatisfactory, and if so, how would you improve it in the future?</li> </ol> <p>That 4th question is particularly revealing. No code is going to be perfect, even my own. As such, if a candidate answers \"no\" to that question, then I would be wary of their coding skills. A \"no\" answer usually betrays a Dunning-Kruger effect, where the candidate thinks they are better than they actually are.</p> <p>That said, even a \"yes\" answer with superficial or scant details betrays a lack of thought into the code. Even if the code is, in my own eyes, very well-written, I would still expect the candidate to have some ideas on where the code could be extended for a logical expansion of use cases, refactored, or better tested. If the candidate cannot provide details on these ideas, it would betray their shallow thinking about the problem for which they wrote the code.</p> <p>In the next section, I'll describe my rubric for assessing coding skills.</p>"},{"location":"people-skills/hiring/#coding-skills-rubric","title":"Coding Skills Rubric","text":"Did the candidate offer the details mentioned above without prompting? <p>This is a sign of experience; they know how to handle a code review, which we often do, and are usually confident in their knowledge of their code's strengths and weaknesses.</p> How well-organized was the code? Does it reflect idiomatic domain knowledge, and if so, how? <p>Organizing code logically is a sign of thoroughly thinking through the problem domain. Conversely, messy code usually suggests that the candidate is not well-versed in the problem domain and hence does not have a well-formed opinion on how they can organize code to match their domain knowledge. Additionally, because code is read more than written, a well-organized library of code will be easily reusable by others, thereby giving our team a leverage multiplier in the long run.</p> How well-documented was the code? In what ways does the documentation enable reusability of the code? <p>Documentation is a sign of thoughtfulness. In executing our projects, we consider the problem at hand and the reusability of the code in adjacent problem spaces. Documentation is crucial here. Without good documentation, future colleagues would have difficulty understanding the code and how to use it.</p> Did the candidate exhibit defensive behaviors during the code review? <p>A positive answer to this question is a red flag for us. We want to hire people who are confident in their skills but humble enough to accept feedback. Defensive behaviors shut down feedback, leading to a poor environment for learning and improvement.</p> How strong were reasons for certain design choices over others? <p>This question gives us a sense of the candidate's thought process. Are they thorough in their thinking, or do they rush to a solution? Because our team is building out machine learning systems, we must be careful about our design choices at each step. Therefore, if a candidate does not demonstrate thinking thoroughly through their design choices, then it means we will need to spend time and effort coaching this habit.</p>"},{"location":"people-skills/hiring/#modeling-skills","title":"Modeling Skills","text":"<p>The final aspect of our roles as data scientists is picking and building computational or mathematical models that help us solve the problems we are working on. By their very nature, models are an abstract simplification of the real world. Then, modeling skill is the artful ability to figure out what needs to be abstracted while translating that into code.</p> What do I mean by modeling skills? <p>I'd like to disambiguate what I don't mean by modeling skills. If you're used to building predictive models of the world, you might say something along the lines of, \"XGBoost is all you need for tabular data, variants of UNet for image machine learning, and a Transformer for sequence data.\" Or you might be used to trying several <code>scikit-learn</code> models and deciding on one based on cross-validation. In my mind, this is not skillful modeling; it's just following a recipe. By modeling skill, I am referring to the ability to do physics-style mechanistic modeling.</p> <p>As a team embedded in a larger scientific research organization, there is something deeply dissatisfying with only being able to make predictive models. Being able to build explicit mathematical models is what I mean by modeling skill, whether that involves the use of differential equations, state space models, or other forms of mechanistic modeling.</p> <p>Now, how do we assess modeling skills? Usually, we do this through their seminar or presentation, which is a standard part of our interview process. Usually, a candidate will present a problem they are working on, how they solved it, and its impact on their colleagues and the business they are part of. During the seminar, we would ask questions about the modeling process and use the candidates' answers to assess their modeling skills. While handy, I think the seminars may be insufficient alone; often, I have found that we need to dig deeper during the 1:1 sessions in order to tease out the necessary details to assess modeling skills.</p>"},{"location":"people-skills/hiring/#modeling-skills-rubric","title":"Modeling Skills Rubric","text":"What alternative models or modeling approaches did the candidate consider, and why were they not chosen? <p>If the candidate can provide a solid and logical reason for their final model choice while contrasting it against other choices, then it means they are familiar with a broad range of approaches, which is a sign of a well-rounded modeling skillset.</p> <p>I would expect this trait in someone who is a Data Scientist-level hire. For a Research Associate-level hire, this would be less of an expectation, but if the candidate possesses this trait, I would be impressed.</p> <p>Someone who can only come up with one model option to solve a problem needs more training in modeling.</p> <p>Finally, if the candidate can't explain why they chose a particular model, that is a red flag, and I would be wary of their modeling skills.</p> If applicable, how clear was the candidate in mapping the parameters of the model onto explainable aspects of the problem? <p>If the model involved was an explicit model of a system, such as a hidden Markov model or a linear model, I would expect the candidate to explain how the parameters of the model map onto the key questions being answered. For example, what does the slope parameter in a linear model mean for this problem? How do we interpret the transition matrix in an HMM?</p> If the candidate presented a neural network model, can the candidate explain the model architecture in detail without jargon? Can they articulate the inductive biases of the model and contrast those biases' suitability for the problem at hand? <p>In particular, I would expect the candidate to explain how they arrived at their preferred model architecture and how they ruled out other architectures. Too often, I have seen candidates pick an architecture they are familiar with or feel is popular or hot without thinking through their choice's pros and cons. (The worst I saw was using a convolutional neural network on tabular data for no good reason -- in a journal article that I reviewed.) A strong candidate would compare and contrast the inductive biases of various architectures.</p>"},{"location":"people-skills/hiring/#final-evaluation","title":"Final Evaluation","text":"<p>As hiring managers, we have tradeoffs to consider when evaluating the candidate along the dimensions above. It is exceedingly rare to find a candidate who is strong in all dimensions. Beyond raw execution ability, complementarity with the rest of the team also matters. Therefore, we must decide which dimensions we are most willing to coach on and in which dimensions we expect a candidate to be well-equipped.</p> <p>Here is an example of a set of questions I would ask myself about a candidate.</p> Am I capable of coaching them something new? <p>I'm more inclined to coach on coding skills than modeling skills, so I would expect a candidate to come in with a robust modeling skillset; my role would be to refine their ability to write software. Hence, I would prioritize candidates according to that criteria. (Other hiring managers would prioritize differently.) Additionally, I am willing to coach on communication skills but less on domain knowledge, so I expect a candidate to come in knowing their science fundamentals well.</p> Does the candidate come equipped with special modeling skills that are complementary to the team's current skill sets? <p>Beyond the role's requirements, we would also like to bring additional skill sets to the team. Doing so lets us tackle a broader range of problems (and therefore deliver more value), foster a learning environment with a diverse breadth of skills, and crucially, build a more resilient team, especially if one of our team members leaves.</p> Will the candidate work well with other teammates and collaborators? <p>For example, are they able to communicate well with others? Do they leave others feeling more confused than clear? If they display a sense of humor, does it mesh well with the team, or does it come off as abrasive?</p> <p>Do they have the confidence to execute their ideas in a group setting with a track record of successful execution to back it up?</p>"},{"location":"people-skills/hiring/#general-lessons-learned","title":"General Lessons Learned","text":"<p>Here are some general lessons that I learned from the hiring process. These should be beneficial to both interviewers and interviewees.</p>"},{"location":"people-skills/hiring/#details-matter","title":"Details matter","text":"<p>Firstly, details matter! A pattern you may see above is that I expect to see many details relevant to the skill under assessment. An apocryphal story I read before stated that details are how Elon Musk tells if a candidate is faking it. While I disagree with many of Elon Musk's ways, this is a handy lesson for interviewers and interviewees.</p> <p>For interviewers, it is vital to dig into details. The more information you can fish out, the better handle you'll have on your candidate's capabilities, and you'll be less likely to make a poor hire. For interviewees, conversely, it is important to be forthcoming with details. The more you're able to provide details to your interviewer, especially the ones critical to the job, the more accurately your interviewer will be able to evaluate you. Your challenge, as a candidate, is to be concise and detailed! You don't want to overwhelm your interviewer with irrelevant details. For both parties, the best way to avoid doing so is to treat the interview as a conversation rather than a presentation, where you share information in a back-and-forth manner.</p>"},{"location":"people-skills/hiring/#local-context-matters","title":"Local context matters","text":"<p>Secondly, the team's local context matters! I see teams as continual works-in-progress, puzzles with missing pieces. The missing piece has to fit and enhance a team. And by that fit, I don't just mean the fuzzy cultural fit that can be used as a smokescreen for rejecting a validly great candidate.</p> <p>We need to consider technical skills and what the hiring manager and the rest of the team are willing to coach. Ideally, these should be complementary for the benefit of the team and new hire.</p> <p>We also need to consider whether the candidate's mix of skills can bring a new dimension to the team's capabilities. For example, if the team needs someone with deep learning experience and a candidate also brings probabilistic modeling skills that the team currently lacks, that candidate would bring a unique new capability to the team. Therefore, we would favor that candidate over another who only brings deep learning experience.</p> <p>What does this mean for candidates? First, it can mean that even though you're thoroughly qualified for a role, you might not be the final choice because of the local context of a team.</p> <p>For example, a hiring manager may be willing to coach you on skills you're already strong in, which means they would not be as great for you as a professional development coach. (This is an essential expectation for you to have for your manager!) Therefore, from this perspective only, it would be to your benefit not to be accepted for the role and to continue searching elsewhere.</p> <p>Or it could mean that your skillsets were an excellent match, but another candidate came in with an enhancer skillset that complemented what the team already had. Of course, from this perspective, that other candidate only wowed the hiring manager and the team with their extra skillsets that you didn't have, and they will be valued accordingly. Nonetheless, you should still be confident in your capabilities and continue to search for a team to which you can be a special sauce contributor. That will give you the confidence to know that you possess something special for the team and will be valued accordingly.</p>"},{"location":"people-skills/hiring/#expectations-mapped-to-technical-level","title":"Expectations mapped to technical level","text":"<p>Where I work, we have three broad levels of technical folks, in increasing order:</p> <ol> <li>Research Associates (3 levels, including Senior and Principal)</li> <li>Data Scientists (3 levels, including Senior and Principal)</li> <li>Fellows (3 levels, including Senior and Distinguished)</li> </ol> <p>Research Associate ranks are usually for fresh graduates out of Bachelor's or Master's programs, with the Senior and Principal ranks for longer industry experiences. Data Scientists are for those newly graduated from Ph.D. programs or just finishing their post-doc training without industry experience or Master's graduates with prior industry experience in a related role. Here, the Senior and Principal ranks are generally for existing Data Scientists with a track record of accomplishment in a prior industry role, regardless of prior educational training. \"Fellow\" ranks are for highly skilled and experienced individuals who bring technical, domain, and people leadership skills to the table, with a focus on technical strategy. Think \"Staff Engineer\" for an analogy. (Thought leadership is the summary term for all three.)</p> <p>We calibrate our expectations for each level. For example, our expectations of Data Scientists, who either have extensive industry experience or have completed doctoral training, are much higher than that of a Research Associate. We expect them to be much more mature in their people skills, more polished in their communication skills, and possess greater depth and breadth of domain knowledge. On the other hand, Fellows need to be concerned with technical leadership, partner with our Digital Business Partners to align the team's work with research needs, have a firm grasp of our work's impact to research productivity, and be able to mentor and coach others on effective work practices. Their credibility for such leadership comes from their battle-tested experience.</p> <p>Summarizing my take on the expectations in a table:</p> Level Research Associate Data Scientist Fellow People Skills Good Excellent Excellent Communication Skills Great Polished TED-style :) Scientific Knowledge Single discipline Multi-disciplinary Multi-disciplinary &amp; tied to business Coding Skills Writes organized code that works Develops &amp; maintains project codebases Sets technical standards for team Modeling Skills Knows how to apply models Can develop new mathematical models Sets modeling strategy for the team, invents methods Leadership Owns individual problems Owns and directs a related suite of projects Evangelizes new technologies across business <p>When it comes to coding skills, I think we recognize that most of our candidates fresh from academia would not be as polished as those with industry experience. My experience played out uniquely; I picked up software development skills during my graduate school days, thanks to my involvement in the open-source community. However, most of my peers did not have the same experience, and I am seeing the same situation six years after graduation. Therefore, at the time of writing (January 2023), I consider any demonstrated software development skill to be a differentiating attribute for a candidate. Finally, with increasing seniority, we expect stronger opinions on structuring and organizing code, more extensive experience with code review and a more nuanced understanding of software development best practices.</p> <p>When it comes to modeling skills, I would expect a Research Associate-level candidate to be well-trained in a few modeling techniques (especially those taught in regular university curricula), know where to apply those models, and compare and contrast their use cases. On the other hand, Data Scientists should be able to apply a broader range of modeling techniques (covering both mechanistic and data-driven models), and they should be able to dive into the math behind them. With increasing seniority, candidates should also have a more opinionated philosophy on modeling strategy. For example, where to use a mechanistic model vs. a data-driven model, or, my favorite, being able to cast any modeling problem through the lens of probabilistic modeling.</p>"},{"location":"people-skills/hiring/#continual-challenges-when-interviewing","title":"Continual Challenges when Interviewing","text":"<p>There is still one area that I feel is challenging to assess: how fast a candidate can learn a new thing. I have tried asking questions, such as:</p> <ul> <li>How do you learn a new technical skill?</li> <li>What is your process for learning a new topic?</li> </ul> <p>However, I tend to get generic answers, such as:</p> <ul> <li>Asking other people.</li> <li>Reading papers.</li> <li>Trying out an example with code.</li> </ul> <p>None of those answers really get to the heart of what I'm trying to ask: how does the candidate master a new skill quickly? I probably need a better way to ask the question.</p>"},{"location":"people-skills/hiring/#qa","title":"Q&amp;A","text":"How does conducting a simulated code review compare against other common data science hiring practices, such as live coding, completing a data challenge or a Leetcode challenge? <p>Leetcode challenges are (1) gameable through practice and (2) don't correspond to a real day-to-day activity in our work.</p> <p>Data challenges, on the other hand, are (1) potentially unfair to candidates who have families, and (2) take a lot of effort to evaluate.</p> <p>While I have successfully run a data challenge and recruited an intern candidate before, I don't think it's a scalable way to hire full-timers.</p> <p>Live coding also does not reflect how we work - we usually do things independently first and then get the work reviewed later. Hence, live coding is irrelevant to us, and I choose not to evaluate candidates based on it. Moreover, for candidates who have never done live coding before, it can be stressful and may lead to a poor candidate interview experience, which will reflect poorly on us and impact our ability to recruit talented individuals.</p> Are interviews done remotely? If so, how do you assess engagement? <p>Interviews can be done remotely or in person. I usually assess audience engagement by looking at the quality of questions that the audience asks. If the audience is engaged, you'll hear lots of questions that probe the speaker's thought process. It's an aggregate sign that the candidate is matching their message to the audience well. On the other hand, lots of basic questions means that the candidate is missing crucial information in their presentation. Finally, a low-engagement audience will usually be bored by the presentation and not too many questions.</p>"},{"location":"people-skills/hiring/#conclusions","title":"Conclusions","text":"<p>This essay is the culmination of reflecting on my hiring experiences since I joined the industry in 2017. I wrote it down with three goals in mind:</p> <ol> <li>To record what I've learned from my hiring experiences for future reference,</li> <li>To help others who may find the hiring process to be opaque or mystical, and</li> <li>To serve as a conveniently sharable record of my hiring thought process for my colleagues.</li> </ol> <p>If you've found this essay useful as a hiring manager, please consider doing two things:</p> <ol> <li>Sharing it with your peers and colleagues so that they may benefit, and</li> <li>Leave me a note on the repository's discussion forum to let me know what you think.</li> </ol> <p>If, as an interviewee, you are starting to feel intimidated at the process, please reconsider your feelings! I've peeled back the curtain on what I look for in a candidate precisely to help you, the interviewee, understand better what we're looking for. It should help you better prepare for an interview, at least, for an interview with me and my team. Likewise, if you've found the essay useful, please consider sharing it with your peers, and let me know your thoughts and questions on the discussion forum.</p>"},{"location":"people-skills/hiring/#acknowledgments","title":"Acknowledgments","text":"<p>I want to thank my NIBR colleague William J. Godinez and <code>pyjanitor</code> core developer Hector E. Mu\u00f1oz for their feedback on the essay.</p> <p>I would also like to thank my Patreon sponsors for their support! If you find my work valuable, please consider sponsoring here!</p> Sponsors <p>Thank you, Ahmed, Daniel, Sat, Robert, Rafael, Fazal, Hector, Carol, and Eddie!</p>"},{"location":"people-skills/training/","title":"Training and Developing Data Scientists","text":"<p>In The Care and Feeding of Data Scientists, the authors cite a survey by Burtch Works that found that 41% of data scientists surveyed cited challenging work and learning opportunities as factors that motivate them.</p> <p>To this end, the technical skills that I have strived to equip my teammates with include the following:</p> <ul> <li>building probabilistic estimation models that follow a lab experiment's design,</li> <li>building custom deep learning model architectures,</li> <li>digging through someone else's code and modifying it,</li> <li>writing code that is easy to read and maintain,</li> <li>writing documentation that gives the next person enough information to take productive action, and</li> <li>writing tests that verify the correctness of their code.</li> </ul> <p>That list is a vast set of skills to learn. It's not easy to pick up everything within a year; indeed, I don't expect my teammates to have mastered all of these skills within a year of joining the team. Instead, this is a very long-term list of skills to develop; I don't tend to think short-term when it comes to training.</p> <p>How have we gone about this kind of training? In 2022, I've done a few things, including:</p> <ul> <li>running quarterly docathons where we write documentation for our code,</li> <li>hosting workshops where we walk through the fundamentals of deep learning,</li> <li>intentionally holding off on making quick pull requests for minor code fixes and giving chances to teammates instead,</li> <li>scouting out projects for my teammates where probabilistic modeling is needed, and</li> <li>doing model reviews where we walk through the code-based implementation of a model.</li> </ul>"},{"location":"people-skills/training/#onboarding-syllabus","title":"Onboarding Syllabus","text":"<p>I've also created an onboarding syllabus for new teammates. When a new teammate joins the team, I have them go through a personalized onboarding syllabus that I tailor to their role on the team. In my estimation, the new teammate should be able to complete the syllabus within one month of joining. Examples of activities in the syllabus include:</p> <ol> <li>Making a pull request to a repository with improvements to an existing codebase.</li> <li>Deploying an external machine learning model to our production environment.</li> <li>Writing a test for a piece of code.</li> <li>Meeting with suggested individuals.</li> </ol> <p>Though I had the syllabus in place, I didn't do as good a job as I wanted to follow through on the syllabus. During our subsequent 1:1 check-ins, I probably could have scaffolded the check-ins by following up on their progress in completing the onboarding syllabus tasks.</p>"},{"location":"people-skills/training/#first-project","title":"First Project","text":"<p>When a new teammate joined, I also had to think carefully about their first project. While a first project rarely makes or breaks a career, I see it as an excellent opportunity for a new teammate to score early wins, become trained in the tech stack, and get to know collaborators better. It also gives me a first touch point to observe how our new teammate is adjusting to the new environment and where I need to do any course corrections.</p>"},{"location":"people-skills/training/#ways-to-improve","title":"Ways to improve","text":"<p>While I think these are great introductory ways of training, I also think we might be able to do more for systematic training. For example, once we know enough people have the desire to learn a skill, we could organize a multi-day workshop teaching that skill that also focuses on the application of that skill to a real problem.</p> <p>At the same time, I could have done a better job getting us to understand wet lab scientists' needs better. One way I think we can do this is to have days to shadow our wet lab collaborators. That way, we can better understand what they need and how our team can deliver what they need to advance their research efforts.</p>"},{"location":"software-skills/","title":"Software Skills","text":"<p>Because our day-to-day involves writing code, I am convinced that we data scientists need to be equipped with basic software engineering skills. Being equipped with these skills will help us write code that is, in the long-run, easy to recap, remember, reference, review, and rewrite.</p> <p>In this collection of short essays, I will highlight the basic software skills that, if we master, will increase our efficiency and effectiveness in the long-run.</p>"},{"location":"software-skills/#common-objections","title":"Common Objections","text":"<p>If you have heard these suggestions before, then you might have also heard some of the common objections to learning these software practices. I wish to address them here in bulk, so I do not have to address them in-depth in the individual essays.</p>"},{"location":"software-skills/#i-have-not-enough-time","title":"I have not enough time","text":"<p>This objection is one I am sympathetic to, as I operate under time constraints myself.</p> <p>This is the nature of code: written once, used many times. Hence, the best response that I can give is that time taken cutting corners now yields multiples of others' (including your future self's) time wasted navigating an undocumented, spaghetti-code codebase, that is not well-structured either. Cutting out these software practices now makes things much more difficult to maintain and improve code when it goes into production.</p>"},{"location":"software-skills/#my-code-is-only-going-to-be-written-and-read-by-myself","title":"My code is only going to be written and read by myself","text":"<p>At some point, though, there is a high probability that you will end up writing code that someone else has to read and use. That someone else is usually your future self but also teammates that may need to cover for you when you're out. (This applies especially if you're on a data science team.) The time invested in making the code read well now, even on code that does not have to be read by others, will reduce the learning curve pain when you eventually do have to write code for others. You might as well invest the time now while there's less formal scrutiny to practice your software skills. When the stakes are higher, being ready can only be helpful.</p>"},{"location":"software-skills/#i-dont-know-how-to-get-started-there-are-so-many-places-to-begin","title":"I don't know how to get started, there are so many places to begin","text":"<p>Pick any one skill, say, refactoring, and work on it first. You can always add on more skills into your toolkit as you go along.</p>"},{"location":"software-skills/code-formatting/","title":"Formatting your code","text":"<p>One key insight from the Python programming language is that code is read more often than it is written. Hence, writing code in a fashion that makes it easy to read is something that can only be beneficial.</p> <p>But formatting code is a nit-picky and tedious matter, isn't it? Moreover, code style is one of those things that are not substantive enough to engage in flame wars. It really is one of those things we should just get over with, right?</p> <p>Yes, and it is possible to be \"just over and done with it\" if we use automation tools to help us take care of code formatting so that we don't have to think about it.</p>"},{"location":"software-skills/code-formatting/#introducing-black","title":"Introducing <code>black</code>","text":"<p><code>black</code> is an opinionated code formatter for the Python programming language. It comes with sane defaults, and produces consistently formatted code with a single command at the terminal.</p>"},{"location":"software-skills/code-formatting/#installing-black","title":"Installing <code>black</code>","text":"<p>To install it, we can either use <code>pip</code> or <code>conda</code>:</p> <pre><code># for pip users\npip install black\n# for conda users\nconda install black\n</code></pre>"},{"location":"software-skills/code-formatting/#using-black","title":"Using <code>black</code>","text":"<p>We can use black directly at the command line in our project directory, with configurations called at the command line for convenience.</p> <pre><code># Format all .py files within and underneath current working directory.\nblack -l 79 .\n</code></pre>"},{"location":"software-skills/code-formatting/#introducing-isort","title":"Introducing <code>isort</code>","text":"<p><code>isort</code> is a package for sorting your imports in a source <code>.py</code> file. Once again, this is the sort of thing you definitely don't want to do by hand.</p>"},{"location":"software-skills/code-formatting/#installing-isort","title":"Installing <code>isort</code>","text":"<p><code>isort</code> is also conda- and pip-installable.</p> <pre><code># pip users\npip install isort\n# conda users\nconda install isort\n</code></pre>"},{"location":"software-skills/code-formatting/#using-isort","title":"Using <code>isort</code>","text":"<p>Just like with black, we can use <code>isort</code> to automagically sort our imports. As an example we will call it at the command line with certain options enabled.</p> <pre><code># -r: recurses down below the current working directory.\n# -y: automatically overwrite original source file with sorted imports.\nisort -r -y .\n</code></pre>"},{"location":"software-skills/code-formatting/#building-automation-for-code-formatting","title":"Building automation for code formatting","text":"<p>Automatically executing automagic commands is pretty awesome. Let's see how we can enable this.</p>"},{"location":"software-skills/code-formatting/#makefiles","title":"Makefiles","text":"<p>I also place <code>black</code> as part of a series of commands used in code style checking in a Makefile, to run all of those commands together.</p> <pre><code>format:\nisort -r -y .\n    black -l 79 .\n</code></pre> <p>With that Makefile command, we can now execute all code formatting commands with a single call.</p> <p>Side note: I usually do <code>isort</code> first because <code>black</code> will make detect <code>isort</code>-ed code as not properly formatted, hence I defer to <code>black</code> to make the final changes.</p>"},{"location":"software-skills/code-formatting/#pre-commit-hooks","title":"Pre-commit hooks","text":"<p>We can also use pre-commit hooks to catch non-properly-formatted code, and run the code formatters over the code, preventing them from being merged if any formatting has to take place. This ensures thatwe never commit code that is incorrectly formatted.</p> <p>Getting set up with pre-commit hooks is another topic, but there are already great resources that can be searched for online on how to get setup.</p>"},{"location":"software-skills/code-formatting/#concluding-words","title":"Concluding words","text":"<p>I hope this short essay gives you an overview of the tools that you can use to format your code automatically. Code formatting is important for readability, but isn't worth the tedium. Letting automation save your time is the wise thing to do.</p>"},{"location":"software-skills/documentation/","title":"Documenting your code","text":"<p>Writing lightweight documentation is a practice that I found sorely lacking in data science practice. In this essay, I will show you how to introduce lightweight documentation into your code.</p>"},{"location":"software-skills/documentation/#why-document-your-code","title":"Why document your code","text":"<p>There are a few good reasons to document your code.</p> <p>Firstly, your future self will thank you for having a plain English translation of what you intended to do with that block of code. Oftentimes, the intent behind the code is lost in the translation from our heads to actual code.</p> <p>Secondly, other readers of your code will also thank you.</p> <p>Thirdly, by clarifying what exactly you intended to accomplish with a block of code, as well as the major steps taken towards accomplishing those goals, you often will end up with a much cleaner implementation in the end.</p>"},{"location":"software-skills/documentation/#when-to-document-your-code","title":"When to document your code","text":"<p>A pragmatic choice would be once you find yourself accomplishing a logical chunk of work.</p> <p>I usually do it as soon as I define a Python function.</p>"},{"location":"software-skills/documentation/#where-your-code-documentation-should-go","title":"Where your code documentation should go","text":"<p>As a general rule of thumb, having code documentation as close to the actual source code is probably the best way to approach this.</p> <p>For Python programmers, this would imply taking advantage of docstrings!</p> <p>Docstrings occur in the following places:</p> <ol> <li>Right after a function or class method definition.</li> <li>Right inside a class definition.</li> <li>Right at the top of a <code>.py</code> module.</li> </ol> <p>An anti-pattern here would be writing your documentation in an external system, such as a Wiki. (Woe betide the code developer who writes code docs in Confluence...) This is because the documentation is not proximal to the source code. I have found myself forgetting to update the docstrings after updating the source code. If it's easy to forget to update the docs when the docs are right next to the source, imagine how much easier it is to forget to update external docs!</p> <p>Where, then, would documentation on how the code is organized live then? I would argue it should be pushed as close to the source code as possible. For example, we can use the <code>.py</code> module docstrings to describe the intent behind why certain entire modules exist.</p>"},{"location":"software-skills/documentation/#an-example","title":"An example","text":"<p>Here is a skeleton to follow:</p> <pre><code>\"\"\"\nThis module houses all functions that cannot be neatly categorized\nin other places.\n\"\"\"\n\ndef my_function(arg1, arg2):\n\"\"\"\n    Calculates something based on arg1 and arg2.\n\n    This calculated thing is intended to be used\n    by `this_other_function`,\n    so the return type should not be changed.\n\n    :param arg1: Describe arg1\n    :param arg2: Describe arg2\n    :returns: ``the_thing_being_returned``, a pandas DataFrame (for example).\n    \"\"\"\n    the_thing_being_returned = ...  # implement the function\n    return the_thing_being_returned\n</code></pre> <p>Now, let's see this in action with a function that returns a snake-cased version of a string with all punctuation also removed. (This is a simplified implementation of what is implemented in <code>pyjanitor</code>'s <code>clean_names</code> function.)</p> <pre><code>import string\n\ndef clean_string(s):\n\"\"\"\n    Remove all punctuation from string, and convert to lower_snake_case.\n\n    An example of the input and output:\n\n        \"My string!\" -&gt; \"my_string\"\n\n    :param s: String to clean.\n    \"\"\"\n    s = s.replace(string.punctuation, \"_\").replace(\" \", \"_\").strip(\"_\").lower()\n    return s\n</code></pre> <p>You may notice that the docstring is longer than the implementation. Frequently (though not always), I have found that when docstring length exceeds implementation length, it is a sign that the author(s) of the code have been thoughtful about its implementation. This bodes well for working in a team, especially when a data scientist hands over a prototype to the engineering team.</p>"},{"location":"software-skills/documentation/#addressing-objections","title":"Addressing objections","text":"<p>The main objections to injecting \"basic software engineering\" into a data scientist's workflow usually center around not having enough time.</p> <p>As always, I am sympathetic to this objection, because I also operate under time constraints.</p> <p>One thing I will offer is that docs are an investment of time for the team, rather than for the individual. We save multiples of time downstream when we write good docs. One way to conceptualize this is the number of person-hours saved down the road by oneself and one's teammates when good docs exist. We minimize the amount of time spent reading code to grok what it is about.</p> <p>At the same time, the practice of clarifying what we intend to accomplish with the function can help bring clarity to the implementation. This I have mentioned above. Having a clean implementation makes things easier to maintain later on. Hence, time invested now on good docs also helps us later on.</p> <p>As with other software engineering skills, this is a skill that can be picked up, refined, and honed. We get more efficient at writing docs the more we do it.</p>"},{"location":"software-skills/documentation/#parting-words","title":"Parting words","text":"<p>I hope this essay has helped you get a feel for how you can write well-documented code. At the same time, I hope that by showing you a simple anchoring example that you will be able to replicate the pattern in your own work.</p>"},{"location":"software-skills/environment-variables/","title":"A Data Scientist's Guide to Environment Variables","text":"<p>You might have encountered a piece of software asking you for permission to modify your <code>PATH</code> variable, or another program's installation instructions cryptically telling you that you have to \"set your <code>LD_LIBRARY_PATH</code> variable correctly\".</p> <p>As a data scientist, you might encounter other environment variable issues when interacting with your compute stack (particularly if you don't have full control over it, like I do). This post is meant to demystify what an environment variable is, and how it gets used in a data science context.</p>"},{"location":"software-skills/environment-variables/#what-is-an-environment-variable","title":"What Is An Environment Variable?","text":"<p>First off, let me explain what an environment variable is, by going in-depth into the <code>PATH</code> environment variable. I'd encourage you to execute the commands here inside your bash terminal (with appropriate modifications -- read the text to figure out what I'm doing!).</p> <p>When you log into your computer system, say, your local computer\u2019s terminal or your remote server via SSH, your bash interpreter needs to know where to look for particular programs, such as <code>nano</code> (the text editor), or <code>git</code> (your version control software), or your Python executable. This is controlled by your PATH variable. It specifies the paths to folders where your executable programs are found.</p> <p>By historical convention, command line programs, such as <code>nano</code>, <code>which</code>, and <code>top</code>, are found in the directory <code>/usr/bin</code>. By historical convention, the <code>/bin</code> folder is for software binaries, which is why they are named <code>/bin</code>. These are the ones that are bundled with your operating system, and as such, need special permissions to upgrade.</p> <p>Try it out in your terminal:</p> <pre><code>$ which which\n/usr/bin/which\n$ which top\n/usr/bin/top\n</code></pre> <p>Other programs are installed (for whatever reason) into <code>/bin</code> instead. <code>ls</code> is one example:</p> <pre><code>$ which ls\n/bin/ls\n</code></pre> <p>Yet other programs might be installed in other special directories:</p> <pre><code>$ which nano\n/usr/local/bin/nano\n</code></pre> <p>How does your Bash terminal figure out where to go to look for stuff? It uses the <code>PATH</code> environment variable. It looks something like this:</p> <pre><code>$ echo $PATH\n/usr/bin:/bin:/usr/local/bin\n</code></pre> <p>The most important thing to remember about the <code>PATH</code> variable is that it is \"colon-delimited\". That is, each directory path is separated by the next using a \"colon\" (<code>:</code>) character. The order in which your bash terminal is looking for programs goes from left to right:</p> <ul> <li><code>/usr/bin</code></li> <li><code>/bin</code></li> <li><code>/usr/local/bin</code></li> </ul> <p>On my particular computer, when I type in <code>ls</code>, my bash interpreter will look inside the <code>/usr/bin</code> directory first. It'll find that <code>ls</code> doesn't exist in <code>/usr/bin</code>, and so it'll move to the next directory, <code>/bin</code>. Since my <code>ls</code> exists under <code>/bin</code>, it'll execute the <code>ls</code> program from there.</p> <p>You can see, then, that this is simultaneously super flexible for customizing your compute environment, yet also potentially super frustrating if a program modified your <code>PATH</code> variable without you knowing.</p> <p>Wait, you can actually modify your <code>PATH</code> variable? Yep, and there's a few ways to do this.</p>"},{"location":"software-skills/environment-variables/#how-to-modify-the-path-variable","title":"How To Modify the <code>PATH</code> variable","text":""},{"location":"software-skills/environment-variables/#using-a-bash-session","title":"Using a Bash Session","text":"<p>The first way is transient, or temporary, and only occurs for your particular bash session. You can make a folder have higher priority than the existing paths by \"pre-pending\" it to the <code>PATH</code> variable:</p> <pre><code>$ export PATH=/path/to/my/folder:$PATH\n$ echo $PATH\n/path/to/my/folder:/usr/bin:/bin:/usr/local/bin\n</code></pre> <p>Or I can make it have a lower priority than existing paths by \"appending\" it to the <code>PATH</code> variable:</p> <pre><code>$ export PATH=$PATH:/path/to/my/folder\n$ echo $PATH\n/usr/bin:/bin:/usr/local/bin:/path/to/my/folder\n</code></pre> <p>The reason this is temporary is because I only export it during my current bash session.</p>"},{"location":"software-skills/environment-variables/#bashrc-or-bash_profile-file","title":"<code>bashrc</code> or <code>.bash_profile</code> File","text":"<p>If I wanted to make my changes somewhat more permanent, then I would include inside my <code>.bashrc</code> or <code>.bash_profile</code> file. (I recommend using the <code>.bashrc</code> file.) The <code>.bashrc</code>/<code>.bash_profile</code> file lives inside your home directory (your <code>$HOME</code> environment variable specifies this), and is a file that your bash interpreter will execute first load. It will execute all of the commands inside there. This means, you can change your PATH variable by simply putting inside your <code>.bashrc</code>:</p> <pre><code>...other stuff above...\n# Make /path/to/folder have higher priority\nexport PATH=/path/to/folder:$PATH\n\n# Make /path/to/other/folder have lower priority\nexport PATH=$PATH:/path/to/folder\n...other stuff below...\n</code></pre>"},{"location":"software-skills/environment-variables/#data-science-and-the-path-environment-variable","title":"Data Science and the <code>PATH</code> environment variable","text":"<p>Now, how is this relevant to data scientists? Well, if you're a data scientist, chances are that you use Python, and that your Python interpreter comes from the Anaconda Python distribution (a seriously awesome thing, go get it!). What the Anaconda Python installer does is prioritize the <code>/path/to/anaconda/bin</code> folder in the <code>PATH</code> environment variable. You might have other Python interpreters installed on your system (that is, Apple ships its own). However, this <code>PATH</code> modification ensures that each time you type <code>python</code> into your Bash terminal, ou execute the Python interpreter shipped with the Anaconda Python distribution. In my case, after installing the Anaconda Python distribution, my <code>PATH</code> looks like:</p> <pre><code>$ echo $PATH\n/Users/ericmjl/anaconda/bin:/usr/bin:/bin:/usr/local/bin\n</code></pre> <p>Even better, what conda environments do is prepend the path to the conda environment binaries folder while the environment is activated. For example, with my blog, I keep it in an environment named <code>lektor</code>. Thus...</p> <pre><code>$ echo $PATH\n/Users/ericmjl/anaconda/bin:/usr/bin:/bin:/usr/local/bin\n$ which python\n/Users/ericmjl/anaconda/bin/python\n$ source activate lektor\n$ echo $PATH\n/Users/ericmjl/anaconda/envs/lektor/bin:/Users/ericmjl/anaconda/bin:/usr/bin:/bin:/usr/local/bin\n$ which python\n/Users/ericmjl/anaconda/envs/lektor/bin/python\n</code></pre> <p>Notice how the bash terminal now preferentially picks the Python inside the higher-priority <code>lektor</code> environment.</p> <p>If you've gotten to this point, then you'll hopefully realize there's a few important concepts listed here. Let's recap them:</p> <ul> <li><code>PATH</code> is an environment variable stored as a plain text string used by the bash interpreter to figure out where to find executable programs.</li> <li><code>PATH</code> is colon-delimited; higher priority directories are to the left of the string, while lower priority directories are to the right of the string.</li> <li><code>PATH</code> can be modified by prepending or appending directories to the environment variable. It can be done transiently inside a bash session by running the <code>export</code> command at the command prompt, or it can be done permanently across bash sessions by adding an <code>export</code> line inside your <code>.bashrc</code> or <code>.bash_profile</code>.</li> </ul>"},{"location":"software-skills/environment-variables/#other-environment-variables-of-interest","title":"Other Environment Variables of Interest","text":"<p>Now, what other environment variables might a data scientist encounter? These are a sampling of them that you might see, and might have to fix, especially in contexts where your system administrators are off on vacation (or taking too long to respond).</p>"},{"location":"software-skills/environment-variables/#general-use","title":"General Use","text":"<p>For general use**, you'll definitely want to know where your <code>HOME</code> folder is -- on Linux systems, it's often <code>/home/username</code>, while on macOS systems, it's often <code>/Users/username</code>.  You can figure out what <code>HOME</code> is by doing:</p> <pre><code>$ echo $HOME\n/Users/ericmjl\n</code></pre>"},{"location":"software-skills/environment-variables/#python","title":"Python","text":"<p>If you're a Python user, then the <code>PYTHONPATH</code> is one variable that might be useful. It is used by the Python interpreter, and specifies where to find Python modules/packages.</p>"},{"location":"software-skills/environment-variables/#c-libraries","title":"C++ libraries","text":"<p>If you have to deal with C++ libraries, then knowing your <code>LD_LIBRARY_PATH</code> environment variable is going to be very important. I'm not well-versed enough in this to espouse on it intelligently, so I would defer to this website for more information on best practices for using the <code>LD_LIBRARY_PATH</code> variable.</p>"},{"location":"software-skills/environment-variables/#spark","title":"Spark","text":"<p>If you're working with Spark, then the <code>PYSPARK_PYTHON</code> environment variable would be of interest. This essentially tells Spark which Python to use for both its driver and its workers; you can also set the <code>PYSPARK_DRIVER_PYTHON</code> to be separate from the <code>PYSPARK_PYTHON</code> environment variable, if needed.</p>"},{"location":"software-skills/environment-variables/#data-science-apps","title":"Data science apps","text":"<p>If you're developing data science apps, then according to the 12 factor app development principles, your credentials to databases and other sensitive information are securely stored and dynamically loaded into the environment at runtime. How then do you mimic this in a \"local\" environment (i.e. your computer) without hard-coding sensitive information in your source <code>.py</code> files?</p> <p>One way to handle this situation is as follows: Firstly, create a <code>.env</code> file in your home directory. In there, store your credentials:</p> <pre><code>SOME_PASSWORD=\"put_your_pw_here\"\nSOME_USERNAME=\"put_your_username_here\"\n</code></pre> <p>Next, add it to your <code>.gitignore</code>, so you never add it to your version control system.</p> <pre><code># other things\n.env\n</code></pre> <p>Finally, in your source <code>.py</code> files, use <code>python-dotenv</code> to load the environment variables at runtime.</p> <pre><code>from dotenv import load_dotenv\nload_dotenv()\n\nimport os\n\nusername = os.getenv(\"SOME_USERNAME\")\npassword = os.getenv(\"SOME_PASSWORD\")\n</code></pre>"},{"location":"software-skills/environment-variables/#hack-your-environment-variables","title":"Hack Your Environment Variables","text":"<p>This is where the most fun happens! Follow along for some stuff you might be able to do by hacking your environment variables.</p>"},{"location":"software-skills/environment-variables/#hack-1-enable-access-to-pypy","title":"Hack #1: Enable access to PyPy.","text":"<p>I occasionally keep up with the development of PyPy, but because PyPy is not yet the default Python interpreter, and is not yet <code>conda install</code>-able, I have to put it in its own <code>$HOME/pypy/bin</code> directory. To enable access to the PyPy interpreter, I have to make sure that my <code>/path/to/pypy</code> is present in the <code>PATH</code> environment variable, but at a lower priority than my regular CPython interpreter.</p>"},{"location":"software-skills/environment-variables/#hack-2-enable-access-to-other-language-interpreterscompilers","title":"Hack #2: Enable access to other language interpreters/compilers.","text":"<p>This is analogous to PyPy. I once was trying out Lua's JIT interpreter to use Torch for deep learning, and needed to add a path to there in my <code>.bashrc</code>.</p>"},{"location":"software-skills/environment-variables/#hack-3-install-python-packages-to-your-home-directory","title":"Hack #3: Install Python packages to your home directory.","text":"<p>On shared Linux compute systems that use the <code>modules</code> system rather than <code>conda</code> environments, a <code>modulefile</code> that you load might be configured with a virtual environment that you don't have permissions to modify. If you need to install a Python package, you might want to <code>pip install --user my_pkg_name</code>. This will install it to <code>$HOME/.local/lib/python-[version]/site-packages/</code>. Ensuring that your <code>PYTHONPATH</code> includes <code>$HOME/.local/lib/python-[version]/site-packages</code> at a high enough priority is going to be important in this case.</p>"},{"location":"software-skills/environment-variables/#hack-4-debugging-when-things-go-wrong","title":"Hack 4: Debugging when things go wrong.","text":"<p>In case something throws an error, or you have unexpected behaviour -- something I encountered before was my Python interpreter not being found correctly after loading all of my Linux modules -- then a way to debug is to temporarily set your PATH environment variable to some sensible \"defaults\" and sourcing that, effectively \"resetting\" your PATH variable, so that you can manually prepend/append while debugging.</p> <p>To do this, place the following line inside a file named <code>.path_default</code>, inside your home directory:</p> <pre><code>export PATH=\"\"  # resets PATH to an empty string.\nexport PATH=/usr/bin:/bin:/usr/local/bin:$PATH  # this is a sensible default; customize as needed.\n</code></pre> <p>After something goes wrong, you can reset your PATH environment variable by using the \"source\" command:</p> <pre><code>$ echo $PATH\n/some/complicated/path:/more/complicated/paths:/really/complicated/paths\n$ source ~/.path_default\n$ echo $PATH\n/usr/bin:/bin:/usr/local/bin\n</code></pre> <p>Note - you can also execute the exact same commands inside your bash session; the interactivity may also be helpful.</p>"},{"location":"software-skills/environment-variables/#conclusion","title":"Conclusion","text":"<p>I hope you enjoyed this article, and that it'll give you a, ahem, path forward whenever you encounter these environment variables!</p>"},{"location":"software-skills/refactoring/","title":"Refactoring your code","text":"<p>How many times have you found yourself copy/pasting code from one notebook to another? If it the answer is \"many\", then this essay probably has something for you. We're going to look at the practice of \"refactoring\" code, and how it applies in a data science context.</p>"},{"location":"software-skills/refactoring/#why-refactor","title":"Why refactor","text":"<p>When writing code, we intend to have a block of code do one thing. As such, its multiple application should have a single source of truth. However, the practice of copying and pasting code gives us multiple sources of truth. Refactoring code, thus, gives us a way of establishing a single source of truth for our functions, which can be called on in multiple situations.</p>"},{"location":"software-skills/refactoring/#when-to-refactor","title":"When to refactor","text":"<p>The short answer is \"basically whenever you find yourself hitting copy+paste\" on your keyboard.</p>"},{"location":"software-skills/refactoring/#how-do-we-refactor","title":"How do we refactor","text":"<p>The steps involved are as follows.</p> <ol> <li>Wrap the semi-complex block of code in a function.</li> <li>Identify what you would consider to be an \"input\" and \"output\" for the function.</li> <li>Take specific variable names and give them more general names.</li> </ol>"},{"location":"software-skills/refactoring/#an-example","title":"An example","text":"<p>Let's take the example of a chunk of code that takes a protein sequence, compares it to a reference sequence, and returns all of the mutations that it has. (We will only implemenet a naive version for the sake of pedagogy.)</p> <pre><code>sequence1 = ...\nsequence2 = ...\n\nmutations = []\nfor i, (letter1, letter2) in enumerate(zip(sequence1, sequence2)):\n    mutations.append(f\"{letter1}{i+1}{letter2}\")\nmutations = \"; \".join(m for m in mutations)\n</code></pre> <p>This more or less should accomplish what we want. Let's now apply the ideas behind refactoring to this code block.</p> <pre><code>def mutation_string(reference, sequence, sep=\"; \"):\n    mutations = []\n    for i, (letter1, letter2) in enumerate(zip(reference, sequence)):\n        mutations.append(f\"{letter1}{i+1}{letter2}\")\n    return f\"{sep}\".join(m for m in mutations)\n</code></pre> <p>You'll notice the three steps coming into play.</p> <p>Firstly, we simply shifted the main logic of the code into a function definition.</p> <p>Secondly, we then generalized the function a bit, by renaming <code>sequence1</code> and <code>sequence2</code> to what we usually intend for it to be, a <code>sequence</code> of interest and a <code>reference</code> sequence.</p> <p>Finally, we defined those two as inputs, alongside a keyword argument called <code>sep</code>, which defines the separator between each mutation.</p>"},{"location":"software-skills/refactoring/#bonus","title":"Bonus","text":"<p>On the basis of this function definition, we can do some additional neat things!</p> <p>For example, in protein sequence analysis, our <code>reference</code> sequence is usually kept constant. Hence, we can actually create a custom <code>mutation_string</code> for our reference sequence using <code>functools.partial</code> by fixing <code>reference</code> to a particular value, thus eliminating the need to repetitively pass in the same reference string.</p> <pre><code>from functools import partial\n\nprotein1 = ...  # define the string here.\n\nprot1_mut_string = partial(mutation_string, reference=protein1)\n\nprotein2 = ...  # define the string here.\n\nmutstring = prot1_mut_string(sequence=protein2)\n</code></pre>"},{"location":"software-skills/refactoring/#where-should-this-function-be-refactored-to","title":"Where should this function be refactored to","text":"<p>You can choose to keep it in the notebook, and that would be fine if the function was used only in a single notebook.</p> <p>If you find yourself needing to call on that same function from another notebook, do the right thing and create a <code>utils.py</code> (or analogous) Python module that lives in the same directory as the notebook. Then, import the refactored function from <code>utils.py</code>.</p> <p>If you feel sophisticated, you can also create a custom Python library for your project. I will address this in a separate essay.</p> <p>An anti-pattern, though, would be to attempt to treat the notebook as source code and import the function from one notebook into another. Notebooks are great for one thing: weaving functions together into an integrarted analysis. I'm of the opinion that we should use a tool the way it was intended, and bring in other tools to do what we need. In this respect, I think that DataBricks notebooks does the wrong thing by bowing to bad human first instincts rather than encouraging productive behaviours.</p>"},{"location":"software-skills/refactoring/#where-do-we-find-time-to-do-this","title":"Where do we find time to do this","text":"<p>I hear this concern, as I went through the same concerns myself.</p> <p>Isn't it faster to just copy/paste the code? What if I don't end up reusing the code elsewhere? Isn't the time then wasted?</p> <p>In thinking back to my own habits, I realized early on that doing this was not a matter of technical ability but rather a matter of mindset.</p> <p>Investing the time into doing simple refactoring alongside my analyses does take immediate time away from the analysis. However, the deliberate practice of refactoring early on earns back multiples of the time spent as the project progresses.</p> <p>Moreover, if and when the project gets handed over \"in production\", or at least shared with others to use, our colleagues can spend less time is spent navigating a spaghetti-like codebase, and more time can be spent building a proper mental model of the codebase to build on top of.</p> <p>On the possiblity of not reusing the code elsewhere, I would strongly disagree. Refactoring is not a common skill, while copy/pasting code is. Every chance we get to refactor code is practicing the skill, which only gets sharper and more refined as we do it more. Hence, even for the sake of getting more practice makes it worthwhile to do refactoring at every chance.</p>"},{"location":"software-skills/refactoring/#concluding-words","title":"Concluding words","text":"<p>I hope this mini-essay demystifies the practice of code refactoring, and gives you some ideas on how to make it part of your workflow.</p>"},{"location":"software-skills/testing/","title":"Testing your code","text":"<p>Writing tests for code is a basic software skill. Writing tests helps build confidence in the stability of our code.</p>"},{"location":"software-skills/testing/#when-to-write-tests","title":"When to write tests","text":"<p>There are two \"time scales\" at which I think this question can be answered.</p> <p>The first time scale is \"short-term\". As soon as we finish up a function, that first test should be written. Doing so lets us immediately sanity-check our intuition about the newly-written fuction.</p> <p>The second time scale is \"longer-term\". As soon as we discover bugs, new tests should be added to the test suite. Those new tests should either cover that exact bug, or cover the class of bugs together.</p> <p>A general rule-of-thumb that has proven reliable is to write an automated test for anything function you come to rely on.</p>"},{"location":"software-skills/testing/#how-to-get-setup","title":"How to get setup","text":"<p>In a Python project, first ensure that you have <code>pytest</code> installed. If you follow recommended practice and have one <code>conda</code> environment per project, then you should be able to install <code>pytest</code> using <code>conda</code>:</p> <pre><code># if you use conda:\nconda install pytest\n# if you use pip:\npip install pytest\n</code></pre>"},{"location":"software-skills/testing/#the-anatomy-of-a-test","title":"The anatomy of a test","text":"<p>When using <code>pytest</code>, your tests take on the function name:</p> <pre><code>from custom_library import my_function\n\ndef test_my_function():\n\"\"\"Test for my_function.\"\"\"\n    # set up test here.\n    assert some_condition\n</code></pre> <p>We can then execute the test from the command line:</p> <pre><code>pytest .\n</code></pre> <p>Voila! The tests will be executed, and you will see them run one by one.</p>"},{"location":"software-skills/testing/#the-kinds-of-tests-you-could-write","title":"The kinds of tests you could write","text":"<p>Let's go through the kinds of tests you might want to write.</p>"},{"location":"software-skills/testing/#execution-tests","title":"Execution tests","text":"<p>I started with this kind of test because these are the simplest to understand: we simply execute a function to make sure that it runs without breaking.</p> <pre><code>from custom_lib import my_function\n\ndef test_my_function():\n\"\"\"Execution test for my_function.\"\"\"\n    my_function()\n</code></pre> <p>This kind of test is useful when your function is not parameterized, and simply calls on other functions inside your library. It is also incredibly useful as a starter test when you cannot think of a better test to write.</p> <p>One place where I have used this test pattern is when we built a project dashboard using Panel. The dashboard is made from many complex layers of function calls, involving database queries, data preprocessing, cached results, and more. Sporadically, something would break, and it was something difficult to debug. By wrapping the dashboard execution inside a Python function and executing it by simply calling <code>dashboard()</code>, we could discover bugs as soon as they showed up, rather than so-called \"in production\".</p>"},{"location":"software-skills/testing/#example-based-test","title":"Example-based test","text":"<p>An example-based test looks basically like this:</p> <pre><code>from custom_lib import another_function\n\ndef test_another_function():\n    arg1 = ...\n    arg2 = ...\n    result = another_function(arg1, arg2)\n\n    expected_result = ...\n\n    assert result == expected_result\n</code></pre> <p>Basically, we set up the test with an example, and check that when given a set of pre-specified inputs, a particular expected result is returned.</p> <p>When writing code in the notebook, I find myself writing example-based tests informally all the time. They are those \"sanity-checks\" function calls where I manually check that the result looks correct. I am sure you do too.</p> <p>So rather than rely on manually checking, it makes perfect sense to simply copy and paste the code into a test function and execute them.</p>"},{"location":"software-skills/testing/#advanced-testing","title":"Advanced Testing","text":"<p>The above I consider to be basic, bare minimum testing that a data scientist can do. Of course, there are more complex forms of testing that a QA engineer would engage in, and I find it useful to know at least what they are and what tools we have to do these forms of testing in the Python ecosystem:</p> <ul> <li>Parameterized tests: <code>pytest</code> has these capabilities.</li> <li>Property-based tests: <code>hypothesis</code> gives us these capabilities.</li> </ul>"},{"location":"software-skills/testing/#tests-for-data","title":"Tests for Data","text":"<p>Data are notoriously difficult to test, because it is a snapshot of the stochastic state of the world. Nonetheless, if we impose prior knowledge on our testing, we can ensure that certain errors in our data never show up.</p>"},{"location":"software-skills/testing/#nullity-tests","title":"Nullity Tests","text":"<p>For example, if we subject a SQL query to a series of transforms that are supposed to guarantee a densely populated DataFrame, then we can write a nullity test.</p> <pre><code>def test_dataframe_function():\n\"\"\"Ensures that there are no null values in the dataframe function.\"\"\"\n    df = dataframe_function(*args, **kwargs)\n    assert pd.isnull(df).sum().sum() == 0\n</code></pre>"},{"location":"software-skills/testing/#dtype-tests","title":"<code>dtype</code> Tests","text":"<p>We can also check that the dtypes of the dataframe are correct.</p> <pre><code>def test_dataframe_dtypes():\n\"\"\"Checks that the dtypes of the dataframe are correct.\"\"\"\n    dtypes = {\n        \"col1\": float32,\n        \"col2\": int,\n        \"col3\": object,\n    }\n    df = dataframe_function(*args, **kwargs)\n    for col, dtype in dtypes.items():\n        assert df[col].dtype == dtype\n</code></pre>"},{"location":"software-skills/testing/#bounds-tests","title":"Bounds Tests","text":"<p>We can also check to make sure that our dataframe-returning function yields data in the correct bounds for each column.</p> <pre><code>def test_dataframe_bounds():\n\"\"\"Checks that the bounds of datsa are correct.\"\"\"\n    df = dataframe_function(*args, **kwargs)\n    # For a column that can be greater than or equal to zero.\n    assert df[\"column1\"].min() &gt;= 0\n\n    # For a column that can only be non-zero positive.\n    assert df[\"column2\"].min() &gt; 0\n\n    # For a column that can only be non-zero negative.\n    assert df[\"column3\"].max() &lt; 0\n</code></pre> <p>DataFrame tests are a special one for data scientists, because the dataframe is the idiomatic data structure that we engage with on an almost daily basis.</p>"},{"location":"software-skills/testing/#column-name-tests","title":"Column Name Tests","text":"<p>Having stable and consistent column names in the dataframes that we use is extremely important; the column names are like our API to the data. Hence, checking that a suite of expected column names exist in the dataframe can be very useful.</p> <pre><code>def test_dataframe_names():\n\"\"\"Checks that dataframe column names are correct.\"\"\"\n    expected_column_names = [\"col1\", \"col2\", \"col3\"]\n    df = dataframe_function(*args, **kwargs)\n\n    # Check that each of those column names are present\n    for c in expected_column_names:\n        assert c in df.columns\n\n    # (Optional) check that _only_ those columns are present.\n    assert set(df.columns) == set(expected_column_names)\n</code></pre>"},{"location":"software-skills/testing/#other-statistical-property-tests","title":"Other statistical property tests","text":"<p>Testing the mean, median, and mode are difficult, but under some circumstances, such as when we know that the data are drawn from some distribution, we might be able to write a test for the central tendencies of the data.</p> <p>Placing an automated test that checks whether the data matches a particular parameterized distribution with some probability value is generally not a good idea, because it can give a false sense of security. However, if this is a key modelling assumption and you need to keep an automated, rolling check on your data, then having it as a test can help you catch failures in downstream modelling early. In practice, I rarely use this because the speed at which data come in are slow relative to the time I need to check assumptions. Additionally, the stochastic nature of data means that this test would be a flaky one, which is an undesirable property for tests.</p>"},{"location":"software-skills/testing/#parting-words","title":"Parting words","text":"<p>I hope this essay gives you some ideas for implementing testing in your data science workflow. As with other software skills, these are skills that become muscle memory over time, hence taking the time from our daily hustle to practice them makes us more efficient in the long-run. In particular, the consistent practice of testing builds confidence in our codebase, not just for my future self, but also for other colleagues who might end up using the codebase too.</p>"},{"location":"software-skills/testing/#a-glossary-of-testing-in-data-science","title":"A Glossary of Testing in Data Science","text":"<p>Manual testing: Basically where we use a Jupyter notebook and manually inspect that the function works to how we\u2019re expecting.</p> <p>Automated testing: Where we provide a test suite and use a test runner (e.g. <code>pytest</code>) to automatically execute all of the tests in the suite.</p> <p>Example-based testing: Where we provide one or more hard-coded examples in our test suite, and test that our function works on those examples.</p> <p>Parameterized testing: Where we provide examples as parameters to our test functions, helping us reduce code duplication in our test functions. Not necessarily something distinct from example-based testing.</p> <p>Auto-manual testing: A not-so-tongue-in-cheek way of describing automated testing using hard-coded examples.</p> <p>Property-based testing: Where we use an automatic generator of examples that fulfill certain \u201cproperties\u201d. For example, numbers with range constraints, or strings generated from an alphabet of a certain length or less. Property-based testing builds on top of parameterized testing.</p> <p>Data testing: Where we test the \u201ccorrectness\u201d of our data. Property-based testing can be used here, or we can hard-code checks on our data that we know should be invariant over time.</p>"},{"location":"terminal/cli-tools/","title":"Tools and Upgrades for your CLI","text":"<p>In this short essay, I would like to introduce you to a list of awesome command-line tools that I have found on the internet.</p> <p>Most of the tools listed here do one thing really well: they add visual clarity to the text that we are looking at. This is mostly done by colorizing the terminal with syntax highlighting.</p> <p>Without further ado, let's get started listing them.</p>"},{"location":"terminal/cli-tools/#exa","title":"<code>exa</code>","text":"<p><code>exa</code> is a favourite of mine, because it is an almost drop-in replacement for <code>ls</code>, except with saner defaults. It also comes with a saner set of defaults for the <code>tree</code> command.</p> <p>After installing, you can replace <code>ls</code> and <code>tree</code> with <code>exa</code> by aliasing:</p> <pre><code>alias ls='exa --long --git -a --header --group'\nalias tree='exa --tree --level=2 --long -a --header --git'\n</code></pre>"},{"location":"terminal/cli-tools/#tmux","title":"<code>tmux</code>","text":"<p><code>tmux</code> is another daily driver of mine. I use it to keep remote terminal sessions persistent, and use it effectively as a workspace manager between projects.</p>"},{"location":"terminal/cli-tools/#nanorc","title":"<code>nanorc</code>","text":"<p>If you're like me, and are accustomed to the <code>nano</code> text editor rather than <code>vim</code> or <code>emacs</code>, then <code>nanorc</code>, a set of syntax highlighting configurations provided by Anthony Scopatz is an awesome addition to your <code>nano</code> toolkit.</p> <p>(For what it's worth, I wrote this short essay in <code>nano</code>, and <code>nanorc</code> played no small role in making the text readable!)</p>"},{"location":"terminal/cli-tools/#diff-so-fancy","title":"<code>diff-so-fancy</code>","text":"<p><code>diff-so-fancy</code> is a drop-in replacement for <code>diff</code>, and makes it so much easier read diffs between two files.</p> <p>After installation, you can easily replace <code>diff</code> with <code>diff-so-fancy</code> through aliasing:</p> <pre><code>alias diff=\"diff-so-fancy\"\n</code></pre>"},{"location":"terminal/cli-tools/#bat","title":"<code>bat</code>","text":"<p><code>bat</code> is another one of those instant favourites. I use <code>cat</code> and <code>less</code> often to look through files, but <code>bat</code> takes things to another level. It is basically a mash-up between <code>cat</code> and <code>less</code>, allowing you to scroll through your files in a <code>less</code>-like scrolling fashion, while also providing syntax highlighting for the files you open.</p> <p>At the same time, it'll let you concatenate two files together (just like <code>cat</code>) and display them to the screen.</p> <p>After installing, you can replace <code>cat</code> with <code>bat</code> by aliasing as well:</p> <pre><code>alias cat=\"bat\"\n</code></pre>"},{"location":"terminal/cli-tools/#fd","title":"<code>fd</code>","text":"<p><code>fd</code> is another tool that provides saner syntax than the default <code>find</code>.</p> <p>After installing, you can replace <code>find</code> with <code>fd</code> by aliasing:</p> <pre><code>alias find=\"fd\"\n</code></pre>"},{"location":"terminal/cli-tools/#ripgrep","title":"<code>ripgrep</code>","text":"<p><code>ripgrep</code> is a tool that will let you search directories recursively for a particular pattern. This can help you quickly find text inside a file inside the file tree easily.</p>"},{"location":"terminal/cli-tools/#references","title":"References","text":"<p>Vim From Scratch introduced many of the tools shown here, and I want to make sure that the author gets credit for finding and sharing these awesome tools!</p> <p>James Weis introduced me to <code>tmux</code> while in grad school, and I've been hooked ever since.</p>"},{"location":"terminal/pre-commits/","title":"Using <code>pre-commit</code> git hooks to automate code checks","text":"<p>Git hooks are an awesome way to automate checks on your codebase locally before committing them to your code repository.</p> <p>That said, setting them up involves digging into the <code>.git</code> folder of your repository, and can feel intimidating to set up and replicate across multiple local clones of repositories.</p> <p>Thankfully, there is an easier way about.</p> <p>The developers of the <code>pre-commit</code> framework have given us a wonderful tool to standardize and automate the replication of pre-commit git hooks.</p>"},{"location":"terminal/pre-commits/#what-git-hooks-are","title":"What git hooks are","text":"<p>Git hooks are basically commands that are run just before or after git commands are executed. In this essay's context, I basically consider it a great way to run automated checks on our code before we commit them.</p>"},{"location":"terminal/pre-commits/#getting-started-with-pre-commit","title":"Getting started with <code>pre-commit</code>","text":"<p>First off, you should follow the <code>pre-commit</code> instructions for getting setup. These instructions are availble on the <code>pre-commit</code> website. For those of you who know what you are doing and just want something to copy/paste:</p> <pre><code>conda install -c conda-forge pre-commit\npre-commit sample-config &gt; .pre-commit-config.yaml\npre-commit install\npre-commit run --all-files\n</code></pre>"},{"location":"terminal/pre-commits/#configuring-your-pre-commit","title":"Configuring your <code>pre-commit</code>","text":"<p>While the default set is nice, you might want to install other hooks.</p> <p>For example, a Python project might want to default to using <code>black</code> as the code formatter. To enable automatic <code>black</code> formatting and checking before committing code, we need to add <code>black</code> to the configuration file that was produced (<code>.pre-commit-config.yaml</code>).</p> <pre><code>-   repo: https://github.com/psf/black\nrev: 19.3b0\nhooks:\n-   id: black\n</code></pre> <p>A classic mistake that I made was to add black directly underneath the default:</p> <pre><code># THIS IS WRONG!!!\n-   repo: https://github.com/pre-commit/pre-commit-hooks\nrev: v2.3.0\nhooks:\n-   id: check-yaml\n-   id: end-of-file-fixer\n-   id: trailing-whitespace\n-   id: black  # THIS IS WRONG!!!\n</code></pre> <p>You will get an error if you do this. Be forewarned!</p>"},{"location":"terminal/pre-commits/#updating-your-pre-commit-after-updating-pre-commit-configyaml","title":"Updating your pre-commit after updating <code>.pre-commit-config.yaml</code>","text":"<p>If you forgot to add a hook but have just edited the YAML file to do so, you will need to run the command to install the hooks.</p> <pre><code>pre-commit install-hooks\n#  Optional\npre-commit run --all-files\n</code></pre> <p>Now, the new hooks will be installed.</p>"},{"location":"terminal/pre-commits/#what-happens-when-you-use-pre-commit","title":"What happens when you use pre-commit","text":"<p>As soon as you write your commit your source files, just before the commit happens, your installed pre-commit hooks execute. If the hooks modify any files, then the commit is halted, and the files that were modified will show up as being \"modified\" or \"untracked\" in your git status.</p> <p>At this point, add the files that were modified by your pre-commit hooks, commit those files, and re-enter your commit message. In this way, you will prevent yourself from committing code that does not pass your code checks.</p>"},{"location":"terminal/pre-commits/#good-pre-commit-hooks-for-python-projects","title":"Good pre-commit hooks for Python projects","text":"<p>My opinionated list of nice hooks to have can be found below.</p> <ul> <li>black</li> <li>pydocstyle</li> <li>isort</li> </ul>"},{"location":"terminal/pre-commits/#benefits-of-setting-up-pre-commit-and-hooks","title":"Benefits of setting up pre-commit (and hooks)","text":"<p>By setting up a standard configuration that gets checked into source control, we are setting our team up for success working together. Opinionated checks are now delegated to automated machinery rather than requiring human intervention, hence freeing us up to discuss higher order issues rather than nitpicking on code style.</p> <p>Moreover, by using the <code>pre-commit</code> framework, we take a lot of tedium out in setting up the pre-commit git hooks correctly. I've tried to do that before, and found writing the bash script to be a fragile task to execute. It's fragile because I'm not very proficient in Bash, and I have no other way of testing the git pre-commit hooks apart from actually making a commit. Yet, it seems like we should be able to modularize our hooks, such that they are distributed, installed, and executed in a standard fashion. This is what the pre-commit framework gives us.</p>"},{"location":"workflow/code-review/","title":"Practicing Code Review","text":"<p>The practice of code review is extremely beneficial to the practice of software engineering. I believe it has its place in data science as well.</p>"},{"location":"workflow/code-review/#what-code-review-is","title":"What code review is","text":"<p>Code review is the process by which a contributor's newly committed code is reviewed by one or more teammate(s). During the review process, the teammate(s) are tasked with ensuring that they</p> <ul> <li>understand the code and are able to follow the logic,</li> <li>find potential flaws in the newly contributed code,</li> <li>identify poorly documented code and confusing use of variable names,</li> <li>raise constructive questions and provide constructive feedback</li> </ul> <p>on the codebase.</p> <p>If you've done the practice of scientific research before, it is essentially identical to peer review, except with code being the thing being reviewed instead.</p>"},{"location":"workflow/code-review/#what-code-review-isnt","title":"What code review isn't","text":"<p>Code review is not the time for a senior person to slam the contributions of a junior person, nor vice versa.</p>"},{"location":"workflow/code-review/#why-data-scientists-should-do-code-review","title":"Why data scientists should do code review","text":""},{"location":"workflow/code-review/#reason-1-sharing-knowledge","title":"Reason 1: Sharing Knowledge","text":"<p>The first reason is to ensure that project knowledge is shared amongst teammates. By doing this, we ensure that in case the original code creator needs to be offline for whatever reason, others on the team cover for that person and pick up the analysis. When N people review the code, N+1 people know what went on. (It does not necessarily have to be N == number of people on the team.)</p> <p>In the context of notebooks, this is even more important. An analysis is complex, and involves multiple modelling decisions and assumptions. Raising these questions, and pointing out where those assumptions should be documented (particularly in the notebook) is a good way of ensuring that N+1 people know those implicit assumptions that go into the model.</p>"},{"location":"workflow/code-review/#reason-2-catching-mistakes","title":"Reason 2: Catching Mistakes","text":"<p>The second reason is that even so-called \"senior\" data scientists are humans, and will make mistakes. With my interns and less-experienced colleagues, I will invite them to constructively raise queries about my code where it looks confusing to them. Sometimes, their lack of experience gives me an opportunity to explain and share design considerations during the code review process, but at other times, they are correct, and I have made a mistake in my code that should be rectified.</p>"},{"location":"workflow/code-review/#reason-3-social-networking","title":"Reason 3: Social Networking","text":"<p>If your team is remote, then code review can be an incredibly powerful way of interacting with one another in a professional and constructive fashion.</p> <p>Because of code review, even in the absence of in-person chats, we still know someone else is looking at the product of our work. The constructive feedback and the mark of approval at the end of the code review session are little plus points that add up to a great working relationship in the long-run, and reduce the sense of loneliness in working remotely.</p>"},{"location":"workflow/code-review/#what-code-review-can-be","title":"What code review can be","text":"<p>Code review can become a very productive time of learning for all parties. What it takes is the willingness to listen to the critique provided, and the willingness to raise issues on the codebase in a constructive fashion.</p>"},{"location":"workflow/code-review/#how-code-review-happens","title":"How code review happens","text":"<p>Code review happens usually in the context of a pull request to merge contributed code into the master branch. The major version control system hosting platforms (GitHub, BitBucket, GitLab) all provide an interface to show the \"diff\" (i.e. newly contributed or deleted code) and comment directly on the code, in context.</p> <p>As such, code review can happen entirely asynchronously, across time zones, and without needing much in-person interaction.</p> <p>Of course, being able to sync up either via a video call, or by meeting up in person, has numerous advantages by allowing non-verbal communication to take place. This helps with building trust between teammates, and hence doing even \"virtual\" in-person reviews can be a way of being inclusive towards remote colleagues.</p>"},{"location":"workflow/code-review/#parting-words","title":"Parting words","text":"<p>If your firm is set up to use a version control system, then you probably have the facilities to do code review available. I hope this essay encourages you to give it a try.</p>"},{"location":"workflow/effective-commit-messages/","title":"Effective Git Commits in Data Science","text":"<p>Continuing on the theme of the use of Git in data science, I thought I would write about how to use git commits effectively in our day-to-day data science work.</p>"},{"location":"workflow/effective-commit-messages/#how-git-commits-are-intended-to-be-used","title":"How <code>git</code> commits are intended to be used","text":"<p>Git commits are intended to be used as a running log of what gets checked into a code repository. In software engineering, each commit is intended to be a \u201clogical unit of work\u201d.</p> <p>One intent behind defining a commit as a \u201clogical unit of work\u201d is that in case that logical unit of work turned out to be faulty, we can revert that unit of work and only that unit of work without touching other units of work.</p> <p>Git commits can also help us track who made contributions to a repository, as each commit also contains information about the committer (e.g. name and email address).</p> <p>We can view the commit history at the terminal by typing the following incantation:</p> <pre><code>git log --decorate --graph\n</code></pre> <p>That will give us an interface to the commit log. It will show a running log of the commits to the project, as well as every commit message that was put in. Writing commit messages as if we're going to read them at a later date in reverse sequential order can help us write better commit messages.</p>"},{"location":"workflow/effective-commit-messages/#git-commits-in-analysis-heavy-projects","title":"<code>git</code> commits in analysis-heavy projects","text":"<p>In the software world, <code>git</code> commits are a logical way to work. By comparison, in data analysis-heavy work, it is seemingly more difficult to define a \u201clogical unit of work\u201d thank we might in software engineering.</p> <p>After all, what exactly constitutes a \u201clogical unit\u201d of work in data analytics? Is it the answering of a question? That might yield commits/changes that are very large. Is it a software change? That might yield commits/changes that are too small. Admittedly, there is a bit of an art to getting this right.</p> <p>Here, I think treating <code>git</code> commits more as a \"log of work done\" and less of \"report of work done\" might be helpful in adapting <code>git</code> as a lab notebook-style log book.</p>"},{"location":"workflow/effective-commit-messages/#effective-git-commits","title":"Effective <code>git</code> commits","text":"<p>But before we describe how, a few preliminaries are in order. Let\u2019s take a look at what effective and informative commit messages accomplish:</p> <p>Firstly, if we are committing something that is work-in-progress (and yes, this should be permitted, because end-of-day always rolls by), a commit message can mark the fact that there is still work to be done, and describe enough prose to resume context the next day.</p> <p>Secondly, when used in tandem with a timeline, an informative commit message lets us quickly isolate when work was done, thus allowing us to retrace the progression of the project.</p> <p>Finally, good commit messages allow others we collaborate with to get a handle on the work that was already done. Well-written <code>git</code> commit messages can help colleagues that review our work get quickly up-to-speed on what was done, and what to review.</p> <p>In other words, effective commit messages act like documentation for our future selves and for others. Once again, the \u201csocial coding\u201d paradigm comes back.</p> Social coding? <p>Social coding: where we aren\u2019t programming something alone, but rather writing code in collaboration with others\u2019 input. OSS development is a wonderful example of this.</p>"},{"location":"workflow/effective-commit-messages/#git-commit-messages-examples-in-data-science-contexts","title":"<code>git</code> commit messages: examples in data science contexts","text":"<p>Let\u2019s see a few examples in action.</p>"},{"location":"workflow/effective-commit-messages/#the-trivial-change-message","title":"The Trivial Change Message","text":"<p>If we applied trivial changes, such as code formatting, rather than writing a message that read:</p> Don't do this <pre><code>black\n</code></pre> <p>Perhaps a a more informative message might be:</p> Do this <pre><code>Applied code formatting (make format).\n</code></pre> <p>We don\u2019t need an extended message (unlike those we might see later), because it is a trivial change.</p> <p>Now, I have been guilty of just writing <code>black</code> as the commit message, but usually that is in the context where I am working on my own project alone. Keeping in mind that commit messages are intended to be read by others, the more informative version is clearer to read and only takes practice to become second nature.</p>"},{"location":"workflow/effective-commit-messages/#the-work-in-progress-wip-message","title":"The Work-In-Progress (WIP) Message","text":"<p>Sometimes, the end of the day rolls by just like that, or we realize we have a mid-afternoon meeting to attend (these are, the wurst sausages!). In those scenarios, putting in a WIP commit may be helpful.</p> <p>So instead of writing a commit message that reads:</p> Don't do this <pre><code>WIP loaded data\n</code></pre> <p>We instead can write a commit message that reads:</p> Do this <pre><code>WIP finished code that loads data into memory\n\nWe still need to do the following:\n\n- Check statistical covariation between columns\n        and remove correlated features.\n- Identify the best predictors.\n</code></pre> <p>Now, when we look at the <code>git log</code>, we will see something that looks like this right at the top of our development branch:</p> <pre><code>* commit abe3d2e8ed55711a57835d96e67207aa2f07f383 (HEAD -&gt; feature-branch)\n| Author: Me &lt;abc@xyz.com&gt;\n| Date:   Fri Nov 15 14:01:13 2019 -0500\n|\n|     WIP finished code that loads data into memory\n|\n|     We still need to do the following:\n|\n|     - Check statistical covariation between columns and remove correlated features.\n|     - Identify the best predictors.\n|\n* commit ...\n</code></pre> <p>In this way, the <code>git</code> commit log gives us a way to use it as a \u201clab notebook\u201d-style running log of what\u2019s we have done.</p>"},{"location":"workflow/effective-commit-messages/#the-report-on-progress","title":"The Report on Progress","text":"<p>Pedantically, this is distinguished from the WIP message described above by being a \u201cfinal\u201d (but not necessarily binding) message in the work log.</p> <p>An uninformative commit message for this would look like:</p> Don't do this <pre><code>Finally done with model building\n</code></pre> <p>By contrast, an informative one might look something like this:</p> Do this <pre><code>Model building (Issue #34) ready for review\n\nFinished:\n\n- Pipeline taking data from input (strings) to activity prediction.\n- Custom code for data pipeline has been stored in custom package.\n    Tests and docs written.\n- Notebooks documenting work are also written.\n    Static HTML version for archival also generated.\n\nNot done:\n\n- Hyperparameter selection.\n    This is the logical next step,\n    and as agreed at last meeting, of highest priority.\n</code></pre> <p>Admittedly, it can be tough to know when to write this one, and I think it\u2019s because it feels like we might want to be sure that this is absolutely the place that we actually want to write such a message.</p> <p>To this, I would suggest simply commit (pun intended) to writing it when appropriate, and worry about minor things in later commits.</p>"},{"location":"workflow/effective-commit-messages/#squashed-commits","title":"Squashed commits","text":"<p>If we squash commits in our <code>git</code> workflow (e.g. when merging branches), then writing such detailed commit messages might seem unnecessary. To which my response is, yes indeed! In the case of using squashed commits really only the final commit message ends up being stored in the running log of what gets done. Hence, it makes perfect sense to focus writing good commit messages only at the merge stage, rather than at every single commit.</p>"},{"location":"workflow/effective-commit-messages/#intentional-adoption-of-better-commit-messages","title":"Intentional adoption of better commit messages","text":"<p>As I have observed with my own and colleagues\u2019 workflows, we do not regularly write informative commit messages because we don\u2019t read the git log. Then again, we don\u2019t read the git log because it doesn\u2019t contain a lot of information.</p> <p>Hold on, that sounds kind of circular, doesn\u2019t it?</p> <p>I think the chicken-and-egg cycle at some point has to be broken. By starting at some point, we break a vicious cycle of uninformative logging, and allow us to break into a virtuous cycle of good record-keeping. And that really is what this essay is trying to encourage: better record-keeping!</p>"},{"location":"workflow/effective-commit-messages/#further-reading","title":"Further Reading","text":"<ol> <li>How to Write a Git Commit Message by Chris Beams.</li> </ol> A note to Chris <p>Thank you for writing a wonderful article. I'll be praying for a speedy recovery, Chris.</p>"},{"location":"workflow/gitflow/","title":"Principled Git-based Workflow in Collaborative Data Science Projects","text":"<p>GitFlow is an incredible branching model for working with code. In this essay, I would like to introduce it to you, the data scientist, and show how it might be useful in your context, especially for working with multiple colleagues on the same project.</p>"},{"location":"workflow/gitflow/#what-gitflow-is","title":"What GitFlow is","text":"<p>GitFlow is a way of working with multiple collaborators on a git repository. It originated in the software development world, and gives software developers a way of keeping new development work isolated from reviewed, documented, and stable code.</p> <p>At its core, we have a \"source of truth\" branch called <code>master</code>, from which we make branches on which development work happens. Development work basically means new code, added documentation, more tests, etc. When the new code, documentation, tests, and more are reviewed, a pull request is made to merge the new code back into the <code>master</code> branch.</p> <p>Usually, the act of making a branch is paired with raising an issue on an issue tracker, in which the problem and proposed solution are written down. (In other words, the deliverables are explicitly sketched out.) Merging into master is paired with a code review session, in which another colleague (or the tech lead) reviews the code to be merged, and approves (or denies) code merger based on whether the issue raised in the issue tracker has been resolved.</p> <p>From my time experimenting with GitFlow at work, I think that when paired with other principled workflows that doen't directly interact with Git, can I think be of great utility to data scientists. It does, however, involve a bit of change in the common mode of working that data scientists use.</p> Is GitFlow still confusing for you? <p>If so, please check out this article on GitFlow. It includes the appropriate graphics that will make it much clearer. I felt that a detailed explanation here would be rather out of scope.</p> <p>That said, nothing beats trying it out to get a feel for it, so if you're willing to pick it up, I would encourage you to find a software developer in your organization who has experience with GitFlow and ask them to guide you on it.</p>"},{"location":"workflow/gitflow/#gitflow-in-a-data-science-project","title":"GitFlow in a data science project","text":"<p>Here is how I think GitFlow can be successfully deployed in a data science project.</p> <p>Everything starts with the unit of analysis that we are trying to perform.</p> <p>We start by defining the question that we are trying to answer. We then proceed forward by sketching out an analysis plan (let's call this an analysis sketch), which outlines the data sources that we need, the strategy for analyzing the data (roughly including: models we think might be relevant to the scale of the problem, the plots we think might be relevant to make, and where we think, future directions might lie).</p> <p>None of this is binding, which makes the analysis sketch less like a formal pre-registered analysis plan, and more like a tool to be more thoughtful of what we want to do when analyzing our data. After all, one of the myths of data science is that we can \"stir the pile until the data start looking right\".</p> About stirring the pot... <p>If you didn't click the URL to go to XKCD, here's the cartoon embedded below:</p> <p></p> <p>Once we are done with defining the analysis sketch in an issue, we follow the rest of GitFlow-based workflow: We create a branch off from <code>master</code>, execute on our work, and submit a pull request with everything that we have done.</p> <p>We then invite a colleague to review our work, in which the colleague is explicitly checking that we have delivered on our analysis sketch, or if we have changed course, to discuss the analysis with us in a formal setting. Ideally this is done in-person, but by submitting a formal pull request, our colleague can pull down our code and check that things have been done correctly on their computer.</p> Code review <p>If you want to know more about code review, please check out another essay in this collection.</p> <p>If your team has access to a Binder-like service, then review can be done in an even simpler fashion: simply create a Binder session for the colleague's fork, and explore the analyses there in a temporary session.</p> <p>Once the formal review has finished and both colleagues are on the same page with the analysis, the analysis is merged back into the <code>master</code> branch, and considered done. Both parties can now move onto the next analysis.</p>"},{"location":"workflow/gitflow/#mindset-changes-needed-to-make-gitflow-work","title":"Mindset changes needed to make GitFlow work","text":"<p>In this section, I am going to describe some common mindsets that prevent successful adoption of GitFlow that data scientists might employ, and ways to adapt those mindsets to work with GitFlow.</p>"},{"location":"workflow/gitflow/#jumping-straight-into-exploratory-data-analysis-eda","title":"Jumping straight into exploratory data analysis (EDA)","text":"<p>This is a common one that even I have done before. The refrain in our mind is, \"Just give me the CSV file! I will figure something out.\" Famous last words, once we come to terms with the horror that we experience in looking through the data.</p> <p>It seems, though, that we shouldn't be able to sketch an analysis plan for EDA, right?</p> <p>I think that mode of thinking might be a tad pessimistic. What we are trying to accomplish with exploratory data analysis is to establish our own working knowledge on:</p> <ul> <li>The bounds of the data,</li> <li>The types of the data (ordinal, categorical, numeric),</li> <li>The possible definitions of a single sample in the dataset,</li> <li>Covariation between columns of data,</li> <li>Whether or not the data can answer our questions, and</li> <li>Further questions that come up while looking at the data.</li> </ul> <p>Hence, a good analysis sketch to raise for exploratory data analysis would be to write a Jupyter notebook that simply documents all of the above, and then have a colleague review it.</p>"},{"location":"workflow/gitflow/#endless-modelling-experiments","title":"Endless modelling experiments","text":"<p>This is another one of those trops that I fall into often, so I am sympathetic towards others who might do the same.</p> <p>Scientists (of any type, not just data sciensists) usually come with an obsessive streak, and the way it manifests in data science is usually the quest for the best-performing model. However, in most data science settings, the goal we are trying to accomplish requires first proving out the value of our work using some form of prototype, so we cannot afford to chase performance rabbits down their hole.</p> <p>One way to get around this is to think about the problem in two phases.</p> <p>The first phase is model prototyping. As such, in the analysis sketch, we define a deliverable that is \"a machine learning model that predicts Y from X\", leaving out the performance metric for now. In other words, we are establishing a baseline model, and building out the analysis framework for evaluating how good the model is in the larger applied context.</p> <p>We do this in a quick and dirty fashion, and invite a colleague to review our work to ensure that we have not made any elementary statistical errors, and that the framework is correct with respect to the applied problem that we are tackling. (See note below for more detail.)</p> Note: statistical errors <p>For example, we need to get splitting done correctly in a time series setting, which does not have i.i.d. samples, compared to most other ML problems. And in a cheminformatics setting, random splits tend to over-estimate model performance when compared to a real-world setting where new molecules are often out-of-distribution.</p> <p>If we focused on getting a good model right from the get-go, we may end up missing out on elementary details such as these.</p> <p>Once we are done with this, we embark on the second phase, which is model improvement. Here, we define another analysis sketch where we outline the models that we intend to try, and for which the deliverable is now a Jupyter notebook documenting the modelling experiments we tried. As usual, once we are done, we invite a colleague to review the work to make sure that we have conducted it correctly.</p> <p>A key here is to define the task in as neutral and relevant terms as possible. For example, nobody can guarantee an improved model. However, we can promise a comprehensive, if not exhaustive, search through model and parameter space. We can also guarantee delivering recommendations for improvement regardless of what model performance looks like.</p> Note: Neutral forms of goals <p>As expressed on Twitter before, \"the most scary scientist is one with a hypothesis to prove\". A data scientist who declares that a high-performing model will be the goal is probably being delusional. I wish I knew where exactly I saw the quote, and hence will not take credit for that.</p>"},{"location":"workflow/gitflow/#endless-ideation-prototyping","title":"Endless ideation prototyping","text":"<p>Another trap I have fallen into involves endless ideation prototyping, which is very similar to the \"endless modelling experiments\" problem described above.</p> <p>My proposal here, then, is two-fold. Firstly, rather than running down rabbit holes endlessly, we trust our instincts in evaluating the maturity of an idea. Secondly, we ought also to define \"kill/stop criteria\" ahead-of-time, and move as quickly as possible to kill the idea while also documenting it in a Jupyter notebook. If made part of an analysis sketch that is raised on the issue tracker, then we can be kept accountable by our colleagues.</p>"},{"location":"workflow/gitflow/#benefits-of-adopting-gitflow-and-associated-practices","title":"Benefits of adopting GitFlow and associated practices","text":"<p>At its core, adopting a workflow as described above is really about intentionally slowing down our work a little bit so that we are more thoughtful about the work we want to finish. In work with my colleagues, I have found this to be incredibly useful. GitFlow and its associated practices bring a suite of benefits to our projects, and I think it is easy to see how.</p> <p>By spending a bit more time on thought and on execution, we cut down on wasted hours exploring unproductive analysis avenues.</p> <p>By pre-defining deliverables expressed in a neutral form, we reduce stress and pressure on data scientists, We also prevent endless rabbit-hole hacking to achieve those non-neutrally-expressed goals. We also receive a less biased analysis, which I believe can only help with making better decisions.</p> <p>Finally, by inviting colleagues to review our work, we also prevent the silo-ing of knowledge on one person, and instead distribute expertise and knowledge.</p>"},{"location":"workflow/gitflow/#how-to-gradually-adopt-gitflow-in-your-data-science-teams","title":"How to gradually adopt GitFlow in your data science teams","text":"<p>I know that not every single data science team will have adopted GitFlow from the get-go, and so there will have to be some form of ramp-up to get it going productively.</p> <p>Because this is a collaborative workflow, and because adoption is usually done only in the presence of incentives, I think that in order for GitFlow and associated practices to be adopted, one or more champions for using GitFlow needs to be empowered with the authority to use this workflow on any project they embark on. They also have to be sufficiently unpressured to deliver, so that time and performance pressures do not compromise on adoption. Finally, they have to be able to teach <code>git</code> newcomers and debug problems that show up in <code>git</code> branching, and be able to handle the <code>git</code> workflow for colleagues who might not have the time to pick it up.</p> <p>Tooling also has to be present. A modern version control system and associated hosting software, such as BitBucket, GitHub and GitLab, are necessary. Issue trackers also need to be present for each repository (or project, more generally).</p> <p>At my workplace, I have been fortunate to initiate two projects on which we practice GitFlow, bringing along an intern and a colleague one rank above me who were willing to try this out. This has led to much better sharing of the coding and knowledge load, and has also allowed us to cover for one another much more effectively.</p> <p>While above I may have sounded as if there is resistance to adoption, in practice I know that most data scientists instinctively know that proper workflows are going to be highly beneficial, but lack the time/space and incentives to introduce them in, yet would jump at the chance to do so if properly incentivized and given the time and space to do so.</p>"},{"location":"workflow/gitflow/#concluding-words","title":"Concluding words","text":"<p>I hope that I have convinced you that learning GitFlow, and its associated practices, can be incredibly useful for the long-term health and productivity of your data science team(s).</p>"}]}